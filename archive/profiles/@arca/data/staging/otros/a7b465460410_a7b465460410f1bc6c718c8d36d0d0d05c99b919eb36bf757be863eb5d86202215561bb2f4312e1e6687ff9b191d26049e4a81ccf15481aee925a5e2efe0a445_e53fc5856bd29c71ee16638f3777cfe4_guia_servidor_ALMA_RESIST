# ğŸ§  GuÃ­a de Uso â€“ Servidor ALMA_RESIST

## âœ… Paso 1: Instalar dependencias necesarias

```bash
# Activa tu entorno virtual si no lo estÃ¡
source .venv/bin/activate

# Instala las dependencias principales
pip install fastapi uvicorn llama-cpp-python python-multipart cryptography jsonschema
```

---

## ğŸš€ Paso 2: Iniciar el servidor

```bash
uvicorn core.llm_server.main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ§ª Endpoints principales

### 1. Generar respuestas (POST /responder)

```bash
curl -X POST "http://localhost:8000/responder" \
-H "Content-Type: application/json" \
-d '{"prompt": "Â¿CuÃ¡l es el sentido de la vida?"}'
```

Respuesta esperada:
```json
{
  "respuesta": "El sentido de la vida es...",
  "metadata": {
    "modelo": "mistral-7b-q4.gguf",
    "longitud_prompt": 28,
    "timestamp": "2023-10-15T12:34:56.789Z"
  }
}
```

---

### 2. Verificar salud del sistema (GET /health)

```bash
curl http://localhost:8000/health
```

Respuesta esperada:
```json
{
  "status": "OK",
  "model_loaded": true,
  "model_info": {
    "nombre": "mistral-7b-q4.gguf",
    "cuantizaciÃ³n": "Q4",
    "contexto": 4096,
    "tokens_maximos": 512,
    "parametros": "7.0B",
    "versiÃ³n_llama.cpp": "0.1.0"
  },
  "context_entries": 5,
  "graph_nodes": 42
}
```

---

### 3. Obtener historial de contexto (GET /context/history)

```bash
curl "http://localhost:8000/context/history?limit=3"
```

---

### 4. Exportar grafo de memoria (POST /memory/export)

```bash
curl -X POST "http://localhost:8000/memory/export?format=json"
```

---

### 5. Cifrar logs (POST /logs/encrypt)

```bash
curl -X POST http://localhost:8000/logs/encrypt
```

---

## âš™ï¸ ConfiguraciÃ³n del modelo

- **UbicaciÃ³n del modelo**: `core/llm_server/models/`
- **Ejemplo de variable en `main.py`**:
```python
model_path = "models/tu-modelo.gguf"
```

---

## ğŸ“ Estructura recomendada del proyecto

```
ALMA_RESIST/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ llm_server/
â”‚       â”œâ”€â”€ models/              # Modelos GGUF
â”‚       â”œâ”€â”€ transport_data/      # Datos de transporte
â”‚       â”œâ”€â”€ docs/contracts/      # Esquemas JSON
â”‚       â””â”€â”€ logs/                # Logs cifrados
```

---

## ğŸ§¬ Variables de entorno

Crear archivo `.env`:
```
MODEL_PATH=models/mistral-7b-q4.gguf
QUANTIZATION=Q4
```

---

## ğŸ›  Troubleshooting

### Verificar dependencias:

```bash
pip list | grep -E 'fastapi|uvicorn|llama-cpp-python'
```

### Logs al iniciar:

```text
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [12345] using WatchFiles
INFO:     Started server process [12347]
INFO:     Waiting for application startup.
INFO:     alma_server: Modelo models/mistral-7b-q4.gguf cargado exitosamente
INFO:     Application startup complete.
```

---

## â— Problemas comunes

- El modelo no carga:
  - AsegÃºrate de que el archivo del modelo existe.
  - Verifica permisos de lectura.
  - Proba con un modelo mÃ¡s liviano si tenÃ©s poca RAM.

---