            result.update({
                "status": "repaired" if not dry_run else "dry-run",
                "message": msg,
                "changes": changes,
                "errors_fixed": errors
            })
            
            if json_output:
                return 0, log_entry
                
            if dry_run:
                print(f"[DRY-RUN] Se repararían {len(errors)} errores")
            else:
                print(f"Archivo validado y reparado: {len(errors)} correcciones aplicadas")
        else:
            msg = "Metadatos válidos sin cambios"
            log_entry = logger.add_log_entry(
                "validar", file_path, "valid", msg, executor,
                json_output=json_output
            )
            result.update({"status": "valid", "message": msg})
            
            if json_output:
                return 0, log_entry
            print(msg)
            
        return 0, result
        
    except Exception as e:
        msg = f"Error en validación: {str(e)}"
        log_entry = logger.add_log_entry("validar", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"ERROR: {msg}\n")
        return 1, result

def limpiar_archivo(
    file_path: str, 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'limpiar'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("limpiar", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        cleaned_metadata = MetadataValidator.normalize(metadata)
        
        # Eliminar campos no estándar
        standard_fields = set(TEMPLATE_METADATOS.keys())
        non_standard_fields = [k for k in cleaned_metadata.keys() if k not in standard_fields]
        for field in non_standard_fields:
            del cleaned_metadata[field]
        
        # Actualizar historial si hubo cambios
        if cleaned_metadata != original_metadata:
            cleaned_metadata["historial"].append({
                "fecha": datetime.datetime.now().isoformat(),
                "usuario": executor,
                "accion": "limpieza",
                "descripcion": "Metadatos normalizados y limpiados"
            })
            cleaned_metadata["last_modified"] = datetime.datetime.now().isoformat()
            cleaned_metadata["last_modified_by"] = executor
            
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            metadata_handler.write_metadata(file_path, cleaned_metadata, 
                                         content_body, original_content)
            
            changes = len(non_standard_fields) + sum(
                1 for k in original_metadata 
                if original_metadata[k] != cleaned_metadata.get(k, None)
            )
            
            logger.add_log_entry("limpiar", file_path, "cleaned", 
                               f"Aplicadas {changes} limpiezas", executor,
                               metadata_before=original_metadata, 
                               metadata_after=cleaned_metadata)
            print(f"Archivo limpiado: {changes} cambios aplicados")
        else:
            logger.add_log_entry("limpiar", file_path, "clean", 
                               "No se requirieron cambios", executor)
            print("Metadatos ya limpios, no se requirieron cambios")
            
        return 0
        
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("limpiar", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_responsable(
    file_path: str, 
    responsable: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_responsable'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_responsable", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["responsable"] = responsable
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_responsable",
            "descripcion": f"Responsables actualizados: {', '.join(responsable)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_responsable", file_path, "updated", 
                           f"Responsables actualizados a: {', '.join(responsable)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Responsables actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_responsable", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["linked_to"] = linked_to
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(linked_to)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def mostrar_log(
    file_path: str, 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'log'"""
    try:
        metadata, _ = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("log", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        if "historial" not in metadata or not metadata["historial"]:
            print("No hay registros de historial disponibles")
            return 0
            
        print(f"Historial de cambios para: {file_path}")
        print("=" * 60)
        for entry in metadata["historial"]:
            print(f"[{entry.get('fecha', '')}] - {entry.get('usuario', '')}")
            print(f"Acción: {entry.get('accion', '')}")
            print(f"Descripción: {entry.get('descripcion', '')}")
            print("-" * 60)
            
        logger.add_log_entry("log", file_path, "viewed", 
                           "Historial consultado", executor)
        return 0
        
    except Exception as e:
        logger.add_log_entry("log", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

# =========================================
# ENTRYPOINT PRINCIPAL
# =========================================

def load_config() -> dict:
    """
    Carga la configuración desde alma_cleaner_config.yaml si existe,
    si no, usa DEFAULT_CONFIG. Si hay error en el parseo, lo reporta y sigue.
    """
    config_path = "alma_cleaner_config.yaml"
    config = {
        "logs": {
            "ruta": "/home/alma/Alma-Cli/Logs/Cleaner.parquet",
            "nivel": "INFO",
            "formato": "parquet",
            "loguear_errores": True
        },
        "politica_errores": "strict",
        "advertencia_tamano": 1048576,  # 1MB
        "hash_algoritmo": "sha256",
        "max_file_size": MAX_FILE_SIZE,
        "supported_extensions": SUPPORTED_EXTENSIONS,
        "backup_dir": "/tmp/alma_backups"
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.load(f) or {}
            # Actualiza solo claves existentes, no agrega basura
            for section in config:
                if section in user_config:
                    if isinstance(config[section], dict) and isinstance(user_config[section], dict):
                        config[section].update(user_config[section])
                    else:
                        config[section] = user_config[section]
            # Carga especial para parámetros críticos
            if "max_file_size" in user_config:
                config["max_file_size"] = user_config["max_file_size"]
            if "supported_extensions" in user_config:
                config["supported_extensions"] = user_config["supported_extensions"]
            if "backup_dir" in user_config:
                config["backup_dir"] = user_config["backup_dir"]
        except Exception as e:
            sys.stderr.write(f"ADVERTENCIA: Error cargando configuración: {str(e)}\n")
    return config

def main():
    # Configuración inicial
    config = load_config()
    logger = AuditLogger(config)
    metadata_handler = MetadataHandler(config)
    executor = os.environ.get("ALMA_EXECUTOR", "humano")

    # --- PARSER PRINCIPAL ---
    parser = argparse.ArgumentParser(
        prog="alma_cli_cleaner",
        description="CLI profesional para gestión de metadatos ALMA_RESIST"
    )
    parser.add_argument("--version", action="version", version=f"v{VERSION}")

    # --- PARENT PARSER: FLAGS GLOBALES ---
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument("--executor", default=executor, help="Tipo de ejecutor (humano, Kael, Centralesis)")
    parent_parser.add_argument("--force", action="store_true", help="Forzar operaciones potencialmente destructivas")
    parent_parser.add_argument("--dry-run", action="store_true", help="Simular operaciones sin realizar cambios")
    parent_parser.add_argument("--json", action="store_true", help="Salida en formato JSON")

    # --- SUBPARSERS ---
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Subcomando: crear
    crear_parser = subparsers.add_parser("crear", parents=[parent_parser], help="Crear archivo con metadatos")
    crear_parser.add_argument("archivo", help="Ruta del archivo a crear")
    crear_parser.add_argument("--title", required=True, help="Título del archivo")
    crear_parser.add_argument("--responsable", required=True, help="Responsable(s) del archivo")
    crear_parser.add_argument("--linked-to", default="", help="Archivos vinculados (separados por comas)")

    # Subcomando: validar
    validar_parser = subparsers.add_parser("validar", parents=[parent_parser], help="Validar metadatos de archivo")
    validar_parser.add_argument("archivo", help="Ruta del archivo a validar")

    # Subcomando: limpiar
    limpiar_parser = subparsers.add_parser("limpiar", parents=[parent_parser], help="Limpiar metadatos de archivo")
    limpiar_parser.add_argument("archivo", help="Ruta del archivo a limpiar")

    # Subcomando: set_responsable
    resp_parser = subparsers.add_parser("set_responsable", parents=[parent_parser], help="Establecer responsable(s)")
    resp_parser.add_argument("archivo", help="Ruta del archivo")
    resp_parser.add_argument("--responsable", required=True, help="Nuevo(s) responsable(s) (separados por comas)")

    # Subcomando: set_linked
    linked_parser = subparsers.add_parser("set_linked", parents=[parent_parser], help="Establecer archivos vinculados")
    linked_parser.add_argument("archivo", help="Ruta del archivo")
    linked_parser.add_argument("--linked-to", required=True, help="Archivos vinculados (separados por comas)")

    # Subcomando: log
    log_parser = subparsers.add_parser("log", parents=[parent_parser], help="Mostrar historial de cambios")
    log_parser.add_argument("archivo", help="Ruta del archivo")

    args = parser.parse_args()
    json_output = args.json

    # Clarificar comportamiento dry-run + force
    if args.dry_run and args.force:
        print("[ADVERTENCIA] --dry-run tiene prioridad sobre --force. No se realizarán cambios reales")

    # Ejecutar subcomando
    exit_code = 1
    json_result = {}

    try:
        if args.command == "crear":
            linked_list = [x.strip() for x in args.linked_to.split(",")] if args.linked_to else []
            exit_code, json_result = crear_archivo(
                args.archivo, args.title, args.responsable, linked_list,
                args.executor, args.force, logger, metadata_handler, args.dry_run, json_output
            )

        elif args.command == "validar":
            exit_code, json_result = validar_archivo(
                args.archivo, args.executor, args.force, logger, metadata_handler, args.dry_run, json_output
            )

        elif args.command == "limpiar":
            exit_code = limpiar_archivo(
                args.archivo, args.executor, logger, metadata_handler
            )

        elif args.command == "set_responsable":
            responsables_list = [x.strip() for x in args.responsable.split(",")]
            exit_code = set_responsable(
                args.archivo, responsables_list, args.executor, logger, metadata_handler
            )

        elif args.command == "set_linked":
            linked_list = [x.strip() for x in args.linked_to.split(",")]
            exit_code = set_linked(
                args.archivo, linked_list, args.executor, logger, metadata_handler
            )

        elif args.command == "log":
            exit_code = mostrar_log(
                args.archivo, args.executor, logger, metadata_handler
            )

        else:
            sys.stderr.write("Comando no reconocido.\n")
            exit_code = 1

        if json_output and json_result:
            print(json.dumps(json_result, indent=2))

    except Exception as e:
        error_msg = f"ERROR CRÍTICO: {str(e)}"
        logger.add_log_entry(
            getattr(args, "command", "unknown"),
            getattr(args, "archivo", ""),
            "error", error_msg, getattr(args, "executor", "humano")
        )
        if json_output:
            print(json.dumps({
                "status": "error",
                "message": error_msg,
                "command": getattr(args, "command", ""),
                "file": getattr(args, "archivo", "")
            }, indent=2))
        else:
            sys.stderr.write(f"{error_msg}\n")
        exit_code = 1

    finally:
        try:
            logger.flush_logs()
        except Exception as e:
            error_msg = f"ERROR AL ESCRIBIR LOGS: {str(e)}"
            if not json_output:
                sys.stderr.write(f"{error_msg}\n")

    sys.exit(exit_code)

# =========================================
# DOCUMENTACIÓN INTERNA
# =========================================

"""
README INTERNO: ALMA_CLI_CLEANER v0.1.5

MEJORAS PRINCIPALES RESPECTO A v0.1.4:

1.  ELIMINACIÓN DE DUPLICADOS:
    - Método atomic_write duplicado eliminado
    - Se conserva la versión mejorada con manejo de directorios

2.  EXTRACCIÓN YAML MEJORADA:
    - Retorno consistente incluso con archivos YAML vacíos
    - Manejo robusto de contenido vacío (retorna {} en lugar de None)

3.  MANEJO ROBUSTO DE DOCSTRINGS PYTHON:
    - Uso de patrón constante METADATA_PATTERN para consistencia
    - Validación de existencia del bloque antes de reemplazar
    - Error claro si no se encuentra el bloque de metadatos

4.  CORRECCIÓN DE SUBCOMANDO LOG:
    - Se añade parámetro 'executor' faltante en llamada
    - Se incluye manejador de metadatos en la llamada

5.  CONFIGURACIÓN EXTERNA:
    - Parámetros críticos configurados desde alma_cleaner_config.yaml:
        * max_file_size
        * supported_extensions
        * backup_dir
    - Valores por defecto mantenidos si no se especifican

6.  MECANISMO DE BLOQUEO:
    - Implementado con FileLock basado en fcntl
    - Previene condiciones de carrera en escrituras concurrentes
    - Timeout de 30 segundos para evitar bloqueos permanentes

7.  ROLLBACK AUTOMÁTICO:
    - Sistema de backups automáticos antes de cada escritura
    - Restauración automática en caso de error durante la operación
    - Limpieza de backups después de operaciones exitosas

8.  MEJORAS EN MIGRACIÓN DE TEMPLATES:
    - Registro detallado en historial de cambios durante migración
    - Advertencias explícitas en logs sobre campos migrados/eliminados

9.  COMPORTAMIENTO DRY-RUN + FORCE:
    - Prioridad clara a dry-run sobre force
    - Mensaje de advertencia explícito cuando se usan juntos

10. VALIDACIONES ADICIONALES:
    - Verificación de extensión de archivo soportada
    - Validación de existencia de bloque de metadatos en Python
    - Mejor manejo de errores en operaciones de escritura

CHANGELOG:
v0.1.5 - 2025-07-11
-   Corrección de problemas críticos identificados en auditoría
-   Implementación de sistema de bloqueo de archivos
-   Configuración externa de parámetros críticos
-   Rollback automático con sistema de backups
-   Manejo robusto de docstrings Python
-   Clarificación de comportamiento dry-run + force
-   Mejoras en migración de templates antiguos
-   Validaciones adicionales y manejo de errores mejorado

v0.1.4 - 2025-07-10
-   Migrado de PyYAML a ruamel.yaml
-   Implementado dry-run real
-   Añadida escritura atómica
-   Implementado hash de verificación
-   Añadida migración de templates
-   Soporte para salida JSON
-   Validación de archivos binarios/grandes
-   Mejor manejo de docstrings en Python
"""

if __name__ == "__main__":
    main()
```

### Changelog v0.1.5: (bloque yaml)

```yaml
cambios_aplicados_v0.1.5:
  version: "0.1.5"
  fecha: "2025-07-11"
  cambios:
    - eliminar_duplicacion_atomic_write:
        descripcion: "Eliminado método atomic_write duplicado en MetadataHandler"
        accion: "Se conservó la versión mejorada con manejo de directorios"
        impacto: "Reducción de código duplicado y mantenimiento simplificado"
        
    - refactorizar_extraccion_yaml:
        descripcion: "Corrección de retorno en método _extract_from_yaml"
        accion: |
          - Retorno consistente (dict, str) incluso para contenido vacío
          - Archivos YAML vacíos ahora retornan {} en lugar de None
        impacto: "Comportamiento consistente en extracción de metadatos"

    - manejo_robusto_docstrings:
        descripcion: "Manejo robusto de metadatos en archivos Python"
        accion: |
          - Implementado patrón constante METADATA_PATTERN
          - Validación de existencia del bloque antes de reemplazar
          - Error explícito si no se encuentra el bloque de metadatos
        impacto: "Prevención de corrupción de archivos .py"

    - corregir_subcomando_log:
        descripcion: "Corrección de parámetro faltante en subcomando log"
        accion: |
          - Añadido parámetro 'executor' en llamada a mostrar_log()
          - Incluido manejador de metadatos en la llamada
        impacto: "Funcionalidad completa del subcomando log restaurada"

    - configuracion_externa:
        descripcion: "Parámetros configurables desde archivo externo"
        accion: |
          - Carga de alma_cleaner_config.yaml para:
            * max_file_size
            * supported_extensions
            * backup_dir
          - Clase MetadataHandler ahora recibe configuración
        impacto: "Mayor flexibilidad en configuración"

    - file_locking:
        descripcion: "Implementación de mecanismo de bloqueo de archivos"
        accion: |
          - Clase FileLock basada en fcntl
          - Timeout de 30 segundos para evitar bloqueos permanentes
          - Integrado en atomic_write
        impacto: "Prevención de race conditions en escrituras concurrentes"

    - rollback_automatico:
        descripcion: "Sistema de rollback automático"
        accion: |
          - Creación de backups antes de cada escritura
          - Restauración automática en caso de error
          - Eliminación de backups después de operaciones exitosas
        impacto: "Mayor resiliencia frente a fallos"

    - migracion_templates:
        descripcion: "Mejoras en migración de templates antiguos"
        accion: |
          - Registro detallado en historial de cambios
          - Advertencias explícitas en logs
        impacto: "Mayor transparencia en procesos de migración"

    - clarificar_dry_run_force:
        descripcion: "Comportamiento claro para flags combinados"
        accion: |
          - Prioridad explícita a --dry-run sobre --force
          - Mensaje de advertencia cuando se usan juntos
        impacto: "Comportamiento predecible en operaciones complejas"

    - mejoras_seguridad:
        descripcion: "Validaciones adicionales de seguridad"
        accion: |
          - Verificación de extensiones soportadas
          - Validación de existencia de bloque metadatos en Python
          - Manejo mejorado de errores en escritura
        impacto: "Mayor estabilidad y prevención de errores"

  notas_adicionales:
    - "Todas las mejoras críticas de la auditoría fueron implementadas"
    - "Se mantuvo compatibilidad con versiones anteriores"
    - "La estructura modular del código fue preservada y mejorada"
    - "Se añadieron 150+ líneas de documentación interna"
    - "El rendimiento se mantuvo dentro de parámetros aceptables"
```


## 2025-07-10: "ALMA_CLI_CLEANER v0.1.6" Backup en punto 6

### 1. 2025-07-10: Script: "Alma_Cli_Cleaner_v0.1.6" modulo modificado (bloque python)

Se reemplazo el siguiente bloque en el script anterior:
/home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.5/Alma_Cli_Cleaner_v0.1.5.py
y se creo:
/home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Alma_Cli_Cleaner_v0.1.6.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =========================================
# ALMA_CLI_CLEANER v0.1.6
# CLI profesional para gestión de metadatos ALMA_RESIST
# =========================================

# (Se mantiene todo el código anterior sin cambios hasta la sección corregida)

    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass
```

### 2. 2025-07-10: Changelog v0.1.6: (bloque yaml)

Cambios aplicados para la version 0.1.6
Script : /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Alma_Cli_Cleaner_v0.1.6.py

```yaml
cambios_aplicados_v0.1.6:
  version: "0.1.6"
  fecha: "2025-07-11"
  cambios:
    - correccion_sintaxis_atomic_write:
        descripcion: "Corregido error de sintaxis en creación del path del backup"
        accion: "Añadido paréntesis faltante en la llamada os.path.join"
        impacto: "Corregido fallo crítico que impedía ejecución del backup automático"

    - manejo_explicito_errores_backup:
        descripcion: "Añadido manejo explícito de errores en creación de backups"
        accion: |
          - Creación de backups dentro de bloque try-except
          - Mensajes claros de error si falla la creación del backup
        impacto: "Mejor diagnóstico de errores y resiliencia del sistema de backup"

    - configuracion_timeout_filelock:
        descripcion: "Timeout configurable para adquisición de bloqueo de archivo"
        accion: |
          - Timeout ahora es configurable desde la configuración externa
          - Manejo explícito de excepción TimeoutError con logging específico
        impacto: "Mayor flexibilidad y control en condiciones de alta concurrencia"

  notas_adicionales:
    - "Se recomienda verificar configuración externa para nuevo parámetro 'bloqueo_timeout'"
    - "Todos los errores críticos identificados en la auditoría se han corregido"
    - "La estabilidad del sistema frente a operaciones concurrentes ha sido mejorada"
```

### 3. 2025-07-10: Parche realizado a la v0.1.6 en "class FileLock:"

Fecha: 2025-07-10

Se modifico "class FileLock:"

Se estan probando los siguientes modulos hasta ver cual es el correcto.

#### Modulo Historico

```python
class FileLock:
    """Mecanismo de bloqueo de archivos para prevenir condiciones de carrera"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None

    def __enter__(self):
        """Adquiere el bloqueo"""
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > 30:  # Timeout después de 30 segundos
                    raise TimeoutError("No se pudo adquirir el bloqueo después de 30 segundos")
                time.sleep(0.1)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Libera el bloqueo"""
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
            self.lock_fd.close()
            try:
                os.remove(self.lock_file)
            except OSError:
                pass
```

#### Modulo Modificado:

```python
class FileLock:
    def __init__(self, file_path: str, timeout: int = 30):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None
        self.timeout = timeout  # <--- agregalo acá

    def __enter__(self):
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > self.timeout:
                    raise TimeoutError("No se pudo adquirir el bloqueo después de %d segundos" % self.timeout)
                time.sleep(0.1)
```

Nota: No funciono bien a la primera espera correcciones.

#### Modulo FileLock final:

Se genero modulo FileLock final.

```python
class FileLock:
    def __init__(self, file_path: str, timeout: int = 30):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None
        self.timeout = timeout

    def __enter__(self):
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > self.timeout:
                    raise TimeoutError("No se pudo adquirir el bloqueo después de %d segundos" % self.timeout)
                time.sleep(0.1)

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
            self.lock_fd.close()
            try:
                os.remove(self.lock_file)
            except OSError:
                pass
```

Nota: No funciono bien a la primera espera correcciones.



### 4. 2025-07-10: Parche realizado a la v0.1.6 en "log_entry = {}"

Fecha: 2025-07-10

Se modifico: "log_entry = {}"

#### Bloque python reemplazado :

```python
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before,
            "metadata_after": metadata_after
        }
```

#### Bloque python nuevo:

```python
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else {},
            "metadata_after": metadata_after if metadata_after is not None else {},
        }
```

Reemplazo el bloque y pruebo de nuevo el paso 4.

### 5. 2025-07-10: Parche realizado a la v0.1.6 en el punto 4 de las pruebas:

#### 2025-07-10 Fix 1: Modificacion en ""ia_metadata": {},"
Tiro un nuevo error y modifcamos la parte de:

```python
    "ia_metadata": {},
```

Y le agregamos:

```python
    "dummy": None
```
Resultado Final:

```python
    "ia_metadata": {"dummy": None},
```


#### 2025-07-10 Fix 2: Modificacion en "def normalize(metadata: Dict) -> Dict:"

Se agrega estto al bloque

```python
    @staticmethod
    def normalize(metadata: Dict) -> Dict:
        normalized = metadata.copy()
        # ...tu lógica normal...
        if "ia_metadata" not in normalized or not isinstance(normalized["ia_metadata"], dict) or not normalized["ia_metadata"]:
            normalized["ia_metadata"] = {"dummy": None}
        return normalized
```
Se modifico la parte seguimos en el punto 4 de las pruebas


#### 2025-07-10 Fix 3: Modificacion en "def migrate_template(old_metadata: Dict) -> Dict:"

Se modifica el bloque retirando Bloque python historico y se ingresa bloque python nuevo

##### Bloque python historico

Bloque python eliminado:

```python
    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()
        
        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }
        
        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo migrado: {old_field} -> {new_field}"
                })
        
        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            if field in migrated:
                del migrated[field]
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo obsoleto eliminado: {field}"
                })
        
        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated
```

##### Bloque python nuevo

Este bloque se agrego:

```python
    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()

        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }

        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo migrado: {old_field} -> {new_field}"
                })

        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            if field in migrated:
                del migrated[field]
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo obsoleto eliminado: {field}"
                })

        # --- BLOQUE CLAVE: asegurar ia_metadata correcto ---
        if (
            "ia_metadata" not in migrated or
            not isinstance(migrated["ia_metadata"], dict) or
            not migrated["ia_metadata"]
        ):
            migrated["ia_metadata"] = {"dummy": None}
        # --- FIN BLOQUE CLAVE ---

        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated
```

#### 2025-07-10 Fix 4: Modificacion en "def flush_logs(self) -> None:"

##### Prevención de errores Parquet: dicts vacíos en campos anidados

**Motivo del cambio:**  
PyArrow (y el formato Parquet en general) **no permite guardar estructuras de tipo "struct" (dict) completamente vacías** como parte de una tabla. Si en algún campo anidado (por ejemplo, `ia_metadata`) se intenta guardar `{}` en vez de un dict con al menos un campo, la operación falla y arroja errores de schema incompatibles.

**Solución aplicada:**  
Antes de escribir los logs en formato Parquet, se aplica una función recursiva (`fix_empty_structs`) que reemplaza **cualquier dict vacío por un dict con un campo dummy** (`{"dummy": None}`).  
Esto asegura que todos los campos anidados tengan estructura válida para Parquet, eliminando errores y manteniendo compatibilidad incluso si en el futuro se agregan nuevos campos de tipo dict en los metadatos.

**Ventajas:**
- Evita caídas silenciosas y corrupciones de log al trabajar con Parquet.
- Permite que los logs sean siempre homogéneos y robustos, sin importar la fuente de los datos.
- Deja el sistema preparado para futuras integraciones de IA o nuevos campos anidados, sin requerir reescritura de esta lógica.


##### Bloque python historico

Bloque python que fue retirado.

```python
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
            
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                        
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                    
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(self.log_buffer)
                    
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                    
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
            
            self.log_buffer = []
            
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                    
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                    def fix_empty_structs(obj):
                        if isinstance(obj, dict):
                            if not obj:
                                return {"dummy": None}
                            return {k: fix_empty_structs(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [fix_empty_structs(item) for item in obj]
                        else:
                            return obj

                    fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                    # ---- FIN BLOQUE CRÍTICO ----

                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(fixed_buffer)
                
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
        
            self.log_buffer = []
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

```

Problema resuelto 2025-07-10 17:28

#### Backup 2025-07-10 17:51

Link del backup : 
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507101733.py

### 6. 2025-07-10: Parche realizado a la v0.1.6 en el punto 5 de las pruebas: Borra los linked_to anteriores (Backup Realizado en este punto)

Borra los linked_to anteriores , deberiamos corregirlo.

##### Mejora en la función `set_linked`: preservación y suma de archivos vinculados

**Problema anterior:**  
El comando `set_linked` sobreescribía completamente la lista de archivos vinculados (`linked_to`), eliminando los registros previos cada vez que se ejecutaba.

**Solución aplicada:**  
Ahora, al agregar nuevos archivos vinculados, estos se suman a los ya existentes sin generar duplicados, preservando así la trazabilidad histórica de todas las vinculaciones del archivo.

- Los archivos indicados se agregan al final de la lista `linked_to`.
- Si ya existían, no se repiten (se mantiene solo una entrada por vínculo).
- El historial de cambios documenta siempre la última actualización.

**Ventaja:**  
Se garantiza que ningún vínculo previo se pierde, facilitando la navegación y el análisis de relaciones dentro del sistema ALMA_RESIST, y manteniendo la integridad documental a lo largo del tiempo.


##### Bloque python historico

Bloque python que fue retirado.

```python
def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["linked_to"] = linked_to
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(linked_to)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1

        original_metadata = metadata.copy()
        # Sumar a los ya existentes y eliminar duplicados
        prev_links = metadata.get("linked_to", [])
        all_links = list(dict.fromkeys(prev_links + linked_to))  # mantiene orden y no duplica
        metadata["linked_to"] = all_links

        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(all_links)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1
```


##### Backup previo a Reparar

Link :
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507101759.py


##### Prueba de actualización incremental de archivos vinculados (`linked_to`)

**Fecha:** 2025-07-11  
**Script:** `Alma_Cli_Cleaner_v0.1.6.py`  
**Responsable:** Bird

###### Descripción:
Se verificó que la función `set_linked` permite agregar nuevos archivos a la lista `linked_to` de los metadatos de un archivo `.md` **sin eliminar los vínculos anteriores** y **sin duplicar archivos ya vinculados**.

###### Pasos realizados:
1. Se ejecutó el comando:
    ```bash
    python3 Alma_Cli_Cleaner_v0.1.6.py set_linked ./test.md --linked-to "./otro_archivo.md,./mas_vinculos.md"
    ```
    - Resultado: los archivos `./otro_archivo.md` y `./mas_vinculos.md` fueron agregados exitosamente.

2. Se ejecutó un segundo comando para probar la adición incremental:
    ```bash
    python3 Alma_Cli_Cleaner_v0.1.6.py set_linked ./test.md --linked-to "./informe2025.md,./balance_semanal.md,./bitacora_diaria.md"
    ```
    - Resultado: los archivos `./informe2025.md`, `./balance_semanal.md` y `./bitacora_diaria.md` se agregaron al listado, manteniéndose los vínculos anteriores y sin generar duplicados.

###### Verificación:
- El campo `linked_to` en el archivo `test.md` quedó con todos los archivos vinculados, y **no se perdió ningún vínculo previo**.
- El historial de cambios registró correctamente cada actualización.
- El sistema se comportó de acuerdo a la lógica esperada y no se reportaron errores en logs ni en Parquet.

###### Conclusión:
La función de actualización incremental de archivos vinculados (`set_linked`) quedó validada y funcional, garantizando integridad documental y trazabilidad de relaciones en los metadatos del sistema ALMA_RESIST.


### Prueba del historial de cambios (`log`)

**Fecha:** 2025-07-11  
**Script:** `Alma_Cli_Cleaner_v0.1.6.py`  
**Responsable:** Bird

#### Descripción:
Se validó la funcionalidad del subcomando `log`, encargado de mostrar el historial completo de cambios registrados en los metadatos de cada archivo.  
El historial incluye fecha, usuario, acción y descripción de cada modificación significativa.

#### Comando ejecutado:
```bash
python3 Alma_Cli_Cleaner_v0.1.6.py log ./test.md
```

Verificación:

 - El historial mostró correctamente cada acción ejecutada sobre el archivo, incluyendo actualizaciones de responsables y archivos vinculados.

 - Cada entrada incluye marca de tiempo, usuario ejecutor, tipo de acción y descripción asociada.

 - No se detectaron errores en la visualización ni inconsistencias en la trazabilidad de cambios.

Conclusión:

La función de auditoría interna (log) quedó validada, permitiendo seguimiento total y transparente de las modificaciones sobre los archivos del sistema ALMA_RESIST.
Este subcomando es fundamental para la auditoría y la gobernanza documental.

#### Estabilización de schema en logs Parquet: dicts completos desde la primera fila

**Situación:**  
Se detectó que los errores de Parquet al escribir logs provenían de una inconsistencia en el esquema (`schema`) de los campos anidados (`metadata_before` y `metadata_after`). Esto ocurría cuando la primera entrada registrada contenía únicamente un diccionario mínimo (`{"dummy": None}`), mientras que las siguientes incluían todos los campos de metadatos completos. Como Parquet fija el schema a partir de la primera fila, esto causaba errores de incompatibilidad en los registros posteriores.

**Solución aplicada:**  
A partir de ahora, el sistema fuerza que **todas las entradas del log utilicen un diccionario completo con todas las claves definidas en `TEMPLATE_METADATOS`** para los campos `metadata_before` y `metadata_after`, incluso cuando se encuentren vacíos. Esto asegura que el schema del archivo Parquet sea siempre homogéneo, robusto y compatible desde la primera fila.

**Impacto positivo:**  
- Se elimina el riesgo de errores de tipado o incompatibilidad en el log Parquet.
- Se garantiza la trazabilidad y la integridad del historial de auditoría documental, sin importar el flujo ejecutado.
- La solución es escalable y preparada para futuros cambios en la estructura de metadatos.

**Se agrego:**

```python
def empty_metadata_dict():
    # Cada lista tiene un valor real como dummy para fijar el tipo
    return {
        "title": "",
        "uuid": "",
        "tipo": "",
        "schema": "",
        "estado": "",
        "descripcion": "",
        "estructura": [""],  # <-- string dummy
        "tags": [""],        # <-- string dummy
        "linked_to": [""],   # <-- string dummy
        "responsable": [""], # <-- string dummy
        "hash_verificacion": "",
        "historial": [{
            "fecha": "",
            "usuario": "",
            "accion": "",
            "descripcion": ""
        }],  # <-- dict dummy
        "last_modified": "",
        "last_modified_by": "",
        "created_at": "",
        "created_by": "",
        "proceso_origen": "",
        "input_data": [""],
        "output_data": [""],
        "comentarios": "",
        "ia_metadata": {"dummy": None},
        "template_version": "",
    }
```

El error persiste le pedimos a deepsek una refaccion de dos modulos internos.

### 2025-07-10: Refaccion Deepseek

Prompt para DeepSeek - Refactor Logging PyArrow/Parquet
Proyecto: ALMA_CLI_CLEANER
Autor: Santi (Bird)
Fecha: 2025-07-10

```yaml
# ============================================================
#  Prompt para DeepSeek - Refactor Logging Parquet/PyArrow
#  Proyecto: ALMA_CLI_CLEANER v0.1.6
# ============================================================

contexto:
  descripcion: |
    El CLI ALMA_CLI_CLEANER realiza auditoría estructurada de archivos YAML/Markdown y deja un log de eventos en Parquet.
    El buffer de logs es una lista de dicts con metadatos antes y después de cada operación.
    El template de metadatos es fijo y explícito en el código (ver TEMPLATE_METADATOS).
    Las funciones de logging utilizan PyArrow para escribir el buffer, pero el schema infiere tipos incorrectos si el primer registro no contiene datos reales (dummy vacíos).
    El error crítico es "Schema at index 1 was different", por incompatibilidad mínima de tipos (ej: list<item: null> vs list<item: string>).

  objetivo: |
    Refactorizar el módulo de logs para que:
      - Defina el schema de PyArrow explícitamente (sin inferencia automática).
      - Siempre que una lista/dict esté vacía, se fuerce el tipo correcto (string, dict, etc).
      - El sistema pueda crecer en campos y no se rompa nunca por diferencias de tipo.
      - Sea compatible con append incremental: el schema del archivo debe coincidir con el de los nuevos logs.
    El buffer de logs debe ser casteado o rellenado con dummy según el template, antes de serializarse.

  instrucciones:
    - Analizá el código completo (ver abajo).
    - Proponé una solución concreta: bloque de función `flush_logs`, declaración de schema PyArrow explícito, y helpers si es necesario.
    - El bloque de logging debe ser plug-and-play y robusto para cualquier cambio futuro de campos.
    - Dejá el ejemplo de schema (en Python) para referencia.
    - Agregá abajo los módulos a refactorizar (`flush_logs`, `TEMPLATE_METADATOS`, helpers).

codigo:
  - flush_logs: |
      def flush_logs(self) -> None:
          """Escribe todos los logs en buffer al destino configurado"""
          if not self.log_buffer:
              return
        
          try:
              log_path = self.config["logs"]["ruta"]
              log_format = self.config["logs"]["formato"].lower()
              os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
              if log_format == "json":
                  with open(log_path, 'a', encoding='utf-8') as f:
                      for entry in self.log_buffer:
                          f.write(json.dumps(entry) + "\n")
                    
              elif log_format == "yaml":
                  with open(log_path, 'a', encoding='utf-8') as f:
                      yaml.dump_all(self.log_buffer, f)
                
              elif log_format == "parquet":
                  try:
                      import pyarrow as pa
                      import pyarrow.parquet as pq

                      # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                      def fix_empty_structs(obj):
                          if isinstance(obj, dict):
                              if not obj:
                                  return {"dummy": None}
                              return {k: fix_empty_structs(v) for k, v in obj.items()}
                          elif isinstance(obj, list):
                              return [fix_empty_structs(item) for item in obj]
                          else:
                              return obj

                      fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                      # ---- FIN BLOQUE CRÍTICO ----

                      # Convertir a tabla PyArrow
                      table = pa.Table.from_pylist(fixed_buffer)
                
                      # Escribir en modo append
                      if os.path.exists(log_path):
                          existing = pq.read_table(log_path)
                          table = pa.concat_tables([existing, table])
                
                      pq.write_table(table, log_path)
                
                  except ImportError:
                      # Fallback a JSON si pyarrow no está disponible
                      with open(log_path + ".json", 'a', encoding='utf-8') as f:
                          for entry in self.log_buffer:
                              f.write(json.dumps(entry) + "\n")
        
              self.log_buffer = []
        
          except Exception as e:
              sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
  - template_metadatos: |
      TEMPLATE_METADATOS = {
        "title": "",
        "uuid": "",
        "tipo": "",
        "schema": "",
        "estado": "",
        "descripcion": "",
        "estructura": [],
        "tags": [],
        "linked_to": [],
        "responsable": [],
        "hash_verificacion": "",
        "historial": [],
        "last_modified": "",
        "last_modified_by": "",
        "created_at": "",
        "created_by": "",
        "proceso_origen": "",
        "input_data": [],
        "output_data": [],
        "comentarios": "",
        "ia_metadata": {"dummy": None},
        "template_version": TEMPLATE_VERSION,
      }
  - helpers: |
      class AuditLogger:
          """Manejador centralizado de logs y auditoría"""
    
          def __init__(self, config: Dict):
              self.config = config
              self.log_buffer = []
        
          def add_log_entry(
              self, 
              action: str, 
              file_path: str, 
              status: str, 
              details: str, 
              user: str, 
              metadata_before: Dict = None, 
              metadata_after: Dict = None,
              json_output: bool = False
          ) -> Union[Dict, None]:
              """Agrega una entrada al buffer de logs"""
              log_entry = {
                  "timestamp": datetime.datetime.now().isoformat(),
                  "action": action,
                  "file": file_path,
                  "status": status,
                  "details": details,
                  "user": user,
                  "version": VERSION,
                  "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
                  "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
              }
        
              self.log_buffer.append(log_entry)
        
              if json_output:
                  return log_entry
              return None
        
          def flush_logs(self) -> None:
              """Escribe todos los logs en buffer al destino configurado"""
              if not self.log_buffer:
                  return
        
              try:
                  log_path = self.config["logs"]["ruta"]
                  log_format = self.config["logs"]["formato"].lower()
                  os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
                  if log_format == "json":
                      with open(log_path, 'a', encoding='utf-8') as f:
                          for entry in self.log_buffer:
                              f.write(json.dumps(entry) + "\n")
                    
                  elif log_format == "yaml":
                      with open(log_path, 'a', encoding='utf-8') as f:
                          yaml.dump_all(self.log_buffer, f)
                
                  elif log_format == "parquet":
                      try:
                          import pyarrow as pa
                          import pyarrow.parquet as pq

                          # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                          def fix_empty_structs(obj):
                              if isinstance(obj, dict):
                                  if not obj:
                                      return {"dummy": None}
                                  return {k: fix_empty_structs(v) for k, v in obj.items()}
                              elif isinstance(obj, list):
                                  return [fix_empty_structs(item) for item in obj]
                              else:
                                  return obj

                          fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                          # ---- FIN BLOQUE CRÍTICO ----

                          # Convertir a tabla PyArrow
                          table = pa.Table.from_pylist(fixed_buffer)
                
                          # Escribir en modo append
                          if os.path.exists(log_path):
                            existing = pq.read_table(log_path)
                            table = pa.concat_tables([existing, table])
                
                          pq.write_table(table, log_path)
                
                      except ImportError:
                          # Fallback a JSON si pyarrow no está disponible
                          with open(log_path + ".json", 'a', encoding='utf-8') as f:
                              for entry in self.log_buffer:
                                  f.write(json.dumps(entry) + "\n")
        
                  self.log_buffer = []
        
              except Exception as e:
                  sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")   

notas:
  - Si el código necesita helpers de casteo, incluilos.
  - Si hace falta modificar cómo se arma el buffer, sugerilo.
  - Acepta sugerencias de mejora, pero la prioridad es robustez en schema y facilidad de extensión.

# FIN DEL PROMPT

```

#### Respuesta Deepseek

Script sugerido por DeepSeek.

```python
# ====================== HELPERS ======================
def get_arrow_schema() -> pa.Schema:
    """Define schema explícito para registros de log"""
    return pa.schema([
        ("timestamp", pa.string()),
        ("action", pa.string()),
        ("file", pa.string()),
        ("status", pa.string()),
        ("details", pa.string()),
        ("user", pa.string()),
        ("version", pa.string()),
        ("metadata_before", get_metadata_struct()),
        ("metadata_after", get_metadata_struct())
    ])

def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(pa.string())),  # Asumido como lista de strings
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),  # Asumido como lista de strings
        ("output_data", pa.list_(pa.string())), # Asumido como lista de strings
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])

def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)
        
        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}
        
        # Conversión de tipos
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}
        
        normalized[key] = val
    
    return normalized

# ====================== EN AuditLogger ======================
class AuditLogger:
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        self.schema = get_arrow_schema()  # Schema predefinido
    
    def add_log_entry(...):  # Mantener igual
        ...
    
    def flush_logs(self) -> None:
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Normalizar datos usando el template
                    TEMPLATE_METADATOS = { ... }  # Definir aquí o importar
                    normalized_buffer = []
                    for entry in self.log_buffer:
                        norm_entry = entry.copy()
                        norm_entry["metadata_before"] = normalize_data(
                            entry.get("metadata_before", {}),
                            TEMPLATE_METADATOS
                        )
                        norm_entry["metadata_after"] = normalize_data(
                            entry.get("metadata_after", {}),
                            TEMPLATE_METADATOS
                        )
                        normalized_buffer.append(norm_entry)
                    
                    # Crear tabla con schema explícito
                    table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)
                    
                    # Manejo de appends
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        
                        # Unificar schemas si es necesario
                        if existing.schema != self.schema:
                            existing = existing.cast(self.schema)
                        
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON
                    ...
            
            else:
                # Mantener implementación original para JSON/YAML
                ...
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

```yaml
refactor_logging_solution:
  objetivo: "Refactorizar módulo de logs para PyArrow con schema explícito y manejo robusto de tipos"
  cambios_principales:
    - schema_explicito: "Definición manual de tipos PyArrow basada en TEMPLATE_METADATOS"
    - normalizacion_datos: "Relleno de valores vacíos con tipos correctos"
    - manejo_appends: "Unificación de schemas durante escritura incremental"
    - eliminacion_hacks: "Remoción del bloque fix_empty_structs"

  implementacion:
    helpers:
      - nombre: "get_arrow_schema"
        descripcion: "Define schema explícito para registros de log"
        estructura: |
          pa.schema([
            ("timestamp", pa.string()),
            ("action", pa.string()),
            ("file", pa.string()),
            ("status", pa.string()),
            ("details", pa.string()),
            ("user", pa.string()),
            ("version", pa.string()),
            ("metadata_before", get_metadata_struct()),
            ("metadata_after", get_metadata_struct())
          ])

      - nombre: "get_metadata_struct"
        descripcion: "Schema para estructura de metadatos"
        estructura: |
          pa.struct([
            ("title", pa.string()),
            ("uuid", pa.string()),
            ("tipo", pa.string()),
            ("schema", pa.string()),
            ("estado", pa.string()),
            ("descripcion", pa.string()),
            ("estructura", pa.list_(pa.string())),
            ("tags", pa.list_(pa.string())),
            ("linked_to", pa.list_(pa.string())),
            ("responsable", pa.list_(pa.string())),
            ("hash_verificacion", pa.string()),
            ("historial", pa.list_(pa.string())),
            ("last_modified", pa.string()),
            ("last_modified_by", pa.string()),
            ("created_at", pa.string()),
            ("created_by", pa.string()),
            ("proceso_origen", pa.string()),
            ("input_data", pa.list_(pa.string())),
            ("output_data", pa.list_(pa.string())),
            ("comentarios", pa.string()),
            ("ia_metadata", pa.map_(pa.string(), pa.string())),
            ("template_version", pa.string())
          ])

      - nombre: "normalize_data"
        descripcion: "Normaliza datos según template con tipos correctos"
        logica: |
          for key, default_val in template.items():
            val = entry.get(key, default_val)
            if isinstance(default_val, list) and not val: val = []
            elif isinstance(default_val, dict) and not val: val = {}
            if isinstance(default_val, list) and not isinstance(val, list): val = [val]
            elif isinstance(default_val, dict) and not isinstance(val, dict): val = {}
            normalized[key] = val

    cambios_en_AuditLogger:
      - clase: "AuditLogger"
        cambios:
          - en_metodo: "__init__"
            adicion: "self.schema = get_arrow_schema()"

          - en_metodo: "flush_logs"
            reemplazo_bloque_parquet: |
              # Normalizar datos usando template
              normalized_buffer = []
              for entry in self.log_buffer:
                  norm_entry = entry.copy()
                  norm_entry["metadata_before"] = normalize_data(entry.get("metadata_before", {}), TEMPLATE_METADATOS)
                  norm_entry["metadata_after"] = normalize_data(entry.get("metadata_after", {}), TEMPLATE_METADATOS)
                  normalized_buffer.append(norm_entry)
              
              # Crear tabla con schema explícito
              table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)
              
              # Manejo de appends con unificación de schema
              if os.path.exists(log_path):
                  existing = pq.read_table(log_path)
                  if existing.schema != self.schema:
                      existing = existing.cast(self.schema)
                  table = pa.concat_tables([existing, table])

  beneficios:
    - "Elimina errores de inferencia de tipos en PyArrow"
    - "Permite estructuras vacías sin necesidad de hacks (dummy fields)"
    - "Mantiene compatibilidad con archivos existentes"
    - "Permite evolución futura de campos"
    - "Mejor desempeño al evitar conversiones innecesarias"

  notas_importantes:
    - "El campo 'ia_metadata' se define como mapa string->string. Si tiene estructura compleja, usar struct específico"
    - "Campos de lista (tags, linked_to) deben contener elementos del tipo definido"
    - "La normalización garantiza coherencia entre registros vacíos/llenos"
    - "El cast durante append asegura compatibilidad entre versiones"
    - "Template debe mantenerse actualizado con nuevos campos"

  flujo_mejorado:
    1. "Agregar entrada al buffer"
    2. "Al flush: normalizar cada entrada con template"
    3. "Crear tabla PyArrow con schema explícito"
    4. "Si existe archivo: leer, unificar schema y concatenar"
    5. "Escribir nueva tabla unificada"
```

#### Backup Pre-refacción :

Link :
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507102048.py

### 2025-07-10: Remplazo de modulo "classAuditLogger"

Se remplaza el modulo completo AuditLogger

##### Bloque python historico

Bloque python que fue retirado.

```python
class AuditLogger:
    """Manejador centralizado de logs y auditoría"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        
    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
            "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
        }
        
        self.log_buffer.append(log_entry)
        
        if json_output:
            return log_entry
        return None
        
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                    
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                    def fix_empty_structs(obj):
                        if isinstance(obj, dict):
                            if not obj:
                                return {"dummy": None}
                            return {k: fix_empty_structs(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [fix_empty_structs(item) for item in obj]
                        else:
                            return obj

                    fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                    # ---- FIN BLOQUE CRÍTICO ----

                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(fixed_buffer)
                
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
        
            self.log_buffer = []
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
class AuditLogger:
    """Manejador centralizado de logs y auditoría"""

    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        self.schema = get_arrow_schema()  # Schema explícito definido por helper

    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
            "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
        }

        self.log_buffer.append(log_entry)

        if json_output:
            return log_entry
        return None

    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return

        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)

            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")

            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)

            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # --- BLOQUE CRÍTICO: Normalización según schema explícito ---
                    normalized_buffer = []
                    for entry in self.log_buffer:
                        norm_entry = entry.copy()
                        norm_entry["metadata_before"] = normalize_data(
                            entry.get("metadata_before", {}),
                            TEMPLATE_METADATOS
                        )
                        norm_entry["metadata_after"] = normalize_data(
                            entry.get("metadata_after", {}),
                            TEMPLATE_METADATOS
                        )
                        normalized_buffer.append(norm_entry)

                    # Crear tabla con schema explícito
                    table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)

                    # Append si existe archivo previo (y unificar schema)
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        if existing.schema != self.schema:
                            existing = existing.cast(self.schema)
                        table = pa.concat_tables([existing, table])

                    pq.write_table(table, log_path)

                except ImportError:
                    # Fallback a JSON si no hay pyarrow
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")

            self.log_buffer = []

        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

```

### 2025-07-10: CORRECCIÓN SÓLIDA DE BLOQUE CLAVE

Te escribo solo la parte relevante de schema y normalize_data y el dict de ejemplo.

#### 1. TEMPLATE_METADATOS

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
TEMPLATE_METADATOS = {
    "title": "",
    "uuid": "",
    "tipo": "",
    "schema": "",
    "estado": "",
    "descripcion": "",
    "estructura": [],
    "tags": [],
    "linked_to": [],
    "responsable": [],
    "hash_verificacion": "",
    "historial": [],  # <-- Lista vacía
    "last_modified": "",
    "last_modified_by": "",
    "created_at": "",
    "created_by": "",
    "proceso_origen": "",
    "input_data": [],
    "output_data": [],
    "comentarios": "",
    "ia_metadata": {},
    "template_version": TEMPLATE_VERSION,
}
```

##### Bloque python historico

Bloque python que fue retirado.

```python
TEMPLATE_METADATOS = {
    "title": "",
    "uuid": "",
    "tipo": "",
    "schema": "",
    "estado": "",
    "descripcion": "",
    "estructura": [],
    "tags": [],
    "linked_to": [],
    "responsable": [],
    "hash_verificacion": "",
    "historial": [],
    "last_modified": "",
    "last_modified_by": "",
    "created_at": "",
    "created_by": "",
    "proceso_origen": "",
    "input_data": [],
    "output_data": [],
    "comentarios": "",
    "ia_metadata": {"dummy": None},
    "template_version": TEMPLATE_VERSION,
}
```

#### 2. get_metadata_struct mejorado

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(
            pa.struct([
                ("fecha", pa.string()),
                ("usuario", pa.string()),
                ("accion", pa.string()),
                ("descripcion", pa.string()),
            ])
        )),
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),
        ("output_data", pa.list_(pa.string())),
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])
```
##### Bloque python historico

Bloque python que fue retirado.

```python
def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(pa.string())),  # Asumido como lista de strings
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),  # Asumido como lista de strings
        ("output_data", pa.list_(pa.string())), # Asumido como lista de strings
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])
```


#### 3. normalize_data: asegurar listas de dicts

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)

        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}

        # Historial: asegurar lista de dicts
        if key == "historial" and val and isinstance(val, list):
            val = [
                {
                    "fecha": x.get("fecha", ""),
                    "usuario": x.get("usuario", ""),
                    "accion": x.get("accion", ""),
                    "descripcion": x.get("descripcion", "")
                }
                for x in val if isinstance(x, dict)
            ]

        # Conversión de tipos normales
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}

        normalized[key] = val

    return normalized
```

##### Bloque python historico

Bloque python que fue retirado.

```python
def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)
        
        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}
        
        # Conversión de tipos
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}
        
        normalized[key] = val
    
    return normalized
```

#### 4. En flush_logs

No cambies nada, solo asegurate de que la función normalize_data se usa así:

```python
norm_entry["metadata_before"] = normalize_data(
    entry.get("metadata_before", {}),
    TEMPLATE_METADATOS
)
norm_entry["metadata_after"] = normalize_data(
    entry.get("metadata_after", {}),
    TEMPLATE_METADATOS
)
```

#### 5. Se remplaza el principio de atomic_write


##### Bloque python historico

Bloque python que fue retirado.

```python
    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        backup_path = None 
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        backup_path = None
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if backup_path and os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if backup_path and os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass

```


# -------------------------------------
# ----------------Fin------------------
# -------------------------------------

# Development

## Ideas para proxima version :
 - [] Hacer que calcule el hash y mueva los archivos internamente manteniendo el hash y el indice
 - [] Indexar todo en Alma-Cli_inde.json
 - [] Poder hacer todos lso movimientos internos del sistema dejando registrado cada movimiento.
 - [] Que recorra periodicamente los archivos viendo si algo se modifico de lugar.
 - [] Definir funcion para estado para archivar y mover a historico con un script.
 - [] Integrar tests internos por modulo y funcion.

## Utilidades

### Templates (Python ,Yaml, Remplazo de bloques python)

#### Python

```python

```

#### Yaml

```yaml

```

#### Remplazo de bloques python:

##### Bloque python historico

Bloque python que fue retirado.

```python

```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python

```



### 2025-07-10 : Prompt para Kael Terminal cli

```yaml
# ================================================================
# PROMPT INTEGRADOR PARA GPT-4.5 - KAEL CLI ARCHITECTURE UPGRADE
# ================================================================

instrucciones:
  objetivo: |
    Vas a recibir abajo el YAML 'prompt_cleaner_v0.2.1.yaml' que describe la arquitectura y requerimientos de un CLI avanzado para ALMA_RESIST.
    Tu tarea es AUDITARLO y MEJORARLO siguiendo estas reglas y expandiéndolo en un nuevo YAML de arquitectura, bajo el nombre Prompt_Kael_v0.1.0.

  mejoras_y_reglas:
    - "Toda gestión y ejecución de comandos CLI debe ser orquestada desde: /home/alma/Alma-Cli/Alma.py (será el entrypoint y terminal CLI principal)."
    - "El archivo /home/alma/Alma-Cli/Kael/Kael.py funcionará como ADMINISTRADOR de comandos (backend CLI): toda la lógica de comandos y operaciones se ejecuta aquí, pero sólo a través de Alma.py."
    - "El terminal CLI INTERACTIVO y la UI están únicamente en Alma.py. Kael.py nunca debe llamar a Alma.py."
    - "El index global Alma_index.json DEBE estar en la raíz de /Alma-Cli/, ser el único punto de verdad para humanos, ML y agentes IA. Cada registro debe contener campo title y toda metadata necesaria para búsqueda semántica y trazabilidad."
    - "El sistema debe soportar:
        * Gestión total de .xlsx, .json, .yaml, .md, y futuros formatos.
        * Subíndices (por módulo o área), siempre referenciados en el índice global.
        * Limpieza automática: recorre todas las carpetas periódicamente, detecta archivos no gestionados por CLI, y los mueve a /invalidos/ (NO borra directo, solo mueve y loguea).
        * Log de TODO movimiento/acción en Parquet (IA/ML), YAML/JSON (parsing), y MD (legible para humanos).
        * Ejecución simultánea de múltiples comandos CLI sin pérdida de integridad/consistencia.
        * Creación de archivos en cualquier formato soportado con metadatos completos y auditables.
        * Políticas estrictas: cualquier archivo modificado fuera del CLI es movido a /invalidos/ y marcado como no confiable.
        * Tests unitarios y de integración por módulo y función crítica (carpeta tests/)."
    - "Toda arquitectura debe ser explícitamente compatible con integración de módulos ML, RAG y agentes IA autónomos. Todo log, índice y metadata debe ser 100% parseable y auditable por IA."
    - "Aclarar: Desde Prompt_Kael_v0.1.0, la arquitectura es: Alma.py = orquestador UI/CLI, Kael.py = backend de comandos, módulos totalmente desacoplados y testeables."
    - "El YAML que devuelvas debe ser un blueprint funcional, arquitectónico y operativo listo para implementación. NO devuelvas solo sugerencias, sino un YAML real de arquitectura, con todas las mejoras integradas."
    - "Sugerí cualquier otra mejora profesional para robustez, auditabilidad o futuro crecimiento (como hooks, triggers, exportación masiva, etc)."

  uso: |
    Luego de estas instrucciones, abajo se pegará el YAML prompt_cleaner_v0.2.1.yaml para que lo audites, mejores y devuelvas como Prompt_Kael_v0.1.0.
    Tu respuesta debe ser el nuevo YAML estructurado y documentado, sin preámbulos ni comentarios extra.

ejemplo_flujo:
  - "Usuario pega estas instrucciones (en YAML) y el YAML a mejorar (prompt_cleaner_v0.2.1.yaml)"
  - "El modelo GPT-4.5 procesa, audita y genera Prompt_Kael_v0.1.0.yaml listo para desarrollo real, versionado y modular"

# Fin de prompt para integración arquitectónica - KAEL CLI v0.1.0

# ====================================================
# prompt_cleaner_v0.2.1.yaml
# Documento de arquitectura y requerimientos avanzados
# ====================================================

proposito_general: |
  CLI modular, robusto y auditable para gestión, validación,
  trazabilidad y automatización avanzada de archivos en ALMA_RESIST.
  Escalable, con soporte extendido (XLSX, JSON, YAML, MD), hash persistente,
  indexado global/subíndices, terminal interactivo CLI, logs multi-formato y
  preparado para auditarse con IA/ML/RAG.

modularidad_recomendada:
  - main.py: "Entry point. Parser CLI, carga config, inicialización handlers."
  - config.py: "Carga y validación configuración general y específica."
  - metadata_validator.py: "Valida, normaliza y migra templates de metadatos."
  - metadata_handler.py: "IO robusto, extracción/escritura atómica, hashing, backups."
  - audit_logger.py: "Logs estructurados (Parquet, YAML, MD, JSON)."
  - index_manager.py: "Gestión índices global/subíndices, registro integridad."
  - file_watcher.py: "Monitoreo periódico movimientos/modificaciones archivos."
  - archive_manager.py: "Gestiona estado de archivo y migración histórica."
  - file_operations.py: "Funciones avanzadas: mover, copiar, borrar, renombrar."
  - cli_terminal.py: "Terminal CLI interactivo para seleccionar archivos y acciones."
  - commands/: "Cada comando separado modularmente para testing y escalabilidad."
  - tests/: "Tests unitarios e integración por módulo/función."

requerimientos_funcionales:
  validacion_y_normalizacion:
    descripcion: "Validación automática estructura/tipo metadatos. Migración de templates obsoletos."
  hash_integridad:
    descripcion: "SHA-256 hash persistente, verificable y actualizado cada cambio."
  indexado_global_y_subindices:
    descripcion: "Índice global (Alma_index.json) y soporte subíndices (por módulo o área)."
  registro_movimientos:
    descripcion: "Log estructurado de todos los movimientos (mover, copiar, borrar, renombrar)."
  monitoreo_periodico:
    descripcion: "Detección automática periódica de modificaciones externas o desincronizadas."
  archivado_y_estado:
    descripcion: "Gestión estados archivo: activo, histórico, archivado con registros completos."
  soporte_xlsx_json_yaml_md:
    descripcion: "Capacidad completa para manejar, extraer e indexar archivos XLSX, JSON, YAML y MD."
  logs_multiformato:
    descripcion: "Logs auditables en Parquet para IA/ML, YAML/JSON para parsing y MD legible para humanos."
  operaciones_avanzadas:
    descripcion: "Comandos avanzados para mover/copiar/borrar archivos con integridad asegurada."
  terminal_cli_interactivo:
    descripcion: "Terminal interactiva CLI para selección rápida de archivo y operación simultánea."
  ejecucion_simultanea_comandos:
    descripcion: "Capacidad de ejecutar múltiples comandos concurrentemente sin pérdida de integridad."
  testing_modular:
    descripcion: "Cobertura test automática por módulo y funcionalidad crítica."

estructura_index_json:
  path: "Alma_index.json"
  descripcion: "Índice global con subíndices opcionales para módulos internos."
  ejemplo_registro:
    uuid: "UUID del archivo"
    title: "documento.xlsx",
    ruta_actual: "/ruta/actual/documento.xlsx"
    hash: "sha256:abcdef..."
    estado: "activo | historico | archivado"
    last_modified: "timestamp ISO"
    responsables: ["usuario1", "usuario2"]
    linked_to: ["/ruta/otro.md", "/ruta/dato.json"]
    historial:
      - fecha: "2025-07-12T09:30:00Z"
        usuario: "bird"
        accion: "movimiento"
        descripcion: "Archivo movido desde /vieja/ruta.xlsx a /ruta/actual/documento.xlsx"
    version_metadata: "0.2.1"
    comentarios: ""

operaciones_archivos:
  mover:
    descripcion: "Mover archivo con hash persistente y registro completo."
  copiar:
    descripcion: "Copiar archivo manteniendo integridad/hash e indexado."
  borrar:
    descripcion: "Borrar archivo, actualización inmediata de index/logs."
  renombrar:
    descripcion: "Renombrar archivo y actualizar todas las referencias/index/logs."

terminal_cli_interactivo:
  descripcion: "CLI interactivo para navegación, selección visual de archivos y operaciones rápidas."
  funcionalidades:
    - Listar archivos por categoría o módulo.
    - Elegir operaciones (mover/copiar/borrar/renombrar/archivar).
    - Realizar operaciones simultáneas.

ciclo_monitoreo_periodico:
  frecuencia_default: "10 minutos (configurable)."
  acciones:
    - Verificar integridad/hash de cada archivo.
    - Detectar movimientos no registrados, alertar y registrar automáticamente.

manejo_archivo_historico:
  pasos:
    1. Validación existencia e integridad.
    2. Actualización estado en metadatos e índice.
    3. Movimiento físico al directorio histórico.
    4. Registro completo en logs e historial.

log_multiformato:
  formatos_soportados:
    - Parquet: para análisis IA/ML.
    - JSON/YAML: para ingestión y parsing automático.
    - Markdown (MD): para revisión humana.

roles_ia_ml_rag:
  descripcion: |
    Preparado explícitamente para integración con módulos internos/externalizados
    de ML, RAG y motores semánticos. Facilita parsing automático y auditable de logs e índices.

cambios_clave_v021_vs_anteriores:
  - Soporte XLSX/JSON adicional.
  - Terminal CLI interactiva.
  - Operaciones avanzadas (copiar, mover, borrar).
  - Ejecución simultánea comandos.
  - Subíndices independientes.
  - Logs auditables multiformato.

pendiente_recomendado:
  - Integración futura con asistentes inteligentes (Kael, Centralesis).
  - Documentación API completa.
  - Detección automática de archivos huérfanos.
  - Hooks externos (backup remoto, notificaciones).

# Fin de especificación YAML v0.2.1 - Alma_Cli_Cleaner

```


## 2025-07-10 : Prompt_Kael_v0.1.0.yaml

Fecha : 2025-07-10
Linked_to :
 - /home/alma/Alma-Cli/Kael/Documents/Prompts/Kael_v0.1.0/Kael_v0.1.0.yaml
Contenido :
 - Bloque yamal con prompt para Kael_v0.1.0

```yaml
# ====================================================
# Prompt_Kael_v0.1.0.yaml
# Arquitectura CLI avanzada para ALMA_RESIST
# ====================================================

arquitectura_general:
  descripcion: |
    CLI profesional, robusto y altamente modular para gestión integral, automatización,
    validación avanzada, auditoría continua y trazabilidad total de archivos en ALMA_RESIST.
    Escalable, compatible con IA/ML/RAG, con soporte extendido y multi-formato.

entrypoint_cli:
  ruta: "/home/alma/Alma-Cli/Alma.py"
  descripcion: |
    Orquestador principal. Terminal CLI interactivo, parser de comandos, interfaz única de usuario.
    Toda ejecución y comunicación ocurre desde aquí hacia Kael.py.

backend_comandos:
  ruta: "/home/alma/Alma-Cli/Kael/Kael.py"
  descripcion: |
    Backend dedicado exclusivamente a lógica y administración de comandos.
    Kael.py recibe solicitudes únicamente desde Alma.py, ejecuta operaciones atómicas.

estructura_directorios:
  - /home/alma/Alma-Cli/
    - Alma.py (CLI principal)
    - Alma_index.json (índice global)
    - Kael/
      - Kael.py (backend comandos)
      - commands/ (módulos separados y testeables)
      - handlers/ (IO, validación, hashing, operaciones atómicas)
      - validators/ (validación y normalización)
      - logs/ (almacén de logs multiformato)
      - file_watcher.py
      - tests/
    - invalidos/ (archivos no gestionados por CLI)

modularidad_detallada:
  Alma.py:
    - Terminal CLI interactivo
    - Parser comandos y argumentos
    - Comunicador exclusivo con Kael.py

  Kael.py:
    - Recepción segura y validada de comandos desde Alma.py
    - Lógica ejecución comandos
    - Orquestación módulos internos (handlers, validators, logs)

  handlers:
    - config_handler.py: Manejo de configuración global y específica
    - metadata_handler.py: Lectura/escritura atómica, hashing persistente, backups
    - file_operations.py: Operaciones avanzadas (copiar, mover, renombrar, borrar)
    - archive_manager.py: Archivado y migración histórica

  validators:
    - metadata_validator.py: Validador/normalizador estructuras metadata
    - integrity_checker.py: Verificación integridad hashes y metadatos

  logs:
    - audit_logger.py: Registro estructurado en Parquet, YAML/JSON, Markdown

  file_watcher.py:
    descripcion: |
      Monitoreo continuo, validación periódica de integridad/hash,
      detección movimientos externos, ejecución automática de políticas estrictas.
    frecuencia_default: "10 minutos (configurable)"

operaciones_soportadas:
  formatos:
    - XLSX
    - JSON
    - YAML
    - MD
  acciones:
    - mover
    - copiar
    - borrar
    - renombrar
    - archivar (cambio estado histórico)
  integridad:
    hash_persistente: "SHA-256 actualizado automáticamente por cada operación"

politicas_integridad:
  descripcion: |
    Archivo modificado externamente:
      1. Movido automáticamente a /invalidos/.
      2. Marcado como no confiable.
      3. Registro detallado en logs.

estructura_index_json:
  path: "/home/alma/Alma-Cli/Alma_index.json"
  descripcion: "Índice global único, subíndices referenciados por módulo/área"
  estructura_ejemplo:
    uuid: "UUID del archivo"
    title: "documento.xlsx"
    ruta_actual: "/ruta/actual/documento.xlsx"
    hash: "sha256:abcdef..."
    estado: "activo | historico | archivado"
    last_modified: "timestamp ISO"
    responsables: ["usuario1", "usuario2"]
    linked_to: ["/ruta/otro.md", "/ruta/dato.json"]
    historial:
      - fecha: "2025-07-12T09:30:00Z"
        usuario: "bird"
        accion: "movimiento"
        descripcion: "Archivo movido desde /vieja/ruta.xlsx a /ruta/actual/documento.xlsx"
    version_metadata: "0.2.1"
    comentarios: ""

logs_multiformato:
  formatos:
    - Parquet: "Optimizado para IA/ML/RAG"
    - YAML/JSON: "Para parsing y automatización"
    - Markdown: "Legibilidad para humanos"

testing:
  tipo:
    - Unitarios (por módulo)
    - Integración (por funciones críticas)
  ruta: "tests/"

compatibilidad_ml_ia:
  descripcion: |
    Estructura preparada para integración con:
      - Modelos ML locales o externos
      - Agentes autónomos IA
      - Motores semánticos y RAG

hooks_triggers_y_extensiones:
  sugerencias:
    - Hooks pre/post-operación
    - Triggers para backup remoto automático
    - Notificaciones push configurables
    - Exportación masiva periódica (archivado histórico)

mejoras_futuras:
  pendientes_recomendadas:
    - Documentación API REST interna
    - Detección automática de archivos huérfanos
    - Interfaz web básica para exploración índices

# Fin del YAML Prompt_Kael_v0.1.0 - Arquitectura avanzada CLI
```

## 2025-07-11 : Prompt_Kael_v0.1.1.yaml

Fecha : 2025-07-11
Linked_to :
 - /home/alma/Alma-Cli/Kael/Documents/Prompts/Kael_v0.1.1/Kael_v0.1.1.yaml
Contenido :
 - Bloque yamal con prompt para Kael_v0.1.0

```yaml
# ====================================================
# Kael_v0.1.1.yaml
# Arquitectura CLI avanzada para ALMA_RESIST
# ====================================================

arquitectura_general:
  descripcion: |
    CLI profesional, robusto y altamente modular para gestión integral, automatización,
    validación avanzada, auditoría continua y trazabilidad total de archivos en ALMA_RESIST.
    Escalable, compatible con IA/ML/RAG, con soporte extendido, multi-formato y manejo eficiente de archivos pesados.

entrypoint_cli:
  ruta: "/home/alma/Alma-Cli/Alma.py"
  descripcion: |
    Orquestador principal. Terminal CLI interactivo, parser de comandos, interfaz única de usuario.
    Toda ejecución y comunicación ocurre desde aquí hacia Kael.py.

backend_comandos:
  ruta: "/home/alma/Alma-Cli/Kael/Kael.py"
  descripcion: |
    Backend dedicado exclusivamente a lógica y administración de comandos.
    Kael.py recibe solicitudes únicamente desde Alma.py, ejecuta operaciones atómicas con validación estricta de permisos.

estructura_directorios:
  - /home/alma/Alma-Cli/
    - Alma.py (CLI principal)
    - Alma_index.json (índice global)
    - Kael/
      - Kael.py (backend comandos)
      - commands/ (módulos separados y testeables)
      - handlers/ (IO, validación, hashing, operaciones atómicas)
      - validators/ (validación y normalización)
      - logs/ (almacén de logs multiformato)
      - file_watcher.py
      - tests/
    - invalidos/ (archivos no gestionados por CLI)
    - Logs/ (almacén unificado de logs para todo el sistema)

modularidad_detallada:
  Alma.py:
    - Terminal CLI interactivo
    - Parser comandos y argumentos
    - Comunicador exclusivo con Kael.py

  Kael.py:
    - Recepción segura y validada de comandos desde Alma.py
    - Lógica ejecución comandos con validación explícita de permisos
    - Orquestación módulos internos (handlers, validators, logs)

  handlers:
    - config_handler.py: Manejo de configuración global y específica
    - metadata_handler.py: Lectura/escritura atómica, hashing persistente, backups
    - file_operations.py: Operaciones avanzadas (copiar, mover, renombrar, borrar, crear carpetas)
    - archive_manager.py: Archivado y migración histórica

  validators:
    - metadata_validator.py: Validador/normalizador estructuras metadata
    - integrity_checker.py: Verificación integridad hashes y metadatos

  logs:
    - audit_logger.py: Registro estructurado en Parquet (principal), YAML/JSON, Markdown

  file_watcher.py:
    descripcion: |
      Monitoreo continuo, validación periódica de integridad/hash,
      detección movimientos externos, ejecución automática de políticas estrictas.
    frecuencia_default: "10 minutos (configurable)"

operaciones_soportadas:
  formatos:
    - XLSX
    - JSON
    - YAML
    - MD
    - PDF (soporte nuevo)
  acciones:
    - mover
    - copiar
    - borrar
    - renombrar
    - archivar (cambio estado histórico)
    - crear carpeta (nuevo)
  integridad:
    hash_persistente: "SHA-256 actualizado automáticamente por cada operación"

politicas_integridad:
  descripcion: |
    Archivo modificado externamente:
      1. Movido automáticamente a /invalidos/.
      2. Marcado como no confiable.
      3. Registro detallado en logs con explicación manual explícita.

estructura_index_json:
  path: "/home/alma/Alma-Cli/Alma_index.json"
  descripcion: "Índice global único, subíndices referenciados por módulo/área"
  estructura_ejemplo:
    uuid: "UUID del archivo"
    title: "documento.xlsx"
    ruta_actual: "/ruta/actual/documento.xlsx"
    hash: "sha256:abcdef..."
    estado: "activo | historico | archivado"
    last_modified: "timestamp ISO"
    responsables: ["usuario1", "usuario2"]
    linked_to: ["/ruta/otro.md", "/ruta/dato.json"]
    historial:
      - fecha: "2025-07-12T09:30:00Z"
        usuario: "bird"
        accion: "movimiento"
        descripcion: "Archivo movido desde /vieja/ruta.xlsx a /ruta/actual/documento.xlsx"
    version_metadata: "0.2.1"
    comentarios: ""

logs_multiformato:
  formatos:
    - Parquet: "Optimizado para IA/ML/RAG, estructura base unificada para parseabilidad óptima"
    - YAML/JSON: "Para parsing y automatización"
    - Markdown: "Legibilidad para humanos"

base_datos_integracion:
  tipo: "SQLite"
  descripcion: "Base de datos SQLite para indexado global persistente, logs de operaciones críticas y registro completo"
  ruta: "/home/alma/Alma-Cli/Logs/alma_resist.db"

hooks_triggers_y_extensiones:
  sugerencias:
    - Hooks pre/post-operación
    - Triggers para backup remoto automático
    - Notificaciones push configurables
    - Exportación masiva periódica (archivado histórico)

mejoras_futuras:
  pendientes_recomendadas:
    - Documentación API REST interna
    - Detección automática de archivos huérfanos
    - Interfaz web básica para exploración índices
    - Manejo automático de archivos pesados con documentación obligatoria

# Fin del YAML Kael_v0.1.1 - Arquitectura avanzada CLI

```


### Auditoria realizada por : Deepseek

Fecha : 2025-07-11
Linked_to:
 - [Link]
Contenido : Prompt auditoria en bloque yaml

```yaml
# =========================================================
# PROMPT DE AUDITORÍA Y MEJORA CONTINUA PARA DEEPSEEK - KAEL CLI
# =========================================================

instrucciones:
  objetivo: |
    Recibirás el archivo 'Kael_v0.1.1.yaml' (blueprint arquitectónico del CLI de ALMA_RESIST) para que lo audites críticamente como experto en arquitecturas CLI, automatización documental y sistemas multiagente.  
    Tu tarea es:
      - Detectar cualquier mejora, debilidad, ambigüedad o área potencial de fallo.
      - Proponer e INTEGRAR mejoras reales de arquitectura, robustez, modularidad, auditabilidad y extensibilidad.
      - Profundizar en la definición y lógica de carpetas, módulos y flujos internos (que cualquier equipo o IA entienda la estructura y pueda auditarla).
      - Documentar los cambios con un **changelog en YAML**.
      - Presentar la nueva versión como 'Kael_v0.1.2.yaml', lista para implementación.

  requerimientos_mejora:
    - La respuesta debe incluir SOLO:
        1. Un nuevo YAML completo ('Kael_v0.1.2.yaml'), incorporando mejoras técnicas, explicando toda nueva decisión.
        2. Un changelog detallado en YAML (al pie).
        3. Una explicación visual y formal de la estructura de carpetas y su propósito, para que cualquier chat, humano o IA, comprenda al instante la lógica histórica y operativa del CLI.
    - Toda mejora debe priorizar: seguridad, auditabilidad, resiliencia, independencia de módulos, trazabilidad, facilidad de testing, y compatibilidad futura con IA/ML/RAG.
    - Sugerir e integrar cualquier feature profesional de nivel corporativo/empresarial que optimice el sistema.

  flujo_esperado:
    - Pegá abajo el archivo 'Kael_v0.1.1.yaml'
    - La respuesta esperada es: 
        - El YAML 'Kael_v0.1.2.yaml' mejorado y explicado
        - Changelog en YAML
        - Sección visual de estructura de carpetas y lógica modular

# Fin del prompt. Pegá abajo el YAML actual para auditar.
```

# ARQUITECTURA KAEL v0.1.2 (HEXAGONAL)

## Capas Principales
1. **ORQUESTACIÓN (Alma.py)**
   - Función: Interacción humano/sistema
   - Componentes: 
     - Parser de comandos
     - Sanitizador de inputs
     - Enrutador a servicios
   - Regla: Cero lógica de negocio

2. **SERVICIOS (Kael/core/)**
   - Función: Ejecución de casos de uso
   - Componentes:
     - Commands: Módulos atómicos (ej: `move_file.py`)
     - Services: Coordinación compleja (ej: `backup_service.py`)
   - Regla: Independiente de infraestructura

3. **DOMINIO (Kael/domain/)**
   - Función: Corazón del negocio
   - Componentes:
     - Models: Entidades (ej: `FileMetadata`)
     - Validators: Reglas empresariales (ej: `retention_policy_check`)
   - Regla: Cero dependencias externas

4. **INFRAESTRUCTURA (Kael/infrastructure/)**
   - Función: Conexión al mundo exterior
   - Componentes:
     - Handlers: IO local
     - Adapters: Sistemas externos (DBs, cloud, HSM)
   - Regla: Implementa interfaces definidas en dominio

## Flujo de Operación
1. Usuario ejecuta comando en `Alma.py`
2. Alma sanitiza input y envía solicitud a `Kael.py` via IPC
3. Kael.py:
   a. Valida permisos (RBAC)
   b. Crea transacción ACID
   c. Ejecuta servicio correspondiente
   d. Registra auditoría inmutable
4. Servicio:
   a. Usa modelos de dominio para lógica
   b. Delegua IO a adapters
   c. Aplica políticas
5. Resultado:
   - Operación + metadata → DB SQLite
   - Vector embeddings → DuckDB (para IA)
   - Logs → Parquet/Blockchain

## Ventajas Clave
- **Testeabilidad**: 
  - Capas aisladas → mocks sencillos
  - Core/domain prueban sin infra
- **Extensibilidad IA**:
  - Vector embeddings nativos
  - Puntos de inyección para modelos (ai_integration_service.py)
- **Seguridad**:
  - Zero-trust: Verificación en cada capa
  - Secrets: HSM/KMS via adapters
- **Resiliencia**:
  - Circuit-breaker en llamadas externas
  - Auto-reparación con versiones válidas
- **Auditabilidad**:
  - Trazabilidad end-to-end
  - Bitácora inmutable (append-only + hashing encadenado)

## Prompt : Kael_v0.1.2.yaml

Linked_to: /home/alma/Alma-Cli/Kael/Documents/Prompts/Kael_v0.1.2/Kael_v0.1.2.yaml

```yaml
# =========================================================
# ARQUITECTURA MEJORADA - KAEL CLI v0.1.2 (ALMA_RESIST)
# =========================================================
arquitectura_general:
  descripcion: |
    CLI empresarial con arquitectura hexagonal, enfocado en seguridad zero-trust, 
    trazabilidad completa y soporte nativo para IA/ML/RAG. Incluye capas de:
    - Orquestación
    - Lógica de negocio
    - Infraestructura (IO, redes, almacenamiento)
    - Auditoría

entrypoint_cli:
  ruta: "/home/alma/Alma-Cli/Alma.py"
  descripcion: |
    Capa de orquestación: Terminal interactiva con parser de comandos, 
    validación inicial y enrutamiento a servicios. Comunica EXCLUSIVAMENTE 
    con Kael.py via IPC seguro.

backend_comandos:
  ruta: "/home/alma/Alma-Cli/Kael/Kael.py"
  descripcion: |
    Núcleo de servicios: Ejecuta operaciones atómicas con verificación 
    de permisos RBAC (Role-Based Access Control). Expone API interna 
    para futura integración REST.

estructura_directorios:
  - /home/alma/Alma-Cli/
    - Alma.py
    - config.yaml (NUEVO: Configuración global)
    - Kael/
      - Kael.py
      - core/
        - commands/ (Módulos autocontenidos)
        - services/ (Lógica empresarial)
      - infrastructure/
        - handlers/ (IO, redes, almacenamiento)
        - adapters/ (Conexión a sistemas externos)
      - domain/
        - models/ (Entidades y objetos-valor)
        - validators/ (Reglas de negocio)
      - utils/
        - crypto_utils.py (NUEVO: Manejo seguro de claves)
        - error_handlers.py (NUEVO)
      - tests/ (Cobertura >90%)
    - managed_assets/ (REEMPLAZA 'invalidos/')
      - quarantined/ (Archivos no confiables)
      - archived/ (Retención histórica)
    - Logs/ (Centralizado)
      - audit/ (Parquet, JSON-LD)
      - diagnostics/ (Logs de sistema)

modularidad_detallada:
  Alma.py:
    - Terminal CLI interactiva
    - Sanitización de inputs (OWASP)
    - Enrutamiento a servicios

  Kael.py:
    - API de servicios (IPC/REST-ready)
    - Gestor de transacciones (ACID)
    - Control de concurrencia

  core/services:
    - transaction_service.py (NUEVO: Gestor ACID)
    - policy_service.py (NUEVO: Ejecución de políticas)
    - ai_integration_service.py (NUEVO: Puntos de extensión IA)

  infrastructure/adapters:
    - db_adapter.py (NUEVO: SQL/VectorDB)
    - remote_storage_adapter.py (NUEVO: S3/GCS)
    - crypto_adapter.py (NUEVO: HSM/KMS)

  domain/models:
    - FileMetadata.py (NUEVO: Entidad central)
    - AuditTrail.py (NUEVO: Objeto-valor inmutable)

  validators:
    - metadata_schema_validator.py (Con JSON-Schema)
    - integrity_validator.py (Post-quantum ready)

  utils:
    - crypto_utils.py: 
        - Key derivation: Argon2id
        - Cifrado: AES-GCM
    - error_handlers.py:
        - Retry automático
        - Circuit-breaker

operaciones_soportadas:
  formatos:
    - XLSX
    - JSON
    - YAML
    - MD
    - PDF
    - Parquet (NUEVO)
  acciones:
    - mover (con bitácora inmutable)
    - copiar (checksum en tiempo real)
    - borrar (soft-delete con retención)
    - archivar (WORM: Write-Once-Read-Many)
    - validate (NUEVO: Chequeo proactivo)
  integridad:
    hash: "BLAKE3 (NUEVO) + SHA-256 backup"

politicas_integridad:
  niveles:
    - LOW: Notificación
    - MEDIUM: Cuarentena automática
    - HIGH: Auto-reparación con backup
  acciones:
    - Detección cambio hash: Recuperación de versión válida
    - Cambio no autorizado: Rollback transaccional

estructura_metadatos:
  db_primaria: "SQLite (OLTP)"
  db_secundaria: "DuckDB (OLAP/IA)"
  schema_principal: |
    CREATE TABLE asset_metadata (
      id BLOB PRIMARY KEY,        -- UUIDv7
      title TEXT NOT NULL,
      path TEXT UNIQUE,
      hash_primary TEXT,          -- BLAKE3
      hash_secondary TEXT,        -- SHA-256
      status INTEGER CHECK(status IN (0,1,2,3)),  -- 0=active,1=archived,2=quarantined,3=deleted
      created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      modified TIMESTAMP,
      owner TEXT,
      access_control_list JSON,   -- RBAC fine-grained
      history JSON,               -- Inmutabilidad mediante append-only
      vector_embedding BLOB       -- Para RAG/ML
    );

logs_auditoria:
  formatos:
    - Parquet (Esquema: OpenTelemetry)
    - JSON-LD (Trazabilidad semántica)
  politicas:
    - Retención: 7 años
    - Inmutabilidad: Hashes en blockchain interna
  triggers:
    - POST_OPERATION: Log detallado
    - SECURITY_EVENT: Alerta en tiempo real

extensiones_empresariales:
  implementadas:
    - Hooks pre/post operación (plugins/)
    - API de eventos (Webhooks/PubSub)
    - Soporte multi-nube (S3, GCS, Azure)
    - Encriptación E2E (AES-256/GCM)
  futuras:
    - Zero-Knowledge Proofs (validación sin exposición)
    - Modo air-gapped (operación offline)

mejoras_futuras:
  roadmap:
    - Módulo FedRAMP compliance
    - Integración con SIEM (Splunk/ELK)
    - Auto-reparación basada en IA
    - Dashboard de salud del sistema
```

## Changelog v0.1.1 to v0.1.2

Linked_to : /home/alma/Alma-Cli/Kael/Documents/Prompts/Kael_v0.1.2/changelog.yaml

```yaml
changelog:
  version: "0.1.2"
  fecha: "2025-07-11"
  cambios:
    - seguridad:
        - Added: RBAC con ACL granular
        - Added: Encriptación E2E (AES-GCM)
        - Added: Hashing BLAKE3 (post-quantum ready)
        - Removed: SHA-256 como único hash
    - arquitectura:
        - Changed: Estructura hexagonal (core/infra/domain)
        - Added: Capa de adaptadores para infraestructura
        - Added: Modelos de dominio inmutables
    - operaciones:
        - Changed: "invalidos/" → "managed_assets/" con políticas WORM
        - Added: Transacciones ACID para operaciones críticas
        - Added: Auto-reparación de integridad
    - auditoria:
        - Added: Bitácora inmutable (append-only)
        - Added: Schemas OpenTelemetry/JSON-LD
        - Added: Blockchain interna para logs
    - escalabilidad:
        - Added: Soporte vector embeddings (RAG/ML)
        - Added: Integración multi-nube
        - Added: Circuit-breaker para resiliencia
    - mantenibilidad:
        - Added: Módulos autocontenidos (commands/)
        - Added: Error handling estandarizado
        - Added: Tests de integración (90%+ cobertura)
```

##

# -------------------------------------
# ----------------Fin------------------
# -------------------------------------

# Development

## Ideas para proxima version :
 - [] 
 - [] 
 - [] 
 - [] 
 - [] 
 - [] 

## Utilidades

# Templates (Python ,Yaml, Remplazo de bloques python)

#### Python

```python

```

#### Yaml

```yaml

```

#### Remplazo de bloques python:

##### Bloque python historico

Bloque python que fue retirado.

```python

```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python

```

### Auditoria realizada por :

Fecha :
Linked_to:
 - [Link]
Contenido : Prompt auditoria en bloque yaml

```yaml

```