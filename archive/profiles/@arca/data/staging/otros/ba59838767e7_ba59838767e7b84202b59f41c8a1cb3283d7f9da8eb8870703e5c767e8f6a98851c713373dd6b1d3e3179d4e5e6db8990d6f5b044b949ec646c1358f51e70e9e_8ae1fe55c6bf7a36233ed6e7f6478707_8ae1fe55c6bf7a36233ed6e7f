# 🧠 Guía de Uso – Servidor ALMA_RESIST

## ✅ Paso 1: Instalar dependencias necesarias

```bash
# Activa tu entorno virtual si no lo está
source .venv/bin/activate

# Instala las dependencias principales
pip install fastapi uvicorn llama-cpp-python python-multipart cryptography jsonschema
```


## 🧪 Endpoints principales

### 1. Generar respuestas (POST /responder)

```bash
curl -X POST "http://localhost:8000/responder" \
-H "Content-Type: application/json" \
-d '{"prompt": "¿Cuál es el sentido de la vida?"}'
```

Respuesta esperada:
```json
{
  "respuesta": "El sentido de la vida es...",
  "metadata": {
    "modelo": "mistral-7b-q4.gguf",
    "longitud_prompt": 28,
    "timestamp": "2023-10-15T12:34:56.789Z"
  }
}
```


### 3. Obtener historial de contexto (GET /context/history)

```bash
curl "http://localhost:8000/context/history?limit=3"
```


### 5. Cifrar logs (POST /logs/encrypt)

```bash
curl -X POST http://localhost:8000/logs/encrypt
```


## 📁 Estructura recomendada del proyecto

```
ALMA_RESIST/
├── core/
│   └── llm_server/
│       ├── models/              # Modelos GGUF
│       ├── transport_data/      # Datos de transporte
│       ├── docs/contracts/      # Esquemas JSON
│       └── logs/                # Logs cifrados
```


## 🛠 Troubleshooting

### Verificar dependencias:

```bash
pip list | grep -E 'fastapi|uvicorn|llama-cpp-python'
```

### Logs al iniciar:

```text
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [12345] using WatchFiles
INFO:     Started server process [12347]
INFO:     Waiting for application startup.
INFO:     alma_server: Modelo models/mistral-7b-q4.gguf cargado exitosamente
INFO:     Application startup complete.
```

