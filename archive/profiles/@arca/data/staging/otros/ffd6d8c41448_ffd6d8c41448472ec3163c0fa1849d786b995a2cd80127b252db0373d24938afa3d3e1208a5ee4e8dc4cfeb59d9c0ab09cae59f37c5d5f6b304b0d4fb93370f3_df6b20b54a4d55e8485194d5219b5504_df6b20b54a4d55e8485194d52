# 2025-07-10: "ALMA_CLI_CLEANER"

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.0"

Primera version solo prompt.

### prompt_alma_cli_cleaner_v0.1.0.yaml

Prompt primario sin construccion de script

```yaml
# prompt_alma_cli_cleaner_v0.1.0.yaml

titulo: "ALMA_CLI_CLEANER v0.1.0 - Prompt de Diseño y Especificación"
descripcion: |
  Módulo CLI centralizado para limpieza, validación, creación y auditoría de metadatos en el sistema ALMA_RESIST.
  El objetivo es contar con una herramienta robusta, auditable y fácil de mantener, que permita operar sobre cualquier archivo relevante del sistema desde la terminal.
  Debe ser usable por humanos y por IAs de soporte (ej: Kael), con una arquitectura lista para crecer.

objetivo_general: |
  Automatizar y profesionalizar el manejo de metadatos ALMA_RESIST, asegurando trazabilidad, control de cambios y cumplimiento de estándares en todos los archivos clave.

subcomandos:
  - crear:
      descripcion: "Crear archivo nuevo con metadatos completos y formato estándar."
      argumentos:
        - archivo: "Ruta de archivo a crear."
        - --title: "Título inicial del archivo."
        - --responsable: "Responsable(s) inicial(es)."
      requisitos: "Si el archivo existe, debe abortar salvo que se use --force."
      resultado: "Archivo creado con metadatos completos: title, uuid, historial, etc."
  - validar:
      descripcion: "Validar y/o reparar encabezado de metadatos (campos, tipos, valores)."
      argumentos:
        - archivo: "Ruta del archivo a validar."
      resultado: "Corrige y deja el archivo conforme al template. Reporta cambios y deja log."
  - limpiar:
      descripcion: "Normalizar campos, remover espacios, estandarizar tipos y formatos."
      argumentos:
        - archivo: "Ruta del archivo a limpiar."
      resultado: "Archivo con metadatos limpios, sin basura ni valores inválidos."
  - set_responsable:
      descripcion: "Asignar o modificar el/los responsable(s) y actualizar auditoría."
      argumentos:
        - archivo: "Ruta del archivo."
        - --responsable: "Nuevo(s) responsable(s)."
      resultado: "Campo responsable actualizado y log/auditoría de cambio."
  - log:
      descripcion: "Mostrar historial/auditoría/log de cambios sobre el archivo."
      argumentos:
        - archivo: "Ruta del archivo."
      resultado: "Listado de operaciones previas, quién y cuándo realizó cambios."

features_extra:
  - "Soporte para .md, .yaml, .py (bloque YAML en docstring)."
  - "Logs centralizados y auditables en formato parquet/csv/yaml (según config)."
  - "Integración futura con CLI multiagente ALMA (Kael, Centralesis, etc)."
  - "Resiliencia ante errores, dry-run, y opción --force para operaciones críticas."
  - "Puede crecer modular: desde un solo script hasta un paquete completo."

notas:
  - "No hace falta implementar lógica compleja todavía, solo la estructura y flujo CLI."
  - "La documentación interna (docstrings) debe ser clara y profesional."
  - "Prioridad: trazabilidad, simplicidad y facilidad de extensión."
  - "El sistema debe ser usable tanto en CLI local como desde terminal remota/SSH."

pendientes:
  - Definir template de metadatos único y documentado.
  - Ejemplo de uso para cada subcomando.
  - Especificar estructura de logs y niveles de detalle.
```

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.1"

Sin script por ahora.

### prompt_alma_cli_cleaner_v0.1.1.yaml

Prompt para v0.1.1

```yaml
titulo: "ALMA_CLI_CLEANER v0.1.1 - Prompt de Diseño y Especificación Mejorado"
descripcion: |
  Módulo CLI centralizado para limpieza, validación, creación y auditoría de metadatos en el sistema ALMA_RESIST.
  El objetivo es contar con una herramienta robusta, auditable y fácil de mantener, que permita operar sobre cualquier archivo relevante del sistema desde la terminal.
  Usable por humanos y por IAs (Kael, Centralesis), con arquitectura extensible.

autores:
  - nombre: Santi (Bird)
    rol: Arquitecto, usuario principal
  - nombre: Alma
    rol: IA soporte y documentación
mantenedores:
  - nombre: Kael
    rol: Agente IA de auditoría y CLI
  - nombre: Centralesis
    rol: Agente IA de control central

changelog:
  - version: 0.1.1
    fecha: 2025-07-10
    autor: Santi
    descripcion: Se incorporan especificaciones avanzadas de contexto, retrocompatibilidad, roles, política de errores y hooks.

objetivo_general: |
  Automatizar y profesionalizar el manejo de metadatos ALMA_RESIST, asegurando trazabilidad, control de cambios y cumplimiento de estándares en todos los archivos clave.

contexto_operativo:
  ejecutores_permitidos:
    - humano
    - IA (Kael)
    - IA (Centralesis)
  permisos_por_subcomando:
    crear: [humano, Kael]
    validar: [humano, Kael, Centralesis]
    limpiar: [humano, Kael]
    set_responsable: [humano, Kael]
    log: [humano, Kael, Centralesis]
  compatibilidad_minima: 0.1.0
  templates_requeridos:
    - Meta/Metadatos_Template.yaml

archivos_soportados:
  - ".md"
  - ".yaml"
  - ".yml"
  - ".py"
  # Si otro tipo => warning + no ejecuta acción.

migraciones:
  politica_retrocompatibilidad: |
    Si el archivo tiene template viejo, sugerir migración automática o abortar con warning. Registrar en el log cualquier intervención de migración.
  version_template_minima: 0.1.0

configuracion:
  archivo: alma_cleaner_config.yaml
  logs:
    ruta: /home/alma/Alma-Cli/Logs/Cleaner.parquet
    nivel: INFO # Configurable: DEBUG, INFO, WARNING, ERROR
    formato: parquet # Opción futura: csv/yaml
    loguear_errores: true

politica_errores:
  modo: strict # Opciones: strict (aborta), relaxed (warning, sigue)
  log_si_fallo: true
  mensaje_usuario: "Error crítico, abortando. Ver logs."

hooks:
  pre_comando: null # Permitir definir scripts o funciones hook antes de operar (ej. backup automático)
  post_comando: null

subcomandos:
  - crear:
      descripcion: "Crear archivo nuevo con metadatos completos y formato estándar."
      argumentos:
        - archivo: "Ruta de archivo a crear."
        - --title: "Título inicial del archivo."
        - --responsable: "Responsable(s) inicial(es)."
      requisitos: "Si el archivo existe, debe abortar salvo que se use --force."
      resultado: "Archivo creado con metadatos completos: title, uuid, historial, etc."
      ejemplo: |
        alma_cli_cleaner crear /ruta/archivo.md --title "Bitácora 13CC" --responsable "Bird"
  - validar:
      descripcion: "Validar y/o reparar encabezado de metadatos (campos, tipos, valores)."
      argumentos:
        - archivo: "Ruta del archivo a validar."
      resultado: "Corrige y deja el archivo conforme al template. Reporta cambios y deja log."
      ejemplo: |
        alma_cli_cleaner validar /ruta/archivo.yaml
  - limpiar:
      descripcion: "Normalizar campos, remover espacios, estandarizar tipos y formatos."
      argumentos:
        - archivo: "Ruta del archivo a limpiar."
      resultado: "Archivo con metadatos limpios, sin basura ni valores inválidos."
      ejemplo: |
        alma_cli_cleaner limpiar /ruta/archivo.md
  - set_responsable:
      descripcion: "Asignar o modificar el/los responsable(s) y actualizar auditoría."
      argumentos:
        - archivo: "Ruta del archivo."
        - --responsable: "Nuevo(s) responsable(s)."
      resultado: "Campo responsable actualizado y log/auditoría de cambio."
      ejemplo: |
        alma_cli_cleaner set_responsable /ruta/archivo.md --responsable "Centralesis"
  - log:
      descripcion: "Mostrar historial/auditoría/log de cambios sobre el archivo."
      argumentos:
        - archivo: "Ruta del archivo."
      resultado: "Listado de operaciones previas, quién y cuándo realizó cambios."
      ejemplo: |
        alma_cli_cleaner log /ruta/archivo.md

features_extra:
  - "Soporte para .md, .yaml, .py (bloque YAML en docstring)."
  - "Logs centralizados y auditables en formato parquet/csv/yaml (según config)."
  - "Integración futura con CLI multiagente ALMA (Kael, Centralesis, etc)."
  - "Resiliencia ante errores, dry-run, y opción --force para operaciones críticas."
  - "Hooks pre/post comando para backups, validaciones externas, etc."
  - "Puede crecer modular: desde un solo script hasta un paquete completo."
  - "API de retorno: stdout estándar y opción de salida JSON para integración."

ia_metadata:
  reservados_para_ia: true
  comentarios: |
    Espacio para que IAs futuras de ALMA (Kael, Centralesis, LLMs) anoten sugerencias, alertas o refactors automáticos sobre la operación y código base.
  pendiente:
    - Integrar feedback de agentes externos en la lógica de auditoría y control.
    - Implementar análisis semántico del historial y proponer mejoras al template.

ejemplos_de_uso:
  - crear nuevo archivo: |
      alma_cli_cleaner crear /path/to/file.md --title "Bitácora" --responsable "Bird"
  - validar archivo existente: |
      alma_cli_cleaner validar /path/to/file.yaml
  - limpiar archivo: |
      alma_cli_cleaner limpiar /path/to/file.md
  - asignar responsable: |
      alma_cli_cleaner set_responsable /path/to/file.md --responsable "Kael"
  - mostrar historial/log: |
      alma_cli_cleaner log /path/to/file.md

notas:
  - "No hace falta implementar toda la lógica compleja todavía: arrancar con la estructura y el flujo CLI base."
  - "Documentar internamente (docstrings) cada función/acción de manera clara y profesional."
  - "Prioridad: trazabilidad, simplicidad y facilidad de extensión."
  - "El sistema debe ser usable tanto en CLI local como desde terminal remota/SSH."
  - "Si se detecta archivo binario o no soportado: warning y aborta sin modificar."
  - "Proveer siempre código de salida 0=OK, 1=error crítico."

pendientes:
  - Definir template de metadatos único y documentado.
  - Especificar estructura de logs y niveles de detalle.
  - Migrador automático para templates viejos.
  - Changelog documentado desde la v0.1.0.
  - Pruebas de edge-cases y documentación de errores comunes.
  - Integrar feedback de agentes IA en auditoría.
```

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.2"

Link :
 - /home/alma/Alma-Cli/Cleaner/Prompts/History/Alma_Cli_Cleaner_v0.1.0_to_v0.1.4/Alma_Cli_Cleaner_v0.1.2.yaml

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.3"

Link :
 - /home/alma/Alma-Cli/Cleaner/Prompts/History/Alma_Cli_Cleaner_v0.1.0_to_v0.1.4/Alma_Cli_Cleaner_v0.1.3.yaml
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.3/Alma_Cli_Cleaner_v0.1.3.py
  

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.4"

Link :
 - /home/alma/Alma-Cli/Cleaner/Prompts/History/Alma_Cli_Cleaner_v0.1.0_to_v0.1.4/Alma_Cli_Cleaner_v0.1.4.yaml
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.4/Alma_Cli_Cleaner_v0.1.4.py

### Auditoria Tecnica "ALMA_CLI_CLEANER v0.1.4" (bloque yaml)

```yaml
auditoria_tecnica:
  script: ALMA_CLI_CLEANER
  version: "0.1.4"
  fecha_auditoria: "2025-07-10"
  auditor: IA (ChatGPT)

fortalezas:
  - Uso correcto de ruamel.yaml para preservar estructura YAML.
  - Implementación de escritura atómica mediante archivos temporales.
  - Buen manejo básico de dry-run real.
  - Validaciones robustas iniciales (tipos, campos obligatorios).
  - Preparado para integración futura con IA (salida JSON opcional).

problemas_criticos_bloqueantes:
  - metodo_duplicado_atomic_write:
      descripcion: "El método 'atomic_write' está duplicado en MetadataHandler."
      riesgo: alto
  - extraccion_yaml_incorrecta:
      descripcion: "Retorno inconsistente en extracción YAML (método _extract_from_yaml)."
      riesgo: alto
  - manejo_docstrings_python:
      descripcion: "Riesgo de corrupción al reemplazar bloques YAML en archivos .py si no encuentra patrón exacto."
      riesgo: alto
  - subcomando_log_parametros:
      descripcion: "Falta pasar el parámetro 'executor' en el subcomando log en main()."
      riesgo: alto

riesgos_adicionales:
  - race_conditions:
      descripcion: "Falta de mecanismos de file-locking."
      riesgo: medio
  - validacion_migracion_templates:
      descripcion: "Migración de templates antiguos sin advertencias claras."
      riesgo: medio
  - configuracion_hardcoded:
      descripcion: "Tamaño máximo del archivo no configurable externamente."
      riesgo: medio

refactors_criticos:
  - eliminar_duplicacion_atomic_write:
      accion: "Conservar solo una versión simplificada."
  - refactorizar_extraccion_yaml:
      accion: "Corregir retorno en método _extract_from_yaml."
  - manejo_robusto_docstrings:
      accion: "Implementar validación de existencia antes de reemplazar bloques YAML."
  - corregir_subcomando_log:
      accion: "Corregir llamada en main() para pasar correctamente parámetro 'executor'."

mejoras_seguridad_extensibilidad:
  - archivo_config_externo:
      descripcion: "Permitir configuración de MAX_FILE_SIZE y extensiones desde archivo YAML externo."
  - file_locking:
      descripcion: "Implementar mecanismo de bloqueo de archivos para escrituras concurrentes seguras."
  - rollback_automatico:
      descripcion: "Restaurar automáticamente archivo original en caso de errores durante escritura."
  - validacion_logs:
      descripcion: "Validar esquema de logs antes de escribir para evitar corrupción."

comportamientos_inesperados:
  - dry_run_force:
      descripcion: "La combinación --dry-run y --force no está claramente definida."
      recomendacion: "Priorizar --dry-run y emitir advertencia."

checklist_todo_preproduccion:
  - "Eliminar método 'atomic_write' duplicado."
  - "Refactorizar extracción YAML."
  - "Implementar validación robusta de docstrings en archivos Python."
  - "Corregir llamada al subcomando log en main()."
  - "Agregar mecanismo de file-locking."
  - "Implementar rollback automático robusto."
  - "Configurar tamaño máximo del archivo desde archivo externo YAML."
  - "Mejorar validación en migración de templates antiguos."
  - "Definir claramente comportamiento de flags combinadas (--dry-run y --force)."

conclusion:
  estado: "Requiere refactors críticos antes de producción."
  comentario: "El script es sólido pero presenta errores críticos que deben ser corregidos urgentemente antes de desplegar."
auditoria_tecnica:
  script: ALMA_CLI_CLEANER
  version: "0.1.4"
  fecha_auditoria: "2025-07-10"
  auditor: IA (ChatGPT)

fortalezas:
  - Uso correcto de ruamel.yaml para preservar estructura YAML.
  - Implementación de escritura atómica mediante archivos temporales.
  - Buen manejo básico de dry-run real.
  - Validaciones robustas iniciales (tipos, campos obligatorios).
  - Preparado para integración futura con IA (salida JSON opcional).

problemas_criticos_bloqueantes:
  - metodo_duplicado_atomic_write:
      descripcion: "El método 'atomic_write' está duplicado en MetadataHandler."
      riesgo: alto
  - extraccion_yaml_incorrecta:
      descripcion: "Retorno inconsistente en extracción YAML (método _extract_from_yaml)."
      riesgo: alto
  - manejo_docstrings_python:
      descripcion: "Riesgo de corrupción al reemplazar bloques YAML en archivos .py si no encuentra patrón exacto."
      riesgo: alto
  - subcomando_log_parametros:
      descripcion: "Falta pasar el parámetro 'executor' en el subcomando log en main()."
      riesgo: alto

riesgos_adicionales:
  - race_conditions:
      descripcion: "Falta de mecanismos de file-locking."
      riesgo: medio
  - validacion_migracion_templates:
      descripcion: "Migración de templates antiguos sin advertencias claras."
      riesgo: medio
  - configuracion_hardcoded:
      descripcion: "Tamaño máximo del archivo no configurable externamente."
      riesgo: medio

refactors_criticos:
  - eliminar_duplicacion_atomic_write:
      accion: "Conservar solo una versión simplificada."
  - refactorizar_extraccion_yaml:
      accion: "Corregir retorno en método _extract_from_yaml."
  - manejo_robusto_docstrings:
      accion: "Implementar validación de existencia antes de reemplazar bloques YAML."
  - corregir_subcomando_log:
      accion: "Corregir llamada en main() para pasar correctamente parámetro 'executor'."

mejoras_seguridad_extensibilidad:
  - archivo_config_externo:
      descripcion: "Permitir configuración de MAX_FILE_SIZE y extensiones desde archivo YAML externo."
  - file_locking:
      descripcion: "Implementar mecanismo de bloqueo de archivos para escrituras concurrentes seguras."
  - rollback_automatico:
      descripcion: "Restaurar automáticamente archivo original en caso de errores durante escritura."
  - validacion_logs:
      descripcion: "Validar esquema de logs antes de escribir para evitar corrupción."

comportamientos_inesperados:
  - dry_run_force:
      descripcion: "La combinación --dry-run y --force no está claramente definida."
      recomendacion: "Priorizar --dry-run y emitir advertencia."

checklist_todo_preproduccion:
  - "Eliminar método 'atomic_write' duplicado."
  - "Refactorizar extracción YAML."
  - "Implementar validación robusta de docstrings en archivos Python."
  - "Corregir llamada al subcomando log en main()."
  - "Agregar mecanismo de file-locking."
  - "Implementar rollback automático robusto."
  - "Configurar tamaño máximo del archivo desde archivo externo YAML."
  - "Mejorar validación en migración de templates antiguos."
  - "Definir claramente comportamiento de flags combinadas (--dry-run y --force)."

conclusion:
  estado: "Requiere refactors críticos antes de producción."
  comentario: "El script es sólido pero presenta errores críticos que deben ser corregidos urgentemente antes de desplegar."
prompt_refactorizacion:
  titulo: "Prompt de Refactorización para ALMA_CLI_CLEANER v0.1.5"
  descripcion: |
    Solicitud de refactorización crítica del script ALMA_CLI_CLEANER versión 0.1.4. El objetivo es implementar mejoras específicas, claras y robustas, resultando en una nueva versión (0.1.5).
```

### "Prompt de Refactorización para ALMA_CLI_CLEANER to v0.1.5" (bloque yaml)

```yaml
prompt_refactorizacion:
  titulo: "Prompt de Refactorización para ALMA_CLI_CLEANER v0.1.5"
  descripcion: |
    Solicitud de refactorización crítica del script ALMA_CLI_CLEANER versión 0.1.4. El objetivo es implementar mejoras específicas, claras y robustas, resultando en una nueva versión (0.1.5).

  objetivos_refactorizacion:
    - Aplicar todas las reformas críticas identificadas en la auditoría técnica anterior.
    - Mantener el código legible, limpio y modular.
    - Evitar complejidades innecesarias o sobreingeniería.
    - Devolver un único archivo Python listo para ejecución desde CLI.

  mejoras_requeridas:
    - Eliminar método duplicado `atomic_write` conservando una versión simplificada.
    - Corregir método `_extract_from_yaml` para retorno consistente.
    - Robustecer extracción y reemplazo de metadatos YAML en docstrings Python (.py).
    - Corregir subcomando `log` en main() incluyendo el parámetro `executor`.
    - Implementar configuración externa (YAML) para parámetros críticos como `MAX_FILE_SIZE`.
    - Agregar mecanismo básico de file-locking para prevenir race conditions.
    - Clarificar comportamiento cuando se combinan `--dry-run` y `--force` (priorizar `--dry-run`).
    - Implementar rollback automático en caso de error durante escrituras.
    - Mejorar validación en migración de templates antiguos con advertencias claras en logs.

  entregables:
    - Archivo Python completo del script ALMA_CLI_CLEANER versión 0.1.5 mejorado, listo para ejecutar.
    - Explicación detallada de los cambios aplicados en formato YAML estructurado.

  formato_respuesta:
    codigo:
      nombre_archivo: "alma_cli_cleaner_v0.1.5.py"
      descripcion: "Script Python completo, refactorizado y mejorado según requisitos."
    documentacion:
      nombre_archivo: "cambios_aplicados_v0.1.5.yaml"
      descripcion: "Explicación YAML detallada de cada cambio y mejora aplicada."

  notas_importantes:
    - "A continuación de este prompt se adjunta el código completo del script actual (versión 0.1.4). Utiliza este código para razonar y aplicar todas las mejoras requeridas."
    - "Mantén una alta calidad de código y comenta claramente cada función y modificación realizada."
    - "Evita soluciones complejas o difíciles de mantener. Priorizamos robustez, claridad y simplicidad."

codigo_a_refactorizar: |
```

### Script: "Alma_Cli_Cleaner_v0.1.4.py" (bloque python)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =========================================
# ALMA_CLI_CLEANER v0.1.4
# CLI profesional para gestión de metadatos ALMA_RESIST
# =========================================

import os
import sys
import re
import argparse
import uuid
import json
import csv
import datetime
import logging
import hashlib
import shutil
import tempfile
import io
import traceback
from typing import List, Dict, Any, Optional, Tuple, Union
import ruamel.yaml
from ruamel.yaml.comments import CommentedMap
from ruamel.yaml.scalarstring import PreservedScalarString

# =========================================
# CONSTANTES GLOBALES Y CONFIGURACIÓN BASE
# =========================================

VERSION = "0.1.4"
TEMPLATE_VERSION = "0.1.3"
MIN_COMPATIBLE_VERSION = "0.1.0"
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
SUPPORTED_EXTENSIONS = [".md", ".yaml", ".yml", ".py"]
DEFAULT_CONFIG = {
    "logs": {
        "ruta": "/home/alma/Alma-Cli/Logs/Cleaner.parquet",
        "nivel": "INFO",
        "formato": "parquet",
        "loguear_errores": True
    },
    "politica_errores": "strict",
    "advertencia_tamano": 1048576,  # 1MB
    "hash_algoritmo": "sha256"
}
TEMPLATE_METADATOS = CommentedMap({
    "version": TEMPLATE_VERSION,
    "template_version": TEMPLATE_VERSION,
    "title": "",
    "uuid": "",
    "tipo": "",
    "schema": "",
    "estado": "",
    "descripcion": "",
    "estructura": [],
    "tags": [],
    "linked_to": [],
    "responsable": [],
    "hash_verificacion": "",
    "historial": [],
    "last_modified": "",
    "last_modified_by": "",
    "created_at": "",
    "created_by": "",
    "proceso_origen": "",
    "input_data": [],
    "output_data": [],
    "comentarios": "",
    "ia_metadata": {}
})

# Inicializar ruamel.yaml
yaml = ruamel.yaml.YAML()
yaml.preserve_quotes = True
yaml.indent(mapping=2, sequence=4, offset=2)
yaml.width = 120

# =========================================
# ESTRUCTURAS DE DATOS Y VALIDACIONES
# =========================================

class MetadataValidator:
    """Validador y normalizador de estructuras de metadatos"""
    
    REQUIRED_FIELDS = ["title", "uuid", "created_at", "created_by"]
    FIELD_TYPES = {
        "title": str,
        "uuid": str,
        "tipo": str,
        "schema": str,
        "estado": str,
        "descripcion": str,
        "estructura": list,
        "tags": list,
        "linked_to": list,
        "responsable": list,
        "hash_verificacion": str,
        "historial": list,
        "last_modified": str,
        "last_modified_by": str,
        "created_at": str,
        "created_by": str,
        "proceso_origen": str,
        "input_data": list,
        "output_data": list,
        "comentarios": str,
        "ia_metadata": dict
    }

    @staticmethod
    def validate(metadata: Dict) -> Tuple[bool, List[str]]:
        """Valida estructura básica y tipos de metadatos"""
        errors = []
        
        # Verificar campos obligatorios
        for field in MetadataValidator.REQUIRED_FIELDS:
            if field not in metadata:
                errors.append(f"Campo obligatorio faltante: {field}")
        
        # Validar tipos de datos
        for field, expected_type in MetadataValidator.FIELD_TYPES.items():
            if field in metadata:
                if expected_type is str and isinstance(metadata[field], PreservedScalarString):
                    continue
                if not isinstance(metadata[field], expected_type):
                    errors.append(f"Tipo incorrecto para {field}. Esperado: {expected_type.__name__}")
        
        # Verificar compatibilidad de versión
        template_ver = metadata.get("template_version", "0.0.0")
        if template_ver < MIN_COMPATIBLE_VERSION:
            errors.append(f"Versión de template incompatible: {template_ver} < {MIN_COMPATIBLE_VERSION}")
        
        return len(errors) == 0, errors

    @staticmethod
    def normalize(metadata: Dict) -> Dict:
        """Normaliza y limpia los valores de los metadatos"""
        normalized = metadata.copy()
        
        # Normalizar listas
        list_fields = ["tags", "linked_to", "responsable", "input_data", "output_data"]
        for field in list_fields:
            if field in normalized:
                if isinstance(normalized[field], str):
                    normalized[field] = [item.strip() for item in normalized[field].split(",")]
                normalized[field] = [str(item).strip() for item in normalized[field] if item]
        
        # Normalizar strings
        string_fields = ["title", "tipo", "schema", "estado", "descripcion", "comentarios"]
        for field in string_fields:
            if field in normalized:
                if not isinstance(normalized[field], PreservedScalarString):
                    normalized[field] = str(normalized[field]).strip()
                
        # Asegurar historial correcto
        if "historial" in normalized and not isinstance(normalized["historial"], list):
            normalized["historial"] = []
            
        return normalized

    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()
        
        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }
        
        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
        
        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            migrated.pop(field, None)
        
        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated

# =========================================
# MANEJO DE ARCHIVOS Y METADATOS
# =========================================

class MetadataHandler:
    """Manejador de operaciones con metadatos en archivos"""
    
    @staticmethod
    def is_binary_file(file_path: str) -> bool:
        """Detecta si un archivo es binario"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\0' in chunk
        except Exception:
            return True

    @staticmethod
    def extract_metadata(file_path: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos de diferentes tipos de archivos"""
        ext = os.path.splitext(file_path)[1].lower()
        content = ""
        
        # Verificar si es binario
        if MetadataHandler.is_binary_file(file_path):
            raise ValueError(f"Archivo binario detectado: {file_path}")
        
        # Verificar tamaño
        file_size = os.path.getsize(file_path)
        if file_size > MAX_FILE_SIZE:
            raise ValueError(f"Archivo demasiado grande: {file_size} bytes > {MAX_FILE_SIZE}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            raise ValueError("Archivo no es texto válido (UTF-8)")
        except Exception as e:
            raise IOError(f"Error leyendo archivo: {str(e)}")
        
        if ext == ".md":
            return MetadataHandler._extract_from_markdown(content)
        elif ext in [".yaml", ".yml"]:
            return MetadataHandler._extract_from_yaml(content)
        elif ext == ".py":
            return MetadataHandler._extract_from_python(content)
        else:
            raise ValueError(f"Formato no soportado: {ext}")

    @staticmethod
    def _extract_from_markdown(content: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos YAML de archivos Markdown"""
        match = re.search(r'^---\n(.+?)\n---\n(.*)', content, re.DOTALL)
        if match:
            try:
                metadata = yaml.load(match.group(1))
                content_body = match.group(2)
                return metadata, content_body
            except Exception as e:
                raise ValueError(f"Error analizando YAML: {str(e)}")
        return None, content

    @staticmethod
    def _extract_from_yaml(content: str) -> Tuple[Optional[Dict], str]:
        """Carga todo el contenido YAML como metadatos"""
        try:
            return yaml.load(content), ""
        except Exception as e:
            raise ValueError(f"Error analizando YAML: {str(e)}")

    @staticmethod
    def _extract_from_python(content: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos de docstrings en Python"""
        # Busca docstrings con metadatos en cualquier posición
        match = re.search(r'(\'\'\'|\"\"\")\s*?---\n(.+?)\n---\n(.*?)(\1)', content, re.DOTALL)
        if match:
            try:
                metadata = yaml.load(match.group(2))
                return metadata, content
            except Exception as e:
                raise ValueError(f"Error analizando YAML en docstring: {str(e)}")
        return None, content

    @staticmethod
    def calculate_hash(content: str) -> str:
        """Calcula hash de contenido para verificación"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    @staticmethod
    def atomic_write(file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal"""
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return
            
        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # Escribir a archivo temporal
            with tempfile.NamedTemporaryFile(
                mode='w', 
                encoding='utf-8', 
                delete=False,
                dir=os.path.dirname(file_path)
            ) as tmp_file:
                tmp_file.write(content)
                tmp_path = tmp_file.name
            
            # Reemplazar archivo original
            shutil.move(tmp_path, file_path)
        except Exception as e:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)
            raise IOError(f"Error en escritura atómica: {str(e)}")

    @staticmethod
    def write_metadata(
        file_path: str, 
        metadata: Dict, 
        content_body: str = "", 
        original_content: str = "",
        dry_run: bool = False
    ) -> str:
        """Escribe metadatos en diferentes formatos de archivo"""
        ext = os.path.splitext(file_path)[1].lower()
        content = ""
        
        try:
            if ext == ".md":
                # Convertir a CommentedMap para preservar orden
                yaml_metadata = CommentedMap(metadata)
                stream = io.StringIO()
                yaml.dump(yaml_metadata, stream)
                yaml_text = stream.getvalue()
                content = f"---\n{yaml_text}---\n{content_body}"
                
            elif ext in [".yaml", ".yml"]:
                yaml_metadata = CommentedMap(metadata)
                yaml_content = ruamel.yaml.comments.CommentedSeq()
                yaml.dump(yaml_metadata, yaml_content)
                content = str(yaml_content)
                
            elif ext == ".py":
                if original_content:
                    # Reemplazar solo el bloque de metadatos
                    content = re.sub(
                        r'(\'\'\'|\"\"\")\s*?---\n(.+?)\n---\n(.*?)(\1)',
                        f'\\1---\n{yaml.dump(metadata)}\n---\\3\\4',
                        original_content,
                        flags=re.DOTALL,
                        count=1
                    )
                else:
                    content = original_content
            else:
                raise ValueError(f"Formato no soportado: {ext}")
            
            # Calcular y actualizar hash
            new_hash = MetadataHandler.calculate_hash(content)
            metadata["hash_verificacion"] = new_hash
            
            # Escribir de forma atómica
            MetadataHandler.atomic_write(file_path, content, dry_run)
            return new_hash
                
        except Exception as e:
            raise IOError(f"Error escribiendo metadatos: {str(e)}")

    @staticmethod
    def atomic_write(file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal"""
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            dir_name = os.path.dirname(file_path)
            if dir_name:
                os.makedirs(dir_name, exist_ok=True)
            # Escribir a archivo temporal
            with tempfile.NamedTemporaryFile(
                mode='w',
                encoding='utf-8',
                delete=False,
                dir=dir_name if dir_name else "."
            ) as tmp_file:
                tmp_file.write(content)
                tmp_path = tmp_file.name

            shutil.move(tmp_path, file_path)
        except Exception as e:
            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)
            raise IOError(f"Error en escritura atómica: {str(e)}")


# =========================================
# GESTIÓN DE LOGS Y AUDITORÍA
# =========================================

class AuditLogger:
    """Manejador centralizado de logs y auditoría"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        
    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before,
            "metadata_after": metadata_after
        }
        
        self.log_buffer.append(log_entry)
        
        if json_output:
            return log_entry
        return None
        
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
            
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                        
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                    
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(self.log_buffer)
                    
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                    
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
            
            self.log_buffer = []
            
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

# =========================================
# FUNCIONES PRINCIPALES DE SUBCOMANDOS
# =========================================

def crear_archivo(
    file_path: str, 
    title: str, 
    responsable: str, 
    linked_to: List[str], 
    executor: str, 
    force: bool, 
    logger: AuditLogger,
    dry_run: bool,
    json_output: bool
) -> Tuple[int, Optional[Dict]]:
    """Implementa el subcomando 'crear'"""
    result = {"status": "", "message": "", "file": file_path}
    
    if os.path.exists(file_path) and not force:
        msg = "Archivo ya existe y --force no especificado"
        log_entry = logger.add_log_entry("crear", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"Error: {msg}\n")
        return 1, result
        
    metadata = TEMPLATE_METADATOS.copy()
    metadata.update({
        "title": title,
        "uuid": str(uuid.uuid4()),
        "created_at": datetime.datetime.now().isoformat(),
        "created_by": executor,
        "last_modified": datetime.datetime.now().isoformat(),
        "last_modified_by": executor,
        "responsable": [responsable],
        "linked_to": linked_to,
        "historial": [{
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "creacion",
            "descripcion": f"Archivo creado con ALMA_CLI_CLEANER v{VERSION}"
        }]
    })
    
    try:
        # Crear contenido inicial según tipo de archivo
        ext = os.path.splitext(file_path)[1].lower()
        content_body = ""
    
        if ext == ".md":
            content_body = f"# {title}\n\nNuevo archivo creado con ALMA CLI Cleaner v{VERSION}"
        elif ext == ".py":
            content_body = f'"""---\n{yaml.dump(metadata)}\n---\n"""\n\n# Contenido inicial'
    
        # CREA DIRECTORIO SOLO SI HAY
        dir_name = os.path.dirname(file_path)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
    
        # Escribir metadatos (con atomic write y dry-run)
        new_hash = MetadataHandler.write_metadata(
            file_path, 
            metadata, 
            content_body, 
            dry_run=dry_run
        )

        
        msg = f"Archivo creado con título: {title}"
        log_entry = logger.add_log_entry(
            "crear", file_path, "success", msg, executor, json_output=json_output
        )
        
        if dry_run:
            result.update({"status": "dry-run", "message": "Operación simulada"})
        else:
            result.update({"status": "success", "message": msg, "uuid": metadata["uuid"]})
            print(f"Archivo creado exitosamente: {file_path}")
        
        return (0, log_entry) if json_output else (0, result)
        
    except Exception as e:
        msg = f"Error en creación: {str(e)}"
        log_entry = logger.add_log_entry("crear", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"ERROR: {msg}\n")
        return 1, result

def validar_archivo(
    file_path: str, 
    executor: str, 
    force: bool, 
    logger: AuditLogger,
    dry_run: bool,
    json_output: bool
) -> Tuple[int, Optional[Dict]]:
    """Implementa el subcomando 'validar'"""
    result = {"status": "", "message": "", "file": file_path, "errors": []}
    
    try:
        metadata, content_body = MetadataHandler.extract_metadata(file_path)
        if metadata is None:
            msg = "No se encontraron metadatos válidos"
            log_entry = logger.add_log_entry("validar", file_path, "error", msg, executor, json_output=json_output)
            result.update({"status": "error", "message": msg})
            if json_output:
                return 1, log_entry
            sys.stderr.write(f"Error: {msg}\n")
            return 1, result
            
        # Validar estructura básica
        is_valid, errors = MetadataValidator.validate(metadata)
        result["errors"] = errors
        
        if not is_valid and not force:
            msg = f"Errores de validación: {len(errors)}"
            log_entry = logger.add_log_entry(
                "validar", file_path, "error", msg, executor, 
                metadata_before=metadata, json_output=json_output
            )
            result.update({"status": "error", "message": msg})
            
            if json_output:
                return 1, log_entry
                
            sys.stderr.write("Errores de validación encontrados:\n")
            for error in errors:
                sys.stderr.write(f"- {error}\n")
            sys.stderr.write("Use --force para reparar automáticamente\n")
            return 1, result
            
        # Normalizar y reparar metadatos
        fixed_metadata = MetadataValidator.normalize(metadata)
        changes = []

        # --- BLOQUE NUEVO: rellenar UUID y otros campos faltantes en modo FORCE ---
        critical_fields = ["uuid", "created_at", "created_by"]
        if force:
            for field in critical_fields:
                if not fixed_metadata.get(field):
                    if field == "uuid":
                        fixed_metadata[field] = str(uuid.uuid4())
                        changes.append("UUID faltante regenerado")
                    elif field == "created_at":
                        fixed_metadata[field] = datetime.datetime.now().isoformat()
                        changes.append("created_at faltante rellenado")
                    elif field == "created_by":
                        fixed_metadata[field] = executor
                        changes.append("created_by faltante rellenado")
        # --- FIN BLOQUE NUEVO ---

        # Migrar template si es necesario
        template_ver = fixed_metadata.get("template_version", "0.1.0")
        if template_ver < TEMPLATE_VERSION:
            original_ver = template_ver
            fixed_metadata = MetadataValidator.migrate_template(fixed_metadata)
            changes.append(f"Migrado de template v{original_ver} a v{TEMPLATE_VERSION}")
            
        # Actualizar versión de template si es necesario
        if fixed_metadata.get("template_version", "0.1.0") < TEMPLATE_VERSION:
            fixed_metadata["template_version"] = TEMPLATE_VERSION
            changes.append(f"Actualizado a template v{TEMPLATE_VERSION}")
        
        # Actualizar historial si hubo cambios
        if fixed_metadata != metadata or changes:
            fixed_metadata["last_modified"] = datetime.datetime.now().isoformat()
            fixed_metadata["last_modified_by"] = executor
            fixed_metadata["historial"].append({
                "fecha": datetime.datetime.now().isoformat(),
                "usuario": executor,
                "accion": "validación_reparación",
                "descripcion": f"Corregidos {len(errors)} errores. {', '.join(changes)}"
            })
            
            # Escribir cambios
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
                
            new_hash = MetadataHandler.write_metadata(
                file_path, fixed_metadata, content_body, original_content, dry_run
            )
            
            msg = f"Corregidos {len(errors)} errores, {len(changes)} actualizaciones"
            log_entry = logger.add_log_entry(
                "validar", file_path, "repaired", msg, executor,
                metadata_before=metadata, metadata_after=fixed_metadata,
                json_output=json_output
            )
            
            result.update({
                "status": "repaired" if not dry_run else "dry-run",
                "message": msg,
                "changes": changes,
                "errors_fixed": errors
            })
            
            if json_output:
                return 0, log_entry
                
            if dry_run:
                print(f"[DRY-RUN] Se repararían {len(errors)} errores")
            else:
                print(f"Archivo validado y reparado: {len(errors)} correcciones aplicadas")
        else:
            msg = "Metadatos válidos sin cambios"
            log_entry = logger.add_log_entry(
                "validar", file_path, "valid", msg, executor,
                json_output=json_output
            )
            result.update({"status": "valid", "message": msg})
            
            if json_output:
                return 0, log_entry
            print(msg)
            
        return 0, result
        
    except Exception as e:
        msg = f"Error en validación: {str(e)}"
        log_entry = logger.add_log_entry("validar", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"ERROR: {msg}\n")
        return 1, result

def limpiar_archivo(file_path: str, executor: str, logger: AuditLogger) -> int:
    """Implementa el subcomando 'limpiar'"""
    try:
        metadata, content_body = MetadataHandler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("limpiar", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        cleaned_metadata = MetadataValidator.normalize(metadata)
        
        # Eliminar campos no estándar
        standard_fields = set(TEMPLATE_METADATOS.keys())
        non_standard_fields = [k for k in cleaned_metadata.keys() if k not in standard_fields]
        for field in non_standard_fields:
            del cleaned_metadata[field]
        
        # Actualizar historial si hubo cambios
        if cleaned_metadata != original_metadata:
            cleaned_metadata["historial"].append({
                "fecha": datetime.datetime.now().isoformat(),
                "usuario": executor,
                "accion": "limpieza",
                "descripcion": "Metadatos normalizados y limpiados"
            })
            cleaned_metadata["last_modified"] = datetime.datetime.now().isoformat()
            cleaned_metadata["last_modified_by"] = executor
            
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            MetadataHandler.write_metadata(file_path, cleaned_metadata, 
                                         content_body, original_content)
            
            changes = len(non_standard_fields) + sum(
                1 for k in original_metadata 
                if original_metadata[k] != cleaned_metadata.get(k, None)
            )
            
            logger.add_log_entry("limpiar", file_path, "cleaned", 
                               f"Aplicadas {changes} limpiezas", executor,
                               metadata_before=original_metadata, 
                               metadata_after=cleaned_metadata)
            print(f"Archivo limpiado: {changes} cambios aplicados")
        else:
            logger.add_log_entry("limpiar", file_path, "clean", 
                               "No se requirieron cambios", executor)
            print("Metadatos ya limpios, no se requirieron cambios")
            
        return 0
        
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("limpiar", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_responsable(file_path: str, responsable: List[str], 
                   executor: str, logger: AuditLogger) -> int:
    """Implementa el subcomando 'set_responsable'"""
    try:
        metadata, content_body = MetadataHandler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_responsable", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["responsable"] = responsable
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_responsable",
            "descripcion": f"Responsables actualizados: {', '.join(responsable)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        MetadataHandler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_responsable", file_path, "updated", 
                           f"Responsables actualizados a: {', '.join(responsable)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Responsables actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_responsable", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_linked(file_path: str, linked_to: List[str], 
              executor: str, logger: AuditLogger) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = MetadataHandler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["linked_to"] = linked_to
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        MetadataHandler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(linked_to)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def mostrar_log(file_path: str, executor: str, logger: AuditLogger) -> int:
    """Implementa el subcomando 'log'"""
    try:
        metadata, _ = MetadataHandler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("log", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        if "historial" not in metadata or not metadata["historial"]:
            print("No hay registros de historial disponibles")
            return 0
            
        print(f"Historial de cambios para: {file_path}")
        print("=" * 60)
        for entry in metadata["historial"]:
            print(f"[{entry.get('fecha', '')}] - {entry.get('usuario', '')}")
            print(f"Acción: {entry.get('accion', '')}")
            print(f"Descripción: {entry.get('descripcion', '')}")
            print("-" * 60)
            
        logger.add_log_entry("log", file_path, "viewed", 
                           "Historial consultado", executor)
        return 0
        
    except Exception as e:
        logger.add_log_entry("log", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

# =========================================
# ENTRYPOINT PRINCIPAL
# =========================================

def load_config() -> dict:
    """
    Carga la configuración desde alma_cleaner_config.yaml si existe,
    si no, usa DEFAULT_CONFIG. Si hay error en el parseo, lo reporta y sigue.
    """
    config_path = "alma_cleaner_config.yaml"
    config = DEFAULT_CONFIG.copy()
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.load(f) or {}
            # Actualiza solo claves existentes, no agrega basura
            for section in config:
                if section in user_config:
                    if isinstance(config[section], dict) and isinstance(user_config[section], dict):
                        config[section].update(user_config[section])
                    else:
                        config[section] = user_config[section]
        except Exception as e:
            sys.stderr.write(f"ADVERTENCIA: Error cargando configuración: {str(e)}\n")
    return config

def main():
    # Configuración inicial
    config = load_config()
    logger = AuditLogger(config)
    executor = os.environ.get("ALMA_EXECUTOR", "humano")

    # --- PARSER PRINCIPAL ---
    parser = argparse.ArgumentParser(
        prog="alma_cli_cleaner",
        description="CLI profesional para gestión de metadatos ALMA_RESIST"
    )
    parser.add_argument("--version", action="version", version=f"v{VERSION}")

    # --- PARENT PARSER: FLAGS GLOBALES ---
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument("--executor", default=executor, help="Tipo de ejecutor (humano, Kael, Centralesis)")
    parent_parser.add_argument("--force", action="store_true", help="Forzar operaciones potencialmente destructivas")
    parent_parser.add_argument("--dry-run", action="store_true", help="Simular operaciones sin realizar cambios")
    parent_parser.add_argument("--json", action="store_true", help="Salida en formato JSON")

    # --- SUBPARSERS ---
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Subcomando: crear
    crear_parser = subparsers.add_parser("crear", parents=[parent_parser], help="Crear archivo con metadatos")
    crear_parser.add_argument("archivo", help="Ruta del archivo a crear")
    crear_parser.add_argument("--title", required=True, help="Título del archivo")
    crear_parser.add_argument("--responsable", required=True, help="Responsable(s) del archivo")
    crear_parser.add_argument("--linked-to", default="", help="Archivos vinculados (separados por comas)")

    # Subcomando: validar
    validar_parser = subparsers.add_parser("validar", parents=[parent_parser], help="Validar metadatos de archivo")
    validar_parser.add_argument("archivo", help="Ruta del archivo a validar")

    # Subcomando: limpiar
    limpiar_parser = subparsers.add_parser("limpiar", parents=[parent_parser], help="Limpiar metadatos de archivo")
    limpiar_parser.add_argument("archivo", help="Ruta del archivo a limpiar")

    # Subcomando: set_responsable
    resp_parser = subparsers.add_parser("set_responsable", parents=[parent_parser], help="Establecer responsable(s)")
    resp_parser.add_argument("archivo", help="Ruta del archivo")
    resp_parser.add_argument("--responsable", required=True, help="Nuevo(s) responsable(s) (separados por comas)")

    # Subcomando: set_linked
    linked_parser = subparsers.add_parser("set_linked", parents=[parent_parser], help="Establecer archivos vinculados")
    linked_parser.add_argument("archivo", help="Ruta del archivo")
    linked_parser.add_argument("--linked-to", required=True, help="Archivos vinculados (separados por comas)")

    # Subcomando: log
    log_parser = subparsers.add_parser("log", parents=[parent_parser], help="Mostrar historial de cambios")
    log_parser.add_argument("archivo", help="Ruta del archivo")

    # ... resto del main igual (parse_args, dispatch, etc)


    args = parser.parse_args()
    json_output = args.json

    # Ejecutar subcomando
    exit_code = 1
    json_result = {}

    try:
        if args.command == "crear":
            linked_list = [x.strip() for x in args.linked_to.split(",")] if args.linked_to else []
            exit_code, json_result = crear_archivo(
                args.archivo, args.title, args.responsable, linked_list,
                args.executor, args.force, logger, args.dry_run, json_output
            )

        elif args.command == "validar":
            exit_code, json_result = validar_archivo(
                args.archivo, args.executor, args.force, logger, args.dry_run, json_output
            )

        elif args.command == "limpiar":
            exit_code = limpiar_archivo(
                args.archivo, args.executor, logger
            )

        elif args.command == "set_responsable":
            responsables_list = [x.strip() for x in args.responsable.split(",")]
            exit_code = set_responsable(
                args.archivo, responsables_list, args.executor, logger
            )

        elif args.command == "set_linked":
            linked_list = [x.strip() for x in args.linked_to.split(",")]
            exit_code = set_linked(
                args.archivo, linked_list, args.executor, logger
            )

        elif args.command == "log":
            exit_code = mostrar_log(
                args.archivo, logger
            )

        else:
            sys.stderr.write("Comando no reconocido.\n")
            exit_code = 1

        if json_output and json_result:
            print(json.dumps(json_result, indent=2))

    except Exception as e:
        import traceback
        traceback.print_exc()  # Para debuggear en vivo: elimina esto si no querés tanto detalle
        error_msg = f"ERROR CRÍTICO: {str(e)}"
        logger.add_log_entry(
            getattr(args, "command", "unknown"),
            getattr(args, "archivo", ""),
            "error", error_msg, getattr(args, "executor", "humano")
        )
        if json_output:
            print(json.dumps({
                "status": "error",
                "message": error_msg,
                "command": getattr(args, "command", ""),
                "file": getattr(args, "archivo", "")
            }, indent=2))
        else:
            sys.stderr.write(f"{error_msg}\n")
        exit_code = 1

    finally:
        try:
            logger.flush_logs()
        except Exception as e:
            error_msg = f"ERROR AL ESCRIBIR LOGS: {str(e)}"
            if not json_output:
                sys.stderr.write(f"{error_msg}\n")

    sys.exit(exit_code)

# =========================================
# DOCUMENTACIÓN INTERNA
# =========================================

"""
README INTERNO: ALMA_CLI_CLEANER v0.1.4

CARACTERÍSTICAS PRINCIPALES:
1.  Manejo robusto de metadatos con preservación de comentarios y orden
    usando ruamel.yaml
2.  Dry-run real: --dry-run muestra diferencias sin modificar archivos
3.  Escrituras atómicas: Usa archivos temporales + reemplazo para evitar
    corrupción de datos
4.  Verificación de hash: Cada modificación actualiza hash_verificacion
5.  Migración de templates: Convierte campos antiguos a la nueva estructura
6.  Soporte para JSON: --json produce salida machine-readable
7.  Protección contra archivos binarios/grandes
8.  Registro de auditoría en Parquet/JSON/YAML

MEJORAS DE SEGURIDAD:
-   Validación estricta de tipos y estructura
-   Rollback automático mediante escritura atómica
-   Verificación de compatibilidad de versiones
-   Protección contra corrupción de datos

CHANGELOG:
v0.1.4 - 2023-11-15
-   Migrado de PyYAML a ruamel.yaml
-   Implementado dry-run real
-   Añadida escritura atómica
-   Implementado hash de verificación
-   Añadida migración de templates
-   Soporte para salida JSON
-   Validación de archivos binarios/grandes
-   Mejor manejo de docstrings en Python

v0.1.3 - 2023-11-10
-   Versión inicial con funcionalidad básica
"""

if __name__ == "__main__":
    main()
```

## 2025-07-10: "ALMA_CLI_CLEANER v0.1.5"

### Script: "Alma_Cli_Cleaner_v0.1.5" Backup no tocar (bloque python)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =========================================
# ALMA_CLI_CLEANER v0.1.5
# CLI profesional para gestión de metadatos ALMA_RESIST
# =========================================

import os
import sys
import re
import argparse
import uuid
import json
import csv
import datetime
import logging
import hashlib
import shutil
import tempfile
import io
import traceback
import time
import fcntl
from typing import List, Dict, Any, Optional, Tuple, Union
import ruamel.yaml
from ruamel.yaml.comments import CommentedMap
from ruamel.yaml.scalarstring import PreservedScalarString

# =========================================
# CONSTANTES GLOBALES Y CONFIGURACIÓN BASE
# =========================================

VERSION = "0.1.5"
TEMPLATE_VERSION = "0.1.3"
MIN_COMPATIBLE_VERSION = "0.1.0"
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB (valor por defecto)
SUPPORTED_EXTENSIONS = [".md", ".yaml", ".yml", ".py"]  # valores por defecto

# Patrón constante para extracción de metadatos
METADATA_PATTERN = r'(\'\'\'|\"\"\")\s*?---\n(.+?)\n---\n(.*?)(\1)'

# Inicializar ruamel.yaml
yaml = ruamel.yaml.YAML()
yaml.preserve_quotes = True
yaml.indent(mapping=2, sequence=4, offset=2)
yaml.width = 120

# =========================================
# ESTRUCTURAS DE DATOS Y VALIDACIONES
# =========================================

class MetadataValidator:
    """Validador y normalizador de estructuras de metadatos"""
    
    REQUIRED_FIELDS = ["title", "uuid", "created_at", "created_by"]
    FIELD_TYPES = {
        "title": str,
        "uuid": str,
        "tipo": str,
        "schema": str,
        "estado": str,
        "descripcion": str,
        "estructura": list,
        "tags": list,
        "linked_to": list,
        "responsable": list,
        "hash_verificacion": str,
        "historial": list,
        "last_modified": str,
        "last_modified_by": str,
        "created_at": str,
        "created_by": str,
        "proceso_origen": str,
        "input_data": list,
        "output_data": list,
        "comentarios": str,
        "ia_metadata": dict
    }

    @staticmethod
    def validate(metadata: Dict) -> Tuple[bool, List[str]]:
        """Valida estructura básica y tipos de metadatos"""
        errors = []
        
        # Verificar campos obligatorios
        for field in MetadataValidator.REQUIRED_FIELDS:
            if field not in metadata:
                errors.append(f"Campo obligatorio faltante: {field}")
        
        # Validar tipos de datos
        for field, expected_type in MetadataValidator.FIELD_TYPES.items():
            if field in metadata:
                if expected_type is str and isinstance(metadata[field], PreservedScalarString):
                    continue
                if not isinstance(metadata[field], expected_type):
                    errors.append(f"Tipo incorrecto para {field}. Esperado: {expected_type.__name__}")
        
        # Verificar compatibilidad de versión
        template_ver = metadata.get("template_version", "0.0.0")
        if template_ver < MIN_COMPATIBLE_VERSION:
            errors.append(f"Versión de template incompatible: {template_ver} < {MIN_COMPATIBLE_VERSION}")
        
        return len(errors) == 0, errors

    @staticmethod
    def normalize(metadata: Dict) -> Dict:
        """Normaliza y limpia los valores de los metadatos"""
        normalized = metadata.copy()
        
        # Normalizar listas
        list_fields = ["tags", "linked_to", "responsable", "input_data", "output_data"]
        for field in list_fields:
            if field in normalized:
                if isinstance(normalized[field], str):
                    normalized[field] = [item.strip() for item in normalized[field].split(",")]
                normalized[field] = [str(item).strip() for item in normalized[field] if item]
        
        # Normalizar strings
        string_fields = ["title", "tipo", "schema", "estado", "descripcion", "comentarios"]
        for field in string_fields:
            if field in normalized:
                if not isinstance(normalized[field], PreservedScalarString):
                    normalized[field] = str(normalized[field]).strip()
                
        # Asegurar historial correcto
        if "historial" in normalized and not isinstance(normalized["historial"], list):
            normalized["historial"] = []
            
        return normalized

    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()
        
        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }
        
        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo migrado: {old_field} -> {new_field}"
                })
        
        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            if field in migrated:
                del migrated[field]
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo obsoleto eliminado: {field}"
                })
        
        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated

# =========================================
# MANEJO DE ARCHIVOS Y METADATOS
# =========================================

class FileLock:
    """Mecanismo de bloqueo de archivos para prevenir condiciones de carrera"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None

    def __enter__(self):
        """Adquiere el bloqueo"""
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > 30:  # Timeout después de 30 segundos
                    raise TimeoutError("No se pudo adquirir el bloqueo después de 30 segundos")
                time.sleep(0.1)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Libera el bloqueo"""
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
            self.lock_fd.close()
            try:
                os.remove(self.lock_file)
            except OSError:
                pass

class MetadataHandler:
    """Manejador de operaciones con metadatos en archivos"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.max_file_size = config.get("max_file_size", MAX_FILE_SIZE)
        self.supported_extensions = config.get("supported_extensions", SUPPORTED_EXTENSIONS)
        self.backup_dir = config.get("backup_dir", "/tmp/alma_backups")

    def is_binary_file(self, file_path: str) -> bool:
        """Detecta si un archivo es binario"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\0' in chunk
        except Exception:
            return True

    def extract_metadata(self, file_path: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos de diferentes tipos de archivos"""
        ext = os.path.splitext(file_path)[1].lower()
        content = ""
        
        # Verificar extensión soportada
        if ext not in self.supported_extensions:
            raise ValueError(f"Extensión no soportada: {ext}. Extensiones válidas: {', '.join(self.supported_extensions)}")
        
        # Verificar si es binario
        if self.is_binary_file(file_path):
            raise ValueError(f"Archivo binario detectado: {file_path}")
        
        # Verificar tamaño
        file_size = os.path.getsize(file_path)
        if file_size > self.max_file_size:
            raise ValueError(f"Archivo demasiado grande: {file_size} bytes > {self.max_file_size}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            raise ValueError("Archivo no es texto válido (UTF-8)")
        except Exception as e:
            raise IOError(f"Error leyendo archivo: {str(e)}")
        
        if ext == ".md":
            return self._extract_from_markdown(content)
        elif ext in [".yaml", ".yml"]:
            return self._extract_from_yaml(content)
        elif ext == ".py":
            return self._extract_from_python(content)
        else:
            raise ValueError(f"Formato no soportado: {ext}")

    def _extract_from_markdown(self, content: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos YAML de archivos Markdown"""
        match = re.search(r'^---\n(.+?)\n---\n(.*)', content, re.DOTALL)
        if match:
            try:
                metadata = yaml.load(match.group(1))
                content_body = match.group(2)
                return metadata, content_body
            except Exception as e:
                raise ValueError(f"Error analizando YAML: {str(e)}")
        return None, content

    def _extract_from_yaml(self, content: str) -> Tuple[Optional[Dict], str]:
        """Carga todo el contenido YAML como metadatos"""
        try:
            data = yaml.load(content)
            return (data if data is not None else {}), ""
        except Exception as e:
            raise ValueError(f"Error analizando YAML: {str(e)}")

    def _extract_from_python(self, content: str) -> Tuple[Optional[Dict], str]:
        """Extrae metadatos de docstrings en Python"""
        match = re.search(METADATA_PATTERN, content, re.DOTALL)
        if match:
            try:
                metadata = yaml.load(match.group(2))
                return metadata, content
            except Exception as e:
                raise ValueError(f"Error analizando YAML en docstring: {str(e)}")
        return None, content

    @staticmethod
    def calculate_hash(content: str) -> str:
        """Calcula hash de contenido para verificación"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return
            
        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            os.makedirs(self.backup_dir, exist_ok=True)
            
            # Crear backup
            backup_path = os.path.join(self.backup_dir, os.path.basename(file_path) + f".{int(time.time())}.bak"
            if os.path.exists(file_path):
                shutil.copy2(file_path, backup_path)
            
            # Bloqueo de archivo
            with FileLock(file_path):
                # Escribir a archivo temporal
                with tempfile.NamedTemporaryFile(
                    mode='w', 
                    encoding='utf-8', 
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name
                
                # Reemplazar archivo original
                shutil.move(tmp_path, file_path)
                
        except Exception as e:
            # Restaurar backup si existe
            if os.path.exists(backup_path):
                shutil.move(backup_path, file_path)
            
            # Limpiar archivos temporales
            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)
                
            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")
        finally:
            # Eliminar backup después de operación exitosa
            if os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass

    def write_metadata(
        self, 
        file_path: str, 
        metadata: Dict, 
        content_body: str = "", 
        original_content: str = "",
        dry_run: bool = False
    ) -> str:
        """Escribe metadatos en diferentes formatos de archivo"""
        ext = os.path.splitext(file_path)[1].lower()
        content = ""
        
        try:
            if ext == ".md":
                # Convertir a CommentedMap para preservar orden
                yaml_metadata = CommentedMap(metadata)
                stream = io.StringIO()
                yaml.dump(yaml_metadata, stream)
                yaml_text = stream.getvalue()
                content = f"---\n{yaml_text}---\n{content_body}"
                
            elif ext in [".yaml", ".yml"]:
                yaml_metadata = CommentedMap(metadata)
                stream = io.StringIO()
                yaml.dump(yaml_metadata, stream)
                content = stream.getvalue()
                
            elif ext == ".py":
                if original_content:
                    # Verificar si existe el bloque de metadatos
                    if not re.search(METADATA_PATTERN, original_content, re.DOTALL):
                        raise ValueError("No se encontró el bloque de metadatos para reemplazar")
                    
                    # Reemplazar solo el bloque de metadatos
                    content = re.sub(
                        METADATA_PATTERN,
                        f'\\1---\n{yaml.dump(metadata)}\n---\\3\\4',
                        original_content,
                        flags=re.DOTALL,
                        count=1
                    )
                else:
                    content = original_content
            else:
                raise ValueError(f"Formato no soportado: {ext}")
            
            # Calcular y actualizar hash
            new_hash = self.calculate_hash(content)
            metadata["hash_verificacion"] = new_hash
            
            # Escribir de forma atómica
            self.atomic_write(file_path, content, dry_run)
            return new_hash
                
        except Exception as e:
            raise IOError(f"Error escribiendo metadatos: {str(e)}")

# =========================================
# GESTIÓN DE LOGS Y AUDITORÍA
# =========================================

class AuditLogger:
    """Manejador centralizado de logs y auditoría"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        
    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before,
            "metadata_after": metadata_after
        }
        
        self.log_buffer.append(log_entry)
        
        if json_output:
            return log_entry
        return None
        
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
            
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                        
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                    
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(self.log_buffer)
                    
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                    
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
            
            self.log_buffer = []
            
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

# =========================================
# FUNCIONES PRINCIPALES DE SUBCOMANDOS
# =========================================

def crear_archivo(
    file_path: str, 
    title: str, 
    responsable: str, 
    linked_to: List[str], 
    executor: str, 
    force: bool, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler,
    dry_run: bool,
    json_output: bool
) -> Tuple[int, Optional[Dict]]:
    """Implementa el subcomando 'crear'"""
    result = {"status": "", "message": "", "file": file_path}
    
    if os.path.exists(file_path) and not force:
        msg = "Archivo ya existe y --force no especificado"
        log_entry = logger.add_log_entry("crear", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"Error: {msg}\n")
        return 1, result
        
    metadata = TEMPLATE_METADATOS.copy()
    metadata.update({
        "title": title,
        "uuid": str(uuid.uuid4()),
        "created_at": datetime.datetime.now().isoformat(),
        "created_by": executor,
        "last_modified": datetime.datetime.now().isoformat(),
        "last_modified_by": executor,
        "responsable": [responsable],
        "linked_to": linked_to,
        "historial": [{
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "creacion",
            "descripcion": f"Archivo creado con ALMA_CLI_CLEANER v{VERSION}"
        }]
    })
    
    try:
        # Crear contenido inicial según tipo de archivo
        ext = os.path.splitext(file_path)[1].lower()
        content_body = ""
    
        if ext == ".md":
            content_body = f"# {title}\n\nNuevo archivo creado con ALMA CLI Cleaner v{VERSION}"
        elif ext == ".py":
            content_body = f'"""---\n{yaml.dump(metadata)}\n---\n"""\n\n# Contenido inicial'
    
        # CREA DIRECTORIO SOLO SI HAY
        dir_name = os.path.dirname(file_path)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
    
        # Escribir metadatos (con atomic write y dry-run)
        new_hash = metadata_handler.write_metadata(
            file_path, 
            metadata, 
            content_body, 
            dry_run=dry_run
        )

        
        msg = f"Archivo creado con título: {title}"
        log_entry = logger.add_log_entry(
            "crear", file_path, "success", msg, executor, json_output=json_output
        )
        
        if dry_run:
            result.update({"status": "dry-run", "message": "Operación simulada"})
        else:
            result.update({"status": "success", "message": msg, "uuid": metadata["uuid"]})
            print(f"Archivo creado exitosamente: {file_path}")
        
        return (0, log_entry) if json_output else (0, result)
        
    except Exception as e:
        msg = f"Error en creación: {str(e)}"
        log_entry = logger.add_log_entry("crear", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"ERROR: {msg}\n")
        return 1, result

def validar_archivo(
    file_path: str, 
    executor: str, 
    force: bool, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler,
    dry_run: bool,
    json_output: bool
) -> Tuple[int, Optional[Dict]]:
    """Implementa el subcomando 'validar'"""
    result = {"status": "", "message": "", "file": file_path, "errors": []}
    
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            msg = "No se encontraron metadatos válidos"
            log_entry = logger.add_log_entry("validar", file_path, "error", msg, executor, json_output=json_output)
            result.update({"status": "error", "message": msg})
            if json_output:
                return 1, log_entry
            sys.stderr.write(f"Error: {msg}\n")
            return 1, result
            
        # Validar estructura básica
        is_valid, errors = MetadataValidator.validate(metadata)
        result["errors"] = errors
        
        if not is_valid and not force:
            msg = f"Errores de validación: {len(errors)}"
            log_entry = logger.add_log_entry(
                "validar", file_path, "error", msg, executor, 
                metadata_before=metadata, json_output=json_output
            )
            result.update({"status": "error", "message": msg})
            
            if json_output:
                return 1, log_entry
                
            sys.stderr.write("Errores de validación encontrados:\n")
            for error in errors:
                sys.stderr.write(f"- {error}\n")
            sys.stderr.write("Use --force para reparar automáticamente\n")
            return 1, result
            
        # Normalizar y reparar metadatos
        fixed_metadata = MetadataValidator.normalize(metadata)
        changes = []

        # --- BLOQUE NUEVO: rellenar UUID y otros campos faltantes en modo FORCE ---
        critical_fields = ["uuid", "created_at", "created_by"]
        if force:
            for field in critical_fields:
                if not fixed_metadata.get(field):
                    if field == "uuid":
                        fixed_metadata[field] = str(uuid.uuid4())
                        changes.append("UUID faltante regenerado")
                    elif field == "created_at":
                        fixed_metadata[field] = datetime.datetime.now().isoformat()
                        changes.append("created_at faltante rellenado")
                    elif field == "created_by":
                        fixed_metadata[field] = executor
                        changes.append("created_by faltante rellenado")
        # --- FIN BLOQUE NUEVO ---

        # Migrar template si es necesario
        template_ver = fixed_metadata.get("template_version", "0.1.0")
        if template_ver < TEMPLATE_VERSION:
            original_ver = template_ver
            fixed_metadata = MetadataValidator.migrate_template(fixed_metadata)
            changes.append(f"Migrado de template v{original_ver} a v{TEMPLATE_VERSION}")
            logger.add_log_entry(
                "validar", file_path, "migración", 
                f"Migrado de template v{original_ver} a v{TEMPLATE_VERSION}", 
                executor
            )
            
        # Actualizar versión de template si es necesario
        if fixed_metadata.get("template_version", "0.1.0") < TEMPLATE_VERSION:
            fixed_metadata["template_version"] = TEMPLATE_VERSION
            changes.append(f"Actualizado a template v{TEMPLATE_VERSION}")
        
        # Actualizar historial si hubo cambios
        if fixed_metadata != metadata or changes:
            fixed_metadata["last_modified"] = datetime.datetime.now().isoformat()
            fixed_metadata["last_modified_by"] = executor
            fixed_metadata["historial"].append({
                "fecha": datetime.datetime.now().isoformat(),
                "usuario": executor,
                "accion": "validación_reparación",
                "descripcion": f"Corregidos {len(errors)} errores. {', '.join(changes)}"
            })
            
            # Escribir cambios
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
                
            new_hash = metadata_handler.write_metadata(
                file_path, fixed_metadata, content_body, original_content, dry_run
            )
            
            msg = f"Corregidos {len(errors)} errores, {len(changes)} actualizaciones"
            log_entry = logger.add_log_entry(
                "validar", file_path, "repaired", msg, executor,
                metadata_before=metadata, metadata_after=fixed_metadata,
                json_output=json_output
            )
            
            result.update({
                "status": "repaired" if not dry_run else "dry-run",
                "message": msg,
                "changes": changes,
                "errors_fixed": errors
            })
            
            if json_output:
                return 0, log_entry
                
            if dry_run:
                print(f"[DRY-RUN] Se repararían {len(errors)} errores")
            else:
                print(f"Archivo validado y reparado: {len(errors)} correcciones aplicadas")
        else:
            msg = "Metadatos válidos sin cambios"
            log_entry = logger.add_log_entry(
                "validar", file_path, "valid", msg, executor,
                json_output=json_output
            )
            result.update({"status": "valid", "message": msg})
            
            if json_output:
                return 0, log_entry
            print(msg)
            
        return 0, result
        
    except Exception as e:
        msg = f"Error en validación: {str(e)}"
        log_entry = logger.add_log_entry("validar", file_path, "error", msg, executor, json_output=json_output)
        result.update({"status": "error", "message": msg})
        if json_output:
            return 1, log_entry
        sys.stderr.write(f"ERROR: {msg}\n")
        return 1, result

def limpiar_archivo(
    file_path: str, 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'limpiar'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("limpiar", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        cleaned_metadata = MetadataValidator.normalize(metadata)
        
        # Eliminar campos no estándar
        standard_fields = set(TEMPLATE_METADATOS.keys())
        non_standard_fields = [k for k in cleaned_metadata.keys() if k not in standard_fields]
        for field in non_standard_fields:
            del cleaned_metadata[field]
        
        # Actualizar historial si hubo cambios
        if cleaned_metadata != original_metadata:
            cleaned_metadata["historial"].append({
                "fecha": datetime.datetime.now().isoformat(),
                "usuario": executor,
                "accion": "limpieza",
                "descripcion": "Metadatos normalizados y limpiados"
            })
            cleaned_metadata["last_modified"] = datetime.datetime.now().isoformat()
            cleaned_metadata["last_modified_by"] = executor
            
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            metadata_handler.write_metadata(file_path, cleaned_metadata, 
                                         content_body, original_content)
            
            changes = len(non_standard_fields) + sum(
                1 for k in original_metadata 
                if original_metadata[k] != cleaned_metadata.get(k, None)
            )
            
            logger.add_log_entry("limpiar", file_path, "cleaned", 
                               f"Aplicadas {changes} limpiezas", executor,
                               metadata_before=original_metadata, 
                               metadata_after=cleaned_metadata)
            print(f"Archivo limpiado: {changes} cambios aplicados")
        else:
            logger.add_log_entry("limpiar", file_path, "clean", 
                               "No se requirieron cambios", executor)
            print("Metadatos ya limpios, no se requirieron cambios")
            
        return 0
        
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("limpiar", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_responsable(
    file_path: str, 
    responsable: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_responsable'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_responsable", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["responsable"] = responsable
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_responsable",
            "descripcion": f"Responsables actualizados: {', '.join(responsable)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_responsable", file_path, "updated", 
                           f"Responsables actualizados a: {', '.join(responsable)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Responsables actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_responsable", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["linked_to"] = linked_to
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(linked_to)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

def mostrar_log(
    file_path: str, 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'log'"""
    try:
        metadata, _ = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("log", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        if "historial" not in metadata or not metadata["historial"]:
            print("No hay registros de historial disponibles")
            return 0
            
        print(f"Historial de cambios para: {file_path}")
        print("=" * 60)
        for entry in metadata["historial"]:
            print(f"[{entry.get('fecha', '')}] - {entry.get('usuario', '')}")
            print(f"Acción: {entry.get('accion', '')}")
            print(f"Descripción: {entry.get('descripcion', '')}")
            print("-" * 60)
            
        logger.add_log_entry("log", file_path, "viewed", 
                           "Historial consultado", executor)
        return 0
        
    except Exception as e:
        logger.add_log_entry("log", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1

# =========================================
# ENTRYPOINT PRINCIPAL
# =========================================

def load_config() -> dict:
    """
    Carga la configuración desde alma_cleaner_config.yaml si existe,
    si no, usa DEFAULT_CONFIG. Si hay error en el parseo, lo reporta y sigue.
    """
    config_path = "alma_cleaner_config.yaml"
    config = {
        "logs": {
            "ruta": "/home/alma/Alma-Cli/Logs/Cleaner.parquet",
            "nivel": "INFO",
            "formato": "parquet",
            "loguear_errores": True
        },
        "politica_errores": "strict",
        "advertencia_tamano": 1048576,  # 1MB
        "hash_algoritmo": "sha256",
        "max_file_size": MAX_FILE_SIZE,
        "supported_extensions": SUPPORTED_EXTENSIONS,
        "backup_dir": "/tmp/alma_backups"
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.load(f) or {}
            # Actualiza solo claves existentes, no agrega basura
            for section in config:
                if section in user_config:
                    if isinstance(config[section], dict) and isinstance(user_config[section], dict):
                        config[section].update(user_config[section])
                    else:
                        config[section] = user_config[section]
            # Carga especial para parámetros críticos
            if "max_file_size" in user_config:
                config["max_file_size"] = user_config["max_file_size"]
            if "supported_extensions" in user_config:
                config["supported_extensions"] = user_config["supported_extensions"]
            if "backup_dir" in user_config:
                config["backup_dir"] = user_config["backup_dir"]
        except Exception as e:
            sys.stderr.write(f"ADVERTENCIA: Error cargando configuración: {str(e)}\n")
    return config

def main():
    # Configuración inicial
    config = load_config()
    logger = AuditLogger(config)
    metadata_handler = MetadataHandler(config)
    executor = os.environ.get("ALMA_EXECUTOR", "humano")

    # --- PARSER PRINCIPAL ---
    parser = argparse.ArgumentParser(
        prog="alma_cli_cleaner",
        description="CLI profesional para gestión de metadatos ALMA_RESIST"
    )
    parser.add_argument("--version", action="version", version=f"v{VERSION}")

    # --- PARENT PARSER: FLAGS GLOBALES ---
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument("--executor", default=executor, help="Tipo de ejecutor (humano, Kael, Centralesis)")
    parent_parser.add_argument("--force", action="store_true", help="Forzar operaciones potencialmente destructivas")
    parent_parser.add_argument("--dry-run", action="store_true", help="Simular operaciones sin realizar cambios")
    parent_parser.add_argument("--json", action="store_true", help="Salida en formato JSON")

    # --- SUBPARSERS ---
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Subcomando: crear
    crear_parser = subparsers.add_parser("crear", parents=[parent_parser], help="Crear archivo con metadatos")
    crear_parser.add_argument("archivo", help="Ruta del archivo a crear")
    crear_parser.add_argument("--title", required=True, help="Título del archivo")
    crear_parser.add_argument("--responsable", required=True, help="Responsable(s) del archivo")
    crear_parser.add_argument("--linked-to", default="", help="Archivos vinculados (separados por comas)")

    # Subcomando: validar
    validar_parser = subparsers.add_parser("validar", parents=[parent_parser], help="Validar metadatos de archivo")
    validar_parser.add_argument("archivo", help="Ruta del archivo a validar")

    # Subcomando: limpiar
    limpiar_parser = subparsers.add_parser("limpiar", parents=[parent_parser], help="Limpiar metadatos de archivo")
    limpiar_parser.add_argument("archivo", help="Ruta del archivo a limpiar")

    # Subcomando: set_responsable
    resp_parser = subparsers.add_parser("set_responsable", parents=[parent_parser], help="Establecer responsable(s)")
    resp_parser.add_argument("archivo", help="Ruta del archivo")
    resp_parser.add_argument("--responsable", required=True, help="Nuevo(s) responsable(s) (separados por comas)")

    # Subcomando: set_linked
    linked_parser = subparsers.add_parser("set_linked", parents=[parent_parser], help="Establecer archivos vinculados")
    linked_parser.add_argument("archivo", help="Ruta del archivo")
    linked_parser.add_argument("--linked-to", required=True, help="Archivos vinculados (separados por comas)")

    # Subcomando: log
    log_parser = subparsers.add_parser("log", parents=[parent_parser], help="Mostrar historial de cambios")
    log_parser.add_argument("archivo", help="Ruta del archivo")

    args = parser.parse_args()
    json_output = args.json

    # Clarificar comportamiento dry-run + force
    if args.dry_run and args.force:
        print("[ADVERTENCIA] --dry-run tiene prioridad sobre --force. No se realizarán cambios reales")

    # Ejecutar subcomando
    exit_code = 1
    json_result = {}

    try:
        if args.command == "crear":
            linked_list = [x.strip() for x in args.linked_to.split(",")] if args.linked_to else []
            exit_code, json_result = crear_archivo(
                args.archivo, args.title, args.responsable, linked_list,
                args.executor, args.force, logger, metadata_handler, args.dry_run, json_output
            )

        elif args.command == "validar":
            exit_code, json_result = validar_archivo(
                args.archivo, args.executor, args.force, logger, metadata_handler, args.dry_run, json_output
            )

        elif args.command == "limpiar":
            exit_code = limpiar_archivo(
                args.archivo, args.executor, logger, metadata_handler
            )

        elif args.command == "set_responsable":
            responsables_list = [x.strip() for x in args.responsable.split(",")]
            exit_code = set_responsable(
                args.archivo, responsables_list, args.executor, logger, metadata_handler
            )

        elif args.command == "set_linked":
            linked_list = [x.strip() for x in args.linked_to.split(",")]
            exit_code = set_linked(
                args.archivo, linked_list, args.executor, logger, metadata_handler
            )

        elif args.command == "log":
            exit_code = mostrar_log(
                args.archivo, args.executor, logger, metadata_handler
            )

        else:
            sys.stderr.write("Comando no reconocido.\n")
            exit_code = 1

        if json_output and json_result:
            print(json.dumps(json_result, indent=2))

    except Exception as e:
        error_msg = f"ERROR CRÍTICO: {str(e)}"
        logger.add_log_entry(
            getattr(args, "command", "unknown"),
            getattr(args, "archivo", ""),
            "error", error_msg, getattr(args, "executor", "humano")
        )
        if json_output:
            print(json.dumps({
                "status": "error",
                "message": error_msg,
                "command": getattr(args, "command", ""),
                "file": getattr(args, "archivo", "")
            }, indent=2))
        else:
            sys.stderr.write(f"{error_msg}\n")
        exit_code = 1

    finally:
        try:
            logger.flush_logs()
        except Exception as e:
            error_msg = f"ERROR AL ESCRIBIR LOGS: {str(e)}"
            if not json_output:
                sys.stderr.write(f"{error_msg}\n")

    sys.exit(exit_code)

# =========================================
# DOCUMENTACIÓN INTERNA
# =========================================

"""
README INTERNO: ALMA_CLI_CLEANER v0.1.5

MEJORAS PRINCIPALES RESPECTO A v0.1.4:

1.  ELIMINACIÓN DE DUPLICADOS:
    - Método atomic_write duplicado eliminado
    - Se conserva la versión mejorada con manejo de directorios

2.  EXTRACCIÓN YAML MEJORADA:
    - Retorno consistente incluso con archivos YAML vacíos
    - Manejo robusto de contenido vacío (retorna {} en lugar de None)

3.  MANEJO ROBUSTO DE DOCSTRINGS PYTHON:
    - Uso de patrón constante METADATA_PATTERN para consistencia
    - Validación de existencia del bloque antes de reemplazar
    - Error claro si no se encuentra el bloque de metadatos

4.  CORRECCIÓN DE SUBCOMANDO LOG:
    - Se añade parámetro 'executor' faltante en llamada
    - Se incluye manejador de metadatos en la llamada

5.  CONFIGURACIÓN EXTERNA:
    - Parámetros críticos configurados desde alma_cleaner_config.yaml:
        * max_file_size
        * supported_extensions
        * backup_dir
    - Valores por defecto mantenidos si no se especifican

6.  MECANISMO DE BLOQUEO:
    - Implementado con FileLock basado en fcntl
    - Previene condiciones de carrera en escrituras concurrentes
    - Timeout de 30 segundos para evitar bloqueos permanentes

7.  ROLLBACK AUTOMÁTICO:
    - Sistema de backups automáticos antes de cada escritura
    - Restauración automática en caso de error durante la operación
    - Limpieza de backups después de operaciones exitosas

8.  MEJORAS EN MIGRACIÓN DE TEMPLATES:
    - Registro detallado en historial de cambios durante migración
    - Advertencias explícitas en logs sobre campos migrados/eliminados

9.  COMPORTAMIENTO DRY-RUN + FORCE:
    - Prioridad clara a dry-run sobre force
    - Mensaje de advertencia explícito cuando se usan juntos

10. VALIDACIONES ADICIONALES:
    - Verificación de extensión de archivo soportada
    - Validación de existencia de bloque de metadatos en Python
    - Mejor manejo de errores en operaciones de escritura

CHANGELOG:
v0.1.5 - 2025-07-11
-   Corrección de problemas críticos identificados en auditoría
-   Implementación de sistema de bloqueo de archivos
-   Configuración externa de parámetros críticos
-   Rollback automático con sistema de backups
-   Manejo robusto de docstrings Python
-   Clarificación de comportamiento dry-run + force
-   Mejoras en migración de templates antiguos
-   Validaciones adicionales y manejo de errores mejorado

v0.1.4 - 2025-07-10
-   Migrado de PyYAML a ruamel.yaml
-   Implementado dry-run real
-   Añadida escritura atómica
-   Implementado hash de verificación
-   Añadida migración de templates
-   Soporte para salida JSON
-   Validación de archivos binarios/grandes
-   Mejor manejo de docstrings en Python
"""

if __name__ == "__main__":
    main()
```

### Changelog v0.1.5: (bloque yaml)

```yaml
cambios_aplicados_v0.1.5:
  version: "0.1.5"
  fecha: "2025-07-11"
  cambios:
    - eliminar_duplicacion_atomic_write:
        descripcion: "Eliminado método atomic_write duplicado en MetadataHandler"
        accion: "Se conservó la versión mejorada con manejo de directorios"
        impacto: "Reducción de código duplicado y mantenimiento simplificado"
        
    - refactorizar_extraccion_yaml:
        descripcion: "Corrección de retorno en método _extract_from_yaml"
        accion: |
          - Retorno consistente (dict, str) incluso para contenido vacío
          - Archivos YAML vacíos ahora retornan {} en lugar de None
        impacto: "Comportamiento consistente en extracción de metadatos"

    - manejo_robusto_docstrings:
        descripcion: "Manejo robusto de metadatos en archivos Python"
        accion: |
          - Implementado patrón constante METADATA_PATTERN
          - Validación de existencia del bloque antes de reemplazar
          - Error explícito si no se encuentra el bloque de metadatos
        impacto: "Prevención de corrupción de archivos .py"

    - corregir_subcomando_log:
        descripcion: "Corrección de parámetro faltante en subcomando log"
        accion: |
          - Añadido parámetro 'executor' en llamada a mostrar_log()
          - Incluido manejador de metadatos en la llamada
        impacto: "Funcionalidad completa del subcomando log restaurada"

    - configuracion_externa:
        descripcion: "Parámetros configurables desde archivo externo"
        accion: |
          - Carga de alma_cleaner_config.yaml para:
            * max_file_size
            * supported_extensions
            * backup_dir
          - Clase MetadataHandler ahora recibe configuración
        impacto: "Mayor flexibilidad en configuración"

    - file_locking:
        descripcion: "Implementación de mecanismo de bloqueo de archivos"
        accion: |
          - Clase FileLock basada en fcntl
          - Timeout de 30 segundos para evitar bloqueos permanentes
          - Integrado en atomic_write
        impacto: "Prevención de race conditions en escrituras concurrentes"

    - rollback_automatico:
        descripcion: "Sistema de rollback automático"
        accion: |
          - Creación de backups antes de cada escritura
          - Restauración automática en caso de error
          - Eliminación de backups después de operaciones exitosas
        impacto: "Mayor resiliencia frente a fallos"

    - migracion_templates:
        descripcion: "Mejoras en migración de templates antiguos"
        accion: |
          - Registro detallado en historial de cambios
          - Advertencias explícitas en logs
        impacto: "Mayor transparencia en procesos de migración"

    - clarificar_dry_run_force:
        descripcion: "Comportamiento claro para flags combinados"
        accion: |
          - Prioridad explícita a --dry-run sobre --force
          - Mensaje de advertencia cuando se usan juntos
        impacto: "Comportamiento predecible en operaciones complejas"

    - mejoras_seguridad:
        descripcion: "Validaciones adicionales de seguridad"
        accion: |
          - Verificación de extensiones soportadas
          - Validación de existencia de bloque metadatos en Python
          - Manejo mejorado de errores en escritura
        impacto: "Mayor estabilidad y prevención de errores"

  notas_adicionales:
    - "Todas las mejoras críticas de la auditoría fueron implementadas"
    - "Se mantuvo compatibilidad con versiones anteriores"
    - "La estructura modular del código fue preservada y mejorada"
    - "Se añadieron 150+ líneas de documentación interna"
    - "El rendimiento se mantuvo dentro de parámetros aceptables"
```


## 2025-07-10: "ALMA_CLI_CLEANER v0.1.6" Backup en punto 6

### 1. 2025-07-10: Script: "Alma_Cli_Cleaner_v0.1.6" modulo modificado (bloque python)

Se reemplazo el siguiente bloque en el script anterior:
/home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.5/Alma_Cli_Cleaner_v0.1.5.py
y se creo:
/home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Alma_Cli_Cleaner_v0.1.6.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =========================================
# ALMA_CLI_CLEANER v0.1.6
# CLI profesional para gestión de metadatos ALMA_RESIST
# =========================================

# (Se mantiene todo el código anterior sin cambios hasta la sección corregida)

    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass
```

### 2. 2025-07-10: Changelog v0.1.6: (bloque yaml)

Cambios aplicados para la version 0.1.6
Script : /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Alma_Cli_Cleaner_v0.1.6.py

```yaml
cambios_aplicados_v0.1.6:
  version: "0.1.6"
  fecha: "2025-07-11"
  cambios:
    - correccion_sintaxis_atomic_write:
        descripcion: "Corregido error de sintaxis en creación del path del backup"
        accion: "Añadido paréntesis faltante en la llamada os.path.join"
        impacto: "Corregido fallo crítico que impedía ejecución del backup automático"

    - manejo_explicito_errores_backup:
        descripcion: "Añadido manejo explícito de errores en creación de backups"
        accion: |
          - Creación de backups dentro de bloque try-except
          - Mensajes claros de error si falla la creación del backup
        impacto: "Mejor diagnóstico de errores y resiliencia del sistema de backup"

    - configuracion_timeout_filelock:
        descripcion: "Timeout configurable para adquisición de bloqueo de archivo"
        accion: |
          - Timeout ahora es configurable desde la configuración externa
          - Manejo explícito de excepción TimeoutError con logging específico
        impacto: "Mayor flexibilidad y control en condiciones de alta concurrencia"

  notas_adicionales:
    - "Se recomienda verificar configuración externa para nuevo parámetro 'bloqueo_timeout'"
    - "Todos los errores críticos identificados en la auditoría se han corregido"
    - "La estabilidad del sistema frente a operaciones concurrentes ha sido mejorada"
```

### 3. 2025-07-10: Parche realizado a la v0.1.6 en "class FileLock:"

Fecha: 2025-07-10

Se modifico "class FileLock:"

Se estan probando los siguientes modulos hasta ver cual es el correcto.

#### Modulo Historico

```python
class FileLock:
    """Mecanismo de bloqueo de archivos para prevenir condiciones de carrera"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None

    def __enter__(self):
        """Adquiere el bloqueo"""
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > 30:  # Timeout después de 30 segundos
                    raise TimeoutError("No se pudo adquirir el bloqueo después de 30 segundos")
                time.sleep(0.1)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Libera el bloqueo"""
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
            self.lock_fd.close()
            try:
                os.remove(self.lock_file)
            except OSError:
                pass
```

#### Modulo Modificado:

```python
class FileLock:
    def __init__(self, file_path: str, timeout: int = 30):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None
        self.timeout = timeout  # <--- agregalo acá

    def __enter__(self):
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > self.timeout:
                    raise TimeoutError("No se pudo adquirir el bloqueo después de %d segundos" % self.timeout)
                time.sleep(0.1)
```

Nota: No funciono bien a la primera espera correcciones.

#### Modulo FileLock final:

Se genero modulo FileLock final.

```python
class FileLock:
    def __init__(self, file_path: str, timeout: int = 30):
        self.file_path = file_path
        self.lock_file = file_path + ".lock"
        self.lock_fd = None
        self.timeout = timeout

    def __enter__(self):
        self.lock_fd = open(self.lock_file, 'w')
        start_time = time.time()
        while True:
            try:
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return self
            except IOError:
                if time.time() - start_time > self.timeout:
                    raise TimeoutError("No se pudo adquirir el bloqueo después de %d segundos" % self.timeout)
                time.sleep(0.1)

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
            self.lock_fd.close()
            try:
                os.remove(self.lock_file)
            except OSError:
                pass
```

Nota: No funciono bien a la primera espera correcciones.



### 4. 2025-07-10: Parche realizado a la v0.1.6 en "log_entry = {}"

Fecha: 2025-07-10

Se modifico: "log_entry = {}"

#### Bloque python reemplazado :

```python
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before,
            "metadata_after": metadata_after
        }
```

#### Bloque python nuevo:

```python
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else {},
            "metadata_after": metadata_after if metadata_after is not None else {},
        }
```

Reemplazo el bloque y pruebo de nuevo el paso 4.

### 5. 2025-07-10: Parche realizado a la v0.1.6 en el punto 4 de las pruebas:

#### 2025-07-10 Fix 1: Modificacion en ""ia_metadata": {},"
Tiro un nuevo error y modifcamos la parte de:

```python
    "ia_metadata": {},
```

Y le agregamos:

```python
    "dummy": None
```
Resultado Final:

```python
    "ia_metadata": {"dummy": None},
```


#### 2025-07-10 Fix 2: Modificacion en "def normalize(metadata: Dict) -> Dict:"

Se agrega estto al bloque

```python
    @staticmethod
    def normalize(metadata: Dict) -> Dict:
        normalized = metadata.copy()
        # ...tu lógica normal...
        if "ia_metadata" not in normalized or not isinstance(normalized["ia_metadata"], dict) or not normalized["ia_metadata"]:
            normalized["ia_metadata"] = {"dummy": None}
        return normalized
```
Se modifico la parte seguimos en el punto 4 de las pruebas


#### 2025-07-10 Fix 3: Modificacion en "def migrate_template(old_metadata: Dict) -> Dict:"

Se modifica el bloque retirando Bloque python historico y se ingresa bloque python nuevo

##### Bloque python historico

Bloque python eliminado:

```python
    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()
        
        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }
        
        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo migrado: {old_field} -> {new_field}"
                })
        
        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            if field in migrated:
                del migrated[field]
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo obsoleto eliminado: {field}"
                })
        
        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated
```

##### Bloque python nuevo

Este bloque se agrego:

```python
    @staticmethod
    def migrate_template(old_metadata: Dict) -> Dict:
        """Migra metadatos antiguos al template actual"""
        migrated = old_metadata.copy()

        # Mapeo de campos antiguos a nuevos
        field_mappings = {
            "old_responsible": "responsable",
            "linked": "linked_to",
            "description": "descripcion",
            "creation_date": "created_at",
            "creator": "created_by"
        }

        # Aplicar mapeos
        for old_field, new_field in field_mappings.items():
            if old_field in migrated and new_field not in migrated:
                migrated[new_field] = migrated.pop(old_field)
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo migrado: {old_field} -> {new_field}"
                })

        # Eliminar campos obsoletos
        obsolete_fields = ["obsolete_field1", "deprecated_field"]
        for field in obsolete_fields:
            if field in migrated:
                del migrated[field]
                migrated["historial"].append({
                    "fecha": datetime.datetime.now().isoformat(),
                    "usuario": "Sistema",
                    "accion": "migración_template",
                    "descripcion": f"Campo obsoleto eliminado: {field}"
                })

        # --- BLOQUE CLAVE: asegurar ia_metadata correcto ---
        if (
            "ia_metadata" not in migrated or
            not isinstance(migrated["ia_metadata"], dict) or
            not migrated["ia_metadata"]
        ):
            migrated["ia_metadata"] = {"dummy": None}
        # --- FIN BLOQUE CLAVE ---

        # Actualizar versión
        migrated["template_version"] = TEMPLATE_VERSION
        return migrated
```

#### 2025-07-10 Fix 4: Modificacion en "def flush_logs(self) -> None:"

##### Prevención de errores Parquet: dicts vacíos en campos anidados

**Motivo del cambio:**  
PyArrow (y el formato Parquet en general) **no permite guardar estructuras de tipo "struct" (dict) completamente vacías** como parte de una tabla. Si en algún campo anidado (por ejemplo, `ia_metadata`) se intenta guardar `{}` en vez de un dict con al menos un campo, la operación falla y arroja errores de schema incompatibles.

**Solución aplicada:**  
Antes de escribir los logs en formato Parquet, se aplica una función recursiva (`fix_empty_structs`) que reemplaza **cualquier dict vacío por un dict con un campo dummy** (`{"dummy": None}`).  
Esto asegura que todos los campos anidados tengan estructura válida para Parquet, eliminando errores y manteniendo compatibilidad incluso si en el futuro se agregan nuevos campos de tipo dict en los metadatos.

**Ventajas:**
- Evita caídas silenciosas y corrupciones de log al trabajar con Parquet.
- Permite que los logs sean siempre homogéneos y robustos, sin importar la fuente de los datos.
- Deja el sistema preparado para futuras integraciones de IA o nuevos campos anidados, sin requerir reescritura de esta lógica.


##### Bloque python historico

Bloque python que fue retirado.

```python
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
            
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                        
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                    
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(self.log_buffer)
                    
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                    
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
            
            self.log_buffer = []
            
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                    
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                    def fix_empty_structs(obj):
                        if isinstance(obj, dict):
                            if not obj:
                                return {"dummy": None}
                            return {k: fix_empty_structs(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [fix_empty_structs(item) for item in obj]
                        else:
                            return obj

                    fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                    # ---- FIN BLOQUE CRÍTICO ----

                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(fixed_buffer)
                
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
        
            self.log_buffer = []
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

```

Problema resuelto 2025-07-10 17:28

#### Backup 2025-07-10 17:51

Link del backup : 
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507101733.py

### 6. 2025-07-10: Parche realizado a la v0.1.6 en el punto 5 de las pruebas: Borra los linked_to anteriores (Backup Realizado en este punto)

Borra los linked_to anteriores , deberiamos corregirlo.

##### Mejora en la función `set_linked`: preservación y suma de archivos vinculados

**Problema anterior:**  
El comando `set_linked` sobreescribía completamente la lista de archivos vinculados (`linked_to`), eliminando los registros previos cada vez que se ejecutaba.

**Solución aplicada:**  
Ahora, al agregar nuevos archivos vinculados, estos se suman a los ya existentes sin generar duplicados, preservando así la trazabilidad histórica de todas las vinculaciones del archivo.

- Los archivos indicados se agregan al final de la lista `linked_to`.
- Si ya existían, no se repiten (se mantiene solo una entrada por vínculo).
- El historial de cambios documenta siempre la última actualización.

**Ventaja:**  
Se garantiza que ningún vínculo previo se pierde, facilitando la navegación y el análisis de relaciones dentro del sistema ALMA_RESIST, y manteniendo la integridad documental a lo largo del tiempo.


##### Bloque python historico

Bloque python que fue retirado.

```python
def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1
            
        original_metadata = metadata.copy()
        metadata["linked_to"] = linked_to
        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(linked_to)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def set_linked(
    file_path: str, 
    linked_to: List[str], 
    executor: str, 
    logger: AuditLogger,
    metadata_handler: MetadataHandler
) -> int:
    """Implementa el subcomando 'set_linked'"""
    try:
        metadata, content_body = metadata_handler.extract_metadata(file_path)
        if metadata is None:
            logger.add_log_entry("set_linked", file_path, "error", 
                               "No se encontraron metadatos válidos", executor)
            sys.stderr.write("Error: No se encontraron metadatos válidos en el archivo.\n")
            return 1

        original_metadata = metadata.copy()
        # Sumar a los ya existentes y eliminar duplicados
        prev_links = metadata.get("linked_to", [])
        all_links = list(dict.fromkeys(prev_links + linked_to))  # mantiene orden y no duplica
        metadata["linked_to"] = all_links

        metadata["last_modified"] = datetime.datetime.now().isoformat()
        metadata["last_modified_by"] = executor
        metadata["historial"].append({
            "fecha": datetime.datetime.now().isoformat(),
            "usuario": executor,
            "accion": "actualización_linked",
            "descripcion": f"Archivos vinculados actualizados: {', '.join(linked_to)}"
        })
        
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        metadata_handler.write_metadata(file_path, metadata, 
                                     content_body, original_content)
        
        logger.add_log_entry("set_linked", file_path, "updated", 
                           f"Vinculos actualizados a: {', '.join(all_links)}", 
                           executor, metadata_before=original_metadata, 
                           metadata_after=metadata)
        print(f"Archivos vinculados actualizados exitosamente")
        return 0
    except Exception as e:
        traceback.print_exc()
        logger.add_log_entry("set_linked", file_path, "error", str(e), executor)
        sys.stderr.write(f"ERROR: {str(e)}\n")
        return 1
```


##### Backup previo a Reparar

Link :
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507101759.py


##### Prueba de actualización incremental de archivos vinculados (`linked_to`)

**Fecha:** 2025-07-11  
**Script:** `Alma_Cli_Cleaner_v0.1.6.py`  
**Responsable:** Bird

###### Descripción:
Se verificó que la función `set_linked` permite agregar nuevos archivos a la lista `linked_to` de los metadatos de un archivo `.md` **sin eliminar los vínculos anteriores** y **sin duplicar archivos ya vinculados**.

###### Pasos realizados:
1. Se ejecutó el comando:
    ```bash
    python3 Alma_Cli_Cleaner_v0.1.6.py set_linked ./test.md --linked-to "./otro_archivo.md,./mas_vinculos.md"
    ```
    - Resultado: los archivos `./otro_archivo.md` y `./mas_vinculos.md` fueron agregados exitosamente.

2. Se ejecutó un segundo comando para probar la adición incremental:
    ```bash
    python3 Alma_Cli_Cleaner_v0.1.6.py set_linked ./test.md --linked-to "./informe2025.md,./balance_semanal.md,./bitacora_diaria.md"
    ```
    - Resultado: los archivos `./informe2025.md`, `./balance_semanal.md` y `./bitacora_diaria.md` se agregaron al listado, manteniéndose los vínculos anteriores y sin generar duplicados.

###### Verificación:
- El campo `linked_to` en el archivo `test.md` quedó con todos los archivos vinculados, y **no se perdió ningún vínculo previo**.
- El historial de cambios registró correctamente cada actualización.
- El sistema se comportó de acuerdo a la lógica esperada y no se reportaron errores en logs ni en Parquet.

###### Conclusión:
La función de actualización incremental de archivos vinculados (`set_linked`) quedó validada y funcional, garantizando integridad documental y trazabilidad de relaciones en los metadatos del sistema ALMA_RESIST.


### Prueba del historial de cambios (`log`)

**Fecha:** 2025-07-11  
**Script:** `Alma_Cli_Cleaner_v0.1.6.py`  
**Responsable:** Bird

#### Descripción:
Se validó la funcionalidad del subcomando `log`, encargado de mostrar el historial completo de cambios registrados en los metadatos de cada archivo.  
El historial incluye fecha, usuario, acción y descripción de cada modificación significativa.

#### Comando ejecutado:
```bash
python3 Alma_Cli_Cleaner_v0.1.6.py log ./test.md
```

Verificación:

 - El historial mostró correctamente cada acción ejecutada sobre el archivo, incluyendo actualizaciones de responsables y archivos vinculados.

 - Cada entrada incluye marca de tiempo, usuario ejecutor, tipo de acción y descripción asociada.

 - No se detectaron errores en la visualización ni inconsistencias en la trazabilidad de cambios.

Conclusión:

La función de auditoría interna (log) quedó validada, permitiendo seguimiento total y transparente de las modificaciones sobre los archivos del sistema ALMA_RESIST.
Este subcomando es fundamental para la auditoría y la gobernanza documental.

#### Estabilización de schema en logs Parquet: dicts completos desde la primera fila

**Situación:**  
Se detectó que los errores de Parquet al escribir logs provenían de una inconsistencia en el esquema (`schema`) de los campos anidados (`metadata_before` y `metadata_after`). Esto ocurría cuando la primera entrada registrada contenía únicamente un diccionario mínimo (`{"dummy": None}`), mientras que las siguientes incluían todos los campos de metadatos completos. Como Parquet fija el schema a partir de la primera fila, esto causaba errores de incompatibilidad en los registros posteriores.

**Solución aplicada:**  
A partir de ahora, el sistema fuerza que **todas las entradas del log utilicen un diccionario completo con todas las claves definidas en `TEMPLATE_METADATOS`** para los campos `metadata_before` y `metadata_after`, incluso cuando se encuentren vacíos. Esto asegura que el schema del archivo Parquet sea siempre homogéneo, robusto y compatible desde la primera fila.

**Impacto positivo:**  
- Se elimina el riesgo de errores de tipado o incompatibilidad en el log Parquet.
- Se garantiza la trazabilidad y la integridad del historial de auditoría documental, sin importar el flujo ejecutado.
- La solución es escalable y preparada para futuros cambios en la estructura de metadatos.

**Se agrego:**

```python
def empty_metadata_dict():
    # Cada lista tiene un valor real como dummy para fijar el tipo
    return {
        "title": "",
        "uuid": "",
        "tipo": "",
        "schema": "",
        "estado": "",
        "descripcion": "",
        "estructura": [""],  # <-- string dummy
        "tags": [""],        # <-- string dummy
        "linked_to": [""],   # <-- string dummy
        "responsable": [""], # <-- string dummy
        "hash_verificacion": "",
        "historial": [{
            "fecha": "",
            "usuario": "",
            "accion": "",
            "descripcion": ""
        }],  # <-- dict dummy
        "last_modified": "",
        "last_modified_by": "",
        "created_at": "",
        "created_by": "",
        "proceso_origen": "",
        "input_data": [""],
        "output_data": [""],
        "comentarios": "",
        "ia_metadata": {"dummy": None},
        "template_version": "",
    }
```

El error persiste le pedimos a deepsek una refaccion de dos modulos internos.

### 2025-07-10: Refaccion Deepseek

Prompt para DeepSeek - Refactor Logging PyArrow/Parquet
Proyecto: ALMA_CLI_CLEANER
Autor: Santi (Bird)
Fecha: 2025-07-10

```yaml
# ============================================================
#  Prompt para DeepSeek - Refactor Logging Parquet/PyArrow
#  Proyecto: ALMA_CLI_CLEANER v0.1.6
# ============================================================

contexto:
  descripcion: |
    El CLI ALMA_CLI_CLEANER realiza auditoría estructurada de archivos YAML/Markdown y deja un log de eventos en Parquet.
    El buffer de logs es una lista de dicts con metadatos antes y después de cada operación.
    El template de metadatos es fijo y explícito en el código (ver TEMPLATE_METADATOS).
    Las funciones de logging utilizan PyArrow para escribir el buffer, pero el schema infiere tipos incorrectos si el primer registro no contiene datos reales (dummy vacíos).
    El error crítico es "Schema at index 1 was different", por incompatibilidad mínima de tipos (ej: list<item: null> vs list<item: string>).

  objetivo: |
    Refactorizar el módulo de logs para que:
      - Defina el schema de PyArrow explícitamente (sin inferencia automática).
      - Siempre que una lista/dict esté vacía, se fuerce el tipo correcto (string, dict, etc).
      - El sistema pueda crecer en campos y no se rompa nunca por diferencias de tipo.
      - Sea compatible con append incremental: el schema del archivo debe coincidir con el de los nuevos logs.
    El buffer de logs debe ser casteado o rellenado con dummy según el template, antes de serializarse.

  instrucciones:
    - Analizá el código completo (ver abajo).
    - Proponé una solución concreta: bloque de función `flush_logs`, declaración de schema PyArrow explícito, y helpers si es necesario.
    - El bloque de logging debe ser plug-and-play y robusto para cualquier cambio futuro de campos.
    - Dejá el ejemplo de schema (en Python) para referencia.
    - Agregá abajo los módulos a refactorizar (`flush_logs`, `TEMPLATE_METADATOS`, helpers).

codigo:
  - flush_logs: |
      def flush_logs(self) -> None:
          """Escribe todos los logs en buffer al destino configurado"""
          if not self.log_buffer:
              return
        
          try:
              log_path = self.config["logs"]["ruta"]
              log_format = self.config["logs"]["formato"].lower()
              os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
              if log_format == "json":
                  with open(log_path, 'a', encoding='utf-8') as f:
                      for entry in self.log_buffer:
                          f.write(json.dumps(entry) + "\n")
                    
              elif log_format == "yaml":
                  with open(log_path, 'a', encoding='utf-8') as f:
                      yaml.dump_all(self.log_buffer, f)
                
              elif log_format == "parquet":
                  try:
                      import pyarrow as pa
                      import pyarrow.parquet as pq

                      # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                      def fix_empty_structs(obj):
                          if isinstance(obj, dict):
                              if not obj:
                                  return {"dummy": None}
                              return {k: fix_empty_structs(v) for k, v in obj.items()}
                          elif isinstance(obj, list):
                              return [fix_empty_structs(item) for item in obj]
                          else:
                              return obj

                      fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                      # ---- FIN BLOQUE CRÍTICO ----

                      # Convertir a tabla PyArrow
                      table = pa.Table.from_pylist(fixed_buffer)
                
                      # Escribir en modo append
                      if os.path.exists(log_path):
                          existing = pq.read_table(log_path)
                          table = pa.concat_tables([existing, table])
                
                      pq.write_table(table, log_path)
                
                  except ImportError:
                      # Fallback a JSON si pyarrow no está disponible
                      with open(log_path + ".json", 'a', encoding='utf-8') as f:
                          for entry in self.log_buffer:
                              f.write(json.dumps(entry) + "\n")
        
              self.log_buffer = []
        
          except Exception as e:
              sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
  - template_metadatos: |
      TEMPLATE_METADATOS = {
        "title": "",
        "uuid": "",
        "tipo": "",
        "schema": "",
        "estado": "",
        "descripcion": "",
        "estructura": [],
        "tags": [],
        "linked_to": [],
        "responsable": [],
        "hash_verificacion": "",
        "historial": [],
        "last_modified": "",
        "last_modified_by": "",
        "created_at": "",
        "created_by": "",
        "proceso_origen": "",
        "input_data": [],
        "output_data": [],
        "comentarios": "",
        "ia_metadata": {"dummy": None},
        "template_version": TEMPLATE_VERSION,
      }
  - helpers: |
      class AuditLogger:
          """Manejador centralizado de logs y auditoría"""
    
          def __init__(self, config: Dict):
              self.config = config
              self.log_buffer = []
        
          def add_log_entry(
              self, 
              action: str, 
              file_path: str, 
              status: str, 
              details: str, 
              user: str, 
              metadata_before: Dict = None, 
              metadata_after: Dict = None,
              json_output: bool = False
          ) -> Union[Dict, None]:
              """Agrega una entrada al buffer de logs"""
              log_entry = {
                  "timestamp": datetime.datetime.now().isoformat(),
                  "action": action,
                  "file": file_path,
                  "status": status,
                  "details": details,
                  "user": user,
                  "version": VERSION,
                  "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
                  "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
              }
        
              self.log_buffer.append(log_entry)
        
              if json_output:
                  return log_entry
              return None
        
          def flush_logs(self) -> None:
              """Escribe todos los logs en buffer al destino configurado"""
              if not self.log_buffer:
                  return
        
              try:
                  log_path = self.config["logs"]["ruta"]
                  log_format = self.config["logs"]["formato"].lower()
                  os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
                  if log_format == "json":
                      with open(log_path, 'a', encoding='utf-8') as f:
                          for entry in self.log_buffer:
                              f.write(json.dumps(entry) + "\n")
                    
                  elif log_format == "yaml":
                      with open(log_path, 'a', encoding='utf-8') as f:
                          yaml.dump_all(self.log_buffer, f)
                
                  elif log_format == "parquet":
                      try:
                          import pyarrow as pa
                          import pyarrow.parquet as pq

                          # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                          def fix_empty_structs(obj):
                              if isinstance(obj, dict):
                                  if not obj:
                                      return {"dummy": None}
                                  return {k: fix_empty_structs(v) for k, v in obj.items()}
                              elif isinstance(obj, list):
                                  return [fix_empty_structs(item) for item in obj]
                              else:
                                  return obj

                          fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                          # ---- FIN BLOQUE CRÍTICO ----

                          # Convertir a tabla PyArrow
                          table = pa.Table.from_pylist(fixed_buffer)
                
                          # Escribir en modo append
                          if os.path.exists(log_path):
                            existing = pq.read_table(log_path)
                            table = pa.concat_tables([existing, table])
                
                          pq.write_table(table, log_path)
                
                      except ImportError:
                          # Fallback a JSON si pyarrow no está disponible
                          with open(log_path + ".json", 'a', encoding='utf-8') as f:
                              for entry in self.log_buffer:
                                  f.write(json.dumps(entry) + "\n")
        
                  self.log_buffer = []
        
              except Exception as e:
                  sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")   

notas:
  - Si el código necesita helpers de casteo, incluilos.
  - Si hace falta modificar cómo se arma el buffer, sugerilo.
  - Acepta sugerencias de mejora, pero la prioridad es robustez en schema y facilidad de extensión.

# FIN DEL PROMPT

```

#### Respuesta Deepseek

Script sugerido por DeepSeek.

```python
# ====================== HELPERS ======================
def get_arrow_schema() -> pa.Schema:
    """Define schema explícito para registros de log"""
    return pa.schema([
        ("timestamp", pa.string()),
        ("action", pa.string()),
        ("file", pa.string()),
        ("status", pa.string()),
        ("details", pa.string()),
        ("user", pa.string()),
        ("version", pa.string()),
        ("metadata_before", get_metadata_struct()),
        ("metadata_after", get_metadata_struct())
    ])

def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(pa.string())),  # Asumido como lista de strings
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),  # Asumido como lista de strings
        ("output_data", pa.list_(pa.string())), # Asumido como lista de strings
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])

def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)
        
        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}
        
        # Conversión de tipos
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}
        
        normalized[key] = val
    
    return normalized

# ====================== EN AuditLogger ======================
class AuditLogger:
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        self.schema = get_arrow_schema()  # Schema predefinido
    
    def add_log_entry(...):  # Mantener igual
        ...
    
    def flush_logs(self) -> None:
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            if log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq
                    
                    # Normalizar datos usando el template
                    TEMPLATE_METADATOS = { ... }  # Definir aquí o importar
                    normalized_buffer = []
                    for entry in self.log_buffer:
                        norm_entry = entry.copy()
                        norm_entry["metadata_before"] = normalize_data(
                            entry.get("metadata_before", {}),
                            TEMPLATE_METADATOS
                        )
                        norm_entry["metadata_after"] = normalize_data(
                            entry.get("metadata_after", {}),
                            TEMPLATE_METADATOS
                        )
                        normalized_buffer.append(norm_entry)
                    
                    # Crear tabla con schema explícito
                    table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)
                    
                    # Manejo de appends
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        
                        # Unificar schemas si es necesario
                        if existing.schema != self.schema:
                            existing = existing.cast(self.schema)
                        
                        table = pa.concat_tables([existing, table])
                    
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON
                    ...
            
            else:
                # Mantener implementación original para JSON/YAML
                ...
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

```yaml
refactor_logging_solution:
  objetivo: "Refactorizar módulo de logs para PyArrow con schema explícito y manejo robusto de tipos"
  cambios_principales:
    - schema_explicito: "Definición manual de tipos PyArrow basada en TEMPLATE_METADATOS"
    - normalizacion_datos: "Relleno de valores vacíos con tipos correctos"
    - manejo_appends: "Unificación de schemas durante escritura incremental"
    - eliminacion_hacks: "Remoción del bloque fix_empty_structs"

  implementacion:
    helpers:
      - nombre: "get_arrow_schema"
        descripcion: "Define schema explícito para registros de log"
        estructura: |
          pa.schema([
            ("timestamp", pa.string()),
            ("action", pa.string()),
            ("file", pa.string()),
            ("status", pa.string()),
            ("details", pa.string()),
            ("user", pa.string()),
            ("version", pa.string()),
            ("metadata_before", get_metadata_struct()),
            ("metadata_after", get_metadata_struct())
          ])

      - nombre: "get_metadata_struct"
        descripcion: "Schema para estructura de metadatos"
        estructura: |
          pa.struct([
            ("title", pa.string()),
            ("uuid", pa.string()),
            ("tipo", pa.string()),
            ("schema", pa.string()),
            ("estado", pa.string()),
            ("descripcion", pa.string()),
            ("estructura", pa.list_(pa.string())),
            ("tags", pa.list_(pa.string())),
            ("linked_to", pa.list_(pa.string())),
            ("responsable", pa.list_(pa.string())),
            ("hash_verificacion", pa.string()),
            ("historial", pa.list_(pa.string())),
            ("last_modified", pa.string()),
            ("last_modified_by", pa.string()),
            ("created_at", pa.string()),
            ("created_by", pa.string()),
            ("proceso_origen", pa.string()),
            ("input_data", pa.list_(pa.string())),
            ("output_data", pa.list_(pa.string())),
            ("comentarios", pa.string()),
            ("ia_metadata", pa.map_(pa.string(), pa.string())),
            ("template_version", pa.string())
          ])

      - nombre: "normalize_data"
        descripcion: "Normaliza datos según template con tipos correctos"
        logica: |
          for key, default_val in template.items():
            val = entry.get(key, default_val)
            if isinstance(default_val, list) and not val: val = []
            elif isinstance(default_val, dict) and not val: val = {}
            if isinstance(default_val, list) and not isinstance(val, list): val = [val]
            elif isinstance(default_val, dict) and not isinstance(val, dict): val = {}
            normalized[key] = val

    cambios_en_AuditLogger:
      - clase: "AuditLogger"
        cambios:
          - en_metodo: "__init__"
            adicion: "self.schema = get_arrow_schema()"

          - en_metodo: "flush_logs"
            reemplazo_bloque_parquet: |
              # Normalizar datos usando template
              normalized_buffer = []
              for entry in self.log_buffer:
                  norm_entry = entry.copy()
                  norm_entry["metadata_before"] = normalize_data(entry.get("metadata_before", {}), TEMPLATE_METADATOS)
                  norm_entry["metadata_after"] = normalize_data(entry.get("metadata_after", {}), TEMPLATE_METADATOS)
                  normalized_buffer.append(norm_entry)
              
              # Crear tabla con schema explícito
              table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)
              
              # Manejo de appends con unificación de schema
              if os.path.exists(log_path):
                  existing = pq.read_table(log_path)
                  if existing.schema != self.schema:
                      existing = existing.cast(self.schema)
                  table = pa.concat_tables([existing, table])

  beneficios:
    - "Elimina errores de inferencia de tipos en PyArrow"
    - "Permite estructuras vacías sin necesidad de hacks (dummy fields)"
    - "Mantiene compatibilidad con archivos existentes"
    - "Permite evolución futura de campos"
    - "Mejor desempeño al evitar conversiones innecesarias"

  notas_importantes:
    - "El campo 'ia_metadata' se define como mapa string->string. Si tiene estructura compleja, usar struct específico"
    - "Campos de lista (tags, linked_to) deben contener elementos del tipo definido"
    - "La normalización garantiza coherencia entre registros vacíos/llenos"
    - "El cast durante append asegura compatibilidad entre versiones"
    - "Template debe mantenerse actualizado con nuevos campos"

  flujo_mejorado:
    1. "Agregar entrada al buffer"
    2. "Al flush: normalizar cada entrada con template"
    3. "Crear tabla PyArrow con schema explícito"
    4. "Si existe archivo: leer, unificar schema y concatenar"
    5. "Escribir nueva tabla unificada"
```

#### Backup Pre-refacción :

Link :
 - /home/alma/Alma-Cli/Cleaner/CLI/Alma_Cli_Cleaner_v0.1.6/Backup/Alma_Cli_Cleaner_v0.1.6_202507102048.py

### 2025-07-10: Remplazo de modulo "classAuditLogger"

Se remplaza el modulo completo AuditLogger

##### Bloque python historico

Bloque python que fue retirado.

```python
class AuditLogger:
    """Manejador centralizado de logs y auditoría"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        
    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
            "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
        }
        
        self.log_buffer.append(log_entry)
        
        if json_output:
            return log_entry
        return None
        
    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return
        
        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")
                    
            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)
                
            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # ---- BLOQUE CRÍTICO: fuerza que no existan dict vacíos ----
                    def fix_empty_structs(obj):
                        if isinstance(obj, dict):
                            if not obj:
                                return {"dummy": None}
                            return {k: fix_empty_structs(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [fix_empty_structs(item) for item in obj]
                        else:
                            return obj

                    fixed_buffer = [fix_empty_structs(entry) for entry in self.log_buffer]
                    # ---- FIN BLOQUE CRÍTICO ----

                    # Convertir a tabla PyArrow
                    table = pa.Table.from_pylist(fixed_buffer)
                
                    # Escribir en modo append
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        table = pa.concat_tables([existing, table])
                
                    pq.write_table(table, log_path)
                
                except ImportError:
                    # Fallback a JSON si pyarrow no está disponible
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")
        
            self.log_buffer = []
        
        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
class AuditLogger:
    """Manejador centralizado de logs y auditoría"""

    def __init__(self, config: Dict):
        self.config = config
        self.log_buffer = []
        self.schema = get_arrow_schema()  # Schema explícito definido por helper

    def add_log_entry(
        self, 
        action: str, 
        file_path: str, 
        status: str, 
        details: str, 
        user: str, 
        metadata_before: Dict = None, 
        metadata_after: Dict = None,
        json_output: bool = False
    ) -> Union[Dict, None]:
        """Agrega una entrada al buffer de logs"""
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "file": file_path,
            "status": status,
            "details": details,
            "user": user,
            "version": VERSION,
            "metadata_before": metadata_before if metadata_before is not None else empty_metadata_dict(),
            "metadata_after": metadata_after if metadata_after is not None else empty_metadata_dict(),
        }

        self.log_buffer.append(log_entry)

        if json_output:
            return log_entry
        return None

    def flush_logs(self) -> None:
        """Escribe todos los logs en buffer al destino configurado"""
        if not self.log_buffer:
            return

        try:
            log_path = self.config["logs"]["ruta"]
            log_format = self.config["logs"]["formato"].lower()
            os.makedirs(os.path.dirname(log_path), exist_ok=True)

            if log_format == "json":
                with open(log_path, 'a', encoding='utf-8') as f:
                    for entry in self.log_buffer:
                        f.write(json.dumps(entry) + "\n")

            elif log_format == "yaml":
                with open(log_path, 'a', encoding='utf-8') as f:
                    yaml.dump_all(self.log_buffer, f)

            elif log_format == "parquet":
                try:
                    import pyarrow as pa
                    import pyarrow.parquet as pq

                    # --- BLOQUE CRÍTICO: Normalización según schema explícito ---
                    normalized_buffer = []
                    for entry in self.log_buffer:
                        norm_entry = entry.copy()
                        norm_entry["metadata_before"] = normalize_data(
                            entry.get("metadata_before", {}),
                            TEMPLATE_METADATOS
                        )
                        norm_entry["metadata_after"] = normalize_data(
                            entry.get("metadata_after", {}),
                            TEMPLATE_METADATOS
                        )
                        normalized_buffer.append(norm_entry)

                    # Crear tabla con schema explícito
                    table = pa.Table.from_pylist(normalized_buffer, schema=self.schema)

                    # Append si existe archivo previo (y unificar schema)
                    if os.path.exists(log_path):
                        existing = pq.read_table(log_path)
                        if existing.schema != self.schema:
                            existing = existing.cast(self.schema)
                        table = pa.concat_tables([existing, table])

                    pq.write_table(table, log_path)

                except ImportError:
                    # Fallback a JSON si no hay pyarrow
                    with open(log_path + ".json", 'a', encoding='utf-8') as f:
                        for entry in self.log_buffer:
                            f.write(json.dumps(entry) + "\n")

            self.log_buffer = []

        except Exception as e:
            sys.stderr.write(f"ERROR: Fallo al escribir logs: {str(e)}\n")

```

### 2025-07-10: CORRECCIÓN SÓLIDA DE BLOQUE CLAVE

Te escribo solo la parte relevante de schema y normalize_data y el dict de ejemplo.

#### 1. TEMPLATE_METADATOS

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
TEMPLATE_METADATOS = {
    "title": "",
    "uuid": "",
    "tipo": "",
    "schema": "",
    "estado": "",
    "descripcion": "",
    "estructura": [],
    "tags": [],
    "linked_to": [],
    "responsable": [],
    "hash_verificacion": "",
    "historial": [],  # <-- Lista vacía
    "last_modified": "",
    "last_modified_by": "",
    "created_at": "",
    "created_by": "",
    "proceso_origen": "",
    "input_data": [],
    "output_data": [],
    "comentarios": "",
    "ia_metadata": {},
    "template_version": TEMPLATE_VERSION,
}
```

##### Bloque python historico

Bloque python que fue retirado.

```python
TEMPLATE_METADATOS = {
    "title": "",
    "uuid": "",
    "tipo": "",
    "schema": "",
    "estado": "",
    "descripcion": "",
    "estructura": [],
    "tags": [],
    "linked_to": [],
    "responsable": [],
    "hash_verificacion": "",
    "historial": [],
    "last_modified": "",
    "last_modified_by": "",
    "created_at": "",
    "created_by": "",
    "proceso_origen": "",
    "input_data": [],
    "output_data": [],
    "comentarios": "",
    "ia_metadata": {"dummy": None},
    "template_version": TEMPLATE_VERSION,
}
```

#### 2. get_metadata_struct mejorado

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(
            pa.struct([
                ("fecha", pa.string()),
                ("usuario", pa.string()),
                ("accion", pa.string()),
                ("descripcion", pa.string()),
            ])
        )),
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),
        ("output_data", pa.list_(pa.string())),
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])
```
##### Bloque python historico

Bloque python que fue retirado.

```python
def get_metadata_struct() -> pa.StructType:
    """Schema para la estructura de metadatos basado en TEMPLATE_METADATOS"""
    return pa.struct([
        ("title", pa.string()),
        ("uuid", pa.string()),
        ("tipo", pa.string()),
        ("schema", pa.string()),
        ("estado", pa.string()),
        ("descripcion", pa.string()),
        ("estructura", pa.list_(pa.string())),
        ("tags", pa.list_(pa.string())),
        ("linked_to", pa.list_(pa.string())),
        ("responsable", pa.list_(pa.string())),
        ("hash_verificacion", pa.string()),
        ("historial", pa.list_(pa.string())),  # Asumido como lista de strings
        ("last_modified", pa.string()),
        ("last_modified_by", pa.string()),
        ("created_at", pa.string()),
        ("created_by", pa.string()),
        ("proceso_origen", pa.string()),
        ("input_data", pa.list_(pa.string())),  # Asumido como lista de strings
        ("output_data", pa.list_(pa.string())), # Asumido como lista de strings
        ("comentarios", pa.string()),
        ("ia_metadata", pa.map_(pa.string(), pa.string())),
        ("template_version", pa.string())
    ])
```


#### 3. normalize_data: asegurar listas de dicts

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)

        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}

        # Historial: asegurar lista de dicts
        if key == "historial" and val and isinstance(val, list):
            val = [
                {
                    "fecha": x.get("fecha", ""),
                    "usuario": x.get("usuario", ""),
                    "accion": x.get("accion", ""),
                    "descripcion": x.get("descripcion", "")
                }
                for x in val if isinstance(x, dict)
            ]

        # Conversión de tipos normales
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}

        normalized[key] = val

    return normalized
```

##### Bloque python historico

Bloque python que fue retirado.

```python
def normalize_data(entry: dict, template: dict) -> dict:
    """Normaliza datos según template con tipos correctos"""
    normalized = {}
    for key, default_val in template.items():
        val = entry.get(key, default_val)
        
        # Manejo especial para estructuras vacías
        if isinstance(default_val, list) and not val:
            val = []
        elif isinstance(default_val, dict) and not val:
            val = {}
        
        # Conversión de tipos
        if isinstance(default_val, list) and not isinstance(val, list):
            val = [val]
        elif isinstance(default_val, dict) and not isinstance(val, dict):
            val = {}
        
        normalized[key] = val
    
    return normalized
```

#### 4. En flush_logs

No cambies nada, solo asegurate de que la función normalize_data se usa así:

```python
norm_entry["metadata_before"] = normalize_data(
    entry.get("metadata_before", {}),
    TEMPLATE_METADATOS
)
norm_entry["metadata_after"] = normalize_data(
    entry.get("metadata_after", {}),
    TEMPLATE_METADATOS
)
```

#### 5. Se remplaza el principio de atomic_write


##### Bloque python historico

Bloque python que fue retirado.

```python
    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        backup_path = None 
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass
```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python
    def atomic_write(self, file_path: str, content: str, dry_run: bool = False) -> None:
        """Escribe contenido de forma atómica usando archivo temporal con bloqueo"""
        backup_path = None
        if dry_run:
            print(f"[DRY-RUN] Se escribiría en: {file_path}")
            print(f"Contenido propuesto:\n{'-'*40}\n{content}\n{'-'*40}")
            return

        try:
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(file_path), exist_ok=True)

            # Verificación explícita del directorio backup
            try:
                os.makedirs(self.backup_dir, exist_ok=True)
            except OSError as e:
                raise IOError(f"No se pudo crear el directorio backup: {str(e)}")

            # Crear backup con manejo explícito de excepciones
            try:
                backup_path = os.path.join(
                    self.backup_dir,
                    os.path.basename(file_path) + f".{int(time.time())}.bak"
                )
                if os.path.exists(file_path):
                    shutil.copy2(file_path, backup_path)
            except Exception as e:
                raise IOError(f"No se pudo crear backup antes de escritura: {str(e)}")

            # Bloqueo de archivo con timeout configurable
            bloqueo_timeout = self.config.get("bloqueo_timeout", 30)
            with FileLock(file_path, timeout=bloqueo_timeout):
                with tempfile.NamedTemporaryFile(
                    mode='w',
                    encoding='utf-8',
                    delete=False,
                    dir=os.path.dirname(file_path)
                ) as tmp_file:
                    tmp_file.write(content)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, file_path)

        except TimeoutError:
            logger = AuditLogger(self.config)
            logger.add_log_entry(
                "file_lock",
                file_path,
                "error",
                "Timeout al adquirir bloqueo del archivo.",
                user="Sistema"
            )
            raise

        except Exception as e:
            if backup_path and os.path.exists(backup_path):
                shutil.move(backup_path, file_path)

            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)

            raise IOError(f"Error en escritura atómica: {str(e)}. Se restauró backup.")

        finally:
            if backup_path and os.path.exists(backup_path):
                try:
                    os.remove(backup_path)
                except:
                    pass

```


# -------------------------------------
# ----------------Fin------------------
# -------------------------------------

# Development

## Ideas para proxima version :
 - [] Hacer que calcule el hash y mueva los archivos internamente manteniendo el hash y el indice
 - [] Indexar todo en Alma-Cli_inde.json
 - [] Poder hacer todos lso movimientos internos del sistema dejando registrado cada movimiento.
 - [] Que recorra periodicamente los archivos viendo si algo se modifico de lugar.
 - [] Definir funcion para estado para archivar y mover a historico con un script.
 - [] Integrar tests internos por modulo y funcion.

## Utilidades

### Templates (Python ,Yaml, Remplazo de bloques python)

#### Python

```python

```

#### Yaml

```yaml

```

#### Remplazo de bloques python:

##### Bloque python historico

Bloque python que fue retirado.

```python

```

##### Bloque python nuevo

Bloque python que fue ingresado.

```python

```
