# MÃ³dulo `llm_server/` â€“ ALMA_RESIST

Este mÃ³dulo representa el nÃºcleo del servidor de lenguaje natural (LLM) dentro del sistema ALMA_RESIST. Se encarga de la carga de modelos, gestiÃ³n de contexto, cifrado de logs y transporte de mensajes con IA local o remota.

---

## ğŸ§± Estructura de carpetas

```
llm_server/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                  â† Punto de entrada del servidor FastAPI
â”œâ”€â”€ model_wrapper.py         â† Clase principal para interactuar con modelos LLM
â”œâ”€â”€ transport_layer.py       â† Manejo de solicitudes y respuestas con contexto
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ context_tracker.py
â”‚   â””â”€â”€ memory_graph.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ log_crypto.py
â”‚   â””â”€â”€ log_writer.py
```

---

## ğŸ§  Funcionalidad principal

- **Carga de modelos LLM locales** (ej. Mistral vÃ­a `llama.cpp`)
- **Seguimiento de contexto conversacional**
- **Encriptado/descifrado seguro de logs**
- **Logging estructurado con firma**
- **OrquestaciÃ³n de respuestas IA vÃ­a `transport_layer`**

---

## ğŸš€ CÃ³mo iniciar el servidor

```bash
uvicorn core.llm_server.main:app --reload
```

---

## ğŸ§ª CÃ³mo correr los tests

```bash
PYTHONPATH=. pytest tests/
```

---

## ğŸ“‹ Dependencias clave

- `llama-cpp` (si aplica)
- `fastapi`, `uvicorn`
- `cryptography`
- `pytest`

---

## ğŸ“¦ Estado del mÃ³dulo

VersiÃ³n: `0.0.0.4.1`  
Mantenimiento: activo  
AuditorÃ­a pendiente: sÃ­ (DeepSeek)
