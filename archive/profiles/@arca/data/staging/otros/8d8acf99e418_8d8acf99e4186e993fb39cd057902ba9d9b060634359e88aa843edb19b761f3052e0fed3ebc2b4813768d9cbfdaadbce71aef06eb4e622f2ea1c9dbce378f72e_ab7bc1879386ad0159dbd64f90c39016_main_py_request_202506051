---
module: prompts/main_py_request
type: core
status: in_progress
created: '2025-05-26'
linked_to:
- metodologia_doc_ia_v2.md

---
# ğŸ§  Solicitud de ImplementaciÃ³n â€“ `main.py` â€“ Sprint 2.6 â€“ ALMA_RESIST

## ğŸ¯ Objetivo

Implementar el archivo `main.py` del mÃ³dulo `llm_server`, que actuarÃ¡ como servidor API usando FastAPI. Este archivo forma parte del Sprint 2.6 del proyecto ALMA_RESIST.

---

## ğŸ“˜ Requisitos

- Usar FastAPI
- Crear endpoint POST `/responder`
- Recibir un campo `prompt` (string)
- Llamar al mÃ©todo `generate(prompt)` de la clase `ModelWrapper`
- Devolver un JSON con la respuesta del modelo
- Validar errores si el modelo no estÃ¡ cargado
- Debe ser asincrÃ³nico usando `asyncio.to_thread()` o similar

---

## ğŸ”§ Detalles TÃ©cnicos

- El archivo debe ubicarse en: `core/llm_server/main.py`
- `ModelWrapper` ya estÃ¡ implementado parcialmente en `model_wrapper.py`
- El endpoint debe validar entrada con Pydantic
- No debe haber lÃ³gica del modelo en `main.py`, solo orquestaciÃ³n

---

## ğŸ“ Referencias

- Esta tarea corresponde a la **Fase 1** del Sprint 2.6
- Basado en idea base: `idea_base_llm_server_0.0.0.4.1`

---

## ğŸ§ª Ejemplo de uso

```bash
curl -X POST http://localhost:8000/responder -H "Content-Type: application/json" -d '{"prompt": "Â¿QuiÃ©n fue Alan Turing?"}'
```

Esperado:

```json
{
  "respuesta": "Alan Turing fue un matemÃ¡tico britÃ¡nico considerado el padre de la computaciÃ³n moderna..."
}
```
