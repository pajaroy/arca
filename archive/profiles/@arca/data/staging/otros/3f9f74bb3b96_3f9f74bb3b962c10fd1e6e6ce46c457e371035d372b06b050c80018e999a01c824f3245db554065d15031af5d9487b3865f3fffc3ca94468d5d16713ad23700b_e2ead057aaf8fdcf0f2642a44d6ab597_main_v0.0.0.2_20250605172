# core/llm_server/main.py
import asyncio
import os
import sys
import logging
from datetime import datetime
from typing import Optional
from fastapi import FastAPI, HTTPException, status, BackgroundTasks
from pydantic import BaseModel, Field

# Añadir ruta base para imports
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Importar componentes internos
try:
    from model_wrapper.model_wrapper import ModelWrapper
    from transport_layer.transport_layer import TransportLayer
    from integration.context_tracker import ContextTracker
    from integration.memory_graph import MemoryGraph
    from utils.log_writer import LogWriter
    from utils.log_crypto import CryptoEngine
except ImportError as e:
    logging.error(f"Error de importación: {str(e)}")
    raise

# Configuración inicial
app = FastAPI(title="ALMA_RESIST LLM Server", version="0.0.0.4.1")
logger = logging.getLogger("alma_server")
model_wrapper: Optional[ModelWrapper] = None
context_tracker: Optional[ContextTracker] = None
memory_graph: Optional[MemoryGraph] = None
log_writer: Optional[LogWriter] = None
crypto_engine: Optional[CryptoEngine] = None

# Esquemas de validación Pydantic
class PromptRequest(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=2000,
                        example="¿Cuál es el sentido de la vida?",
                        description="Prompt para la IA reflexiva")

class HealthCheck(BaseModel):
    status: str
    model_loaded: bool
    model_info: Optional[dict]
    context_entries: int
    graph_nodes: int

def get_base_path():
    """Devuelve la ruta base del módulo"""
    return os.path.dirname(os.path.abspath(__file__))

def log_event(event_type: str, message: str, metadata: Optional[dict] = None):
    """Registra un evento estructurado con encriptación"""
    if log_writer:
        event = log_writer.log_event(
            event_type=event_type,
            message=message,
            module="llm_server",
            metadata=metadata
        )
        log_writer.write_log(event)
    else:
        logger.error("LogWriter no inicializado - no se puede registrar evento")

def encrypt_logs_background():
    """Tarea en segundo plano para cifrar logs antiguos"""
    try:
        if log_writer and crypto_engine:
            for log_file in log_writer.log_dir.glob("*.log"):
                if not crypto_engine.validar_log_cifrado(str(log_file)):
                    encrypted_file = str(log_file) + ".enc"
                    key, salt = crypto_engine.generar_clave()
                    crypto_engine.encrypt_log(str(log_file), encrypted_file, key, salt)
                    log_file.unlink()
                    log_event("security", f"Log cifrado: {log_file.name}")
    except Exception as e:
        logger.error(f"Error en cifrado de logs: {str(e)}")

@app.on_event("startup")
async def initialize_services():
    """Inicializa todos los servicios al arrancar el servidor"""
    global model_wrapper, context_tracker, memory_graph, log_writer, crypto_engine
    
    try:
        # Obtener ruta base
        base_path = get_base_path()
        
        # 1. Configurar logger y componentes
        log_dir = os.path.join(base_path, "logs")
        os.makedirs(log_dir, exist_ok=True)
        log_writer = LogWriter(log_dir=log_dir)
        crypto_engine = CryptoEngine()
        
        log_event("startup", "Inicializando servicios...")
        
        # 2. Cargar modelo LLM
        model_path = os.path.join(base_path, "models", "mistral-7b-q4.gguf")
        logger.info(f"Intentando cargar modelo desde: {model_path}")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Modelo no encontrado en: {model_path}")
        
        model_wrapper = ModelWrapper(model_path, quantization="Q4")
        model_wrapper.load_model()
        log_event("startup", f"Modelo cargado: {model_path}")
        
        # 3. Inicializar capa de transporte
        contracts_dir = os.path.join(base_path, "docs", "contracts")
        os.makedirs(contracts_dir, exist_ok=True)
        
        transport_layer = TransportLayer(
            contracts_dir=contracts_dir,
            base_dir=os.path.join(base_path, "transport_data")
        )
        log_event("startup", "Capa de transporte inicializada")
        
        # 4. Inicializar seguimiento de contexto y memoria
        context_tracker = ContextTracker(
            storage_path=os.path.join(base_path, "context_history.jsonl")
        )
        
        memory_graph = MemoryGraph(
            storage_path=os.path.join(base_path, "memory_graph.json")
        )
        
        # 5. Cargar contexto previo si existe
        context_history = context_tracker.get_history()
        if context_history:
            memory_graph.merge_from_context(context_tracker)
            log_event("context", 
                     f"Contexto histórico cargado: {len(context_history)} interacciones")
        
        log_event("startup", "Servicio completamente inicializado")
        
    except Exception as e:
        logger.critical(f"Error crítico en inicialización: {str(e)}")
        log_event("startup_error", f"Error en inicialización: {str(e)}")
        # No detenemos el servidor pero marcamos el error
        model_wrapper = None

@app.post("/responder", response_model=dict,
          responses={
              200: {"description": "Respuesta generada exitosamente"},
              503: {"description": "Modelo no cargado o no disponible"}
          })
async def generate_response(
    request: PromptRequest, 
    background_tasks: BackgroundTasks
):
    """Endpoint principal para generación de respuestas reflexivas"""
    if not model_wrapper or not model_wrapper.is_loaded():
        log_event("error", "Intento de uso de modelo no cargado")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="El modelo no está cargado"
        )

    try:
        # Generar respuesta asíncrona
        response = await asyncio.to_thread(
            model_wrapper.generate, 
            request.prompt
        )
        
        # Registrar en contexto
        if context_tracker:
            context_tracker.track_interaction(
                prompt=request.prompt,
                response=response,
                metadata={
                    "model": model_wrapper.get_model_info()["nombre"],
                    "module": "llm_server"
                }
            )
        
        # Actualizar grafo de memoria
        if memory_graph:
            background_tasks.add_task(
                memory_graph.create_edge,
                request.prompt,
                response,
                weight=0.8
            )
        
        log_event("response", "Respuesta generada", {
            "prompt_length": len(request.prompt),
            "response_length": len(response)
        })
        
        return {
            "respuesta": response,
            "metadata": {
                "modelo": model_wrapper.get_model_info()["nombre"],
                "longitud_prompt": len(request.prompt),
                "timestamp": datetime.utcnow().isoformat()
            }
        }
        
    except Exception as e:
        log_event("error", "Error generando respuesta", {
            "error": str(e),
            "prompt": request.prompt[:100]
        })
        logger.error(f"Error en generación: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error interno procesando la solicitud"
        )

@app.get("/health", response_model=HealthCheck)
async def health_check():
    """Endpoint de verificación de salud del servicio"""
    model_loaded = model_wrapper and model_wrapper.is_loaded()
    model_info = model_wrapper.get_model_info() if model_loaded else None
    
    context_count = len(context_tracker) if context_tracker else 0
    graph_nodes = len(memory_graph.graph) if memory_graph else 0
    
    return {
        "status": "OK" if model_loaded else "ERROR",
        "model_loaded": model_loaded,
        "model_info": model_info,
        "context_entries": context_count,
        "graph_nodes": graph_nodes
    }

@app.get("/context/history")
async def get_context_history(limit: int = 10):
    """Obtiene el historial de contexto reciente"""
    if context_tracker:
        return context_tracker.get_history(limit)
    return []

@app.get("/memory/graph")
async def get_memory_graph():
    """Exporta el grafo de memoria en formato JSON"""
    if memory_graph:
        return dict(memory_graph.graph)
    return {}

@app.post("/memory/export")
async def export_memory_graph(format: str = "json"):
    """Exporta el grafo de memoria a un archivo"""
    try:
        if not memory_graph:
            raise Exception("Grafo de memoria no inicializado")
            
        base_path = get_base_path()
        filename = f"memory_export_{datetime.utcnow().strftime('%Y%m%d')}"
        output_path = os.path.join(base_path, "exports", filename)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        if format == "json":
            memory_graph.export_graph(f"{output_path}.json", "json")
        else:
            memory_graph.export_graph(f"{output_path}.graphml", "graphml")
            
        return {"status": "success", "file": filename}
    except Exception as e:
        log_event("error", "Error exportando grafo", {"error": str(e)})
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error exportando grafo: {str(e)}"
        )

@app.post("/logs/encrypt")
async def encrypt_logs_task(background_tasks: BackgroundTasks):
    """Inicia el cifrado de logs en segundo plano"""
    background_tasks.add_task(encrypt_logs_background)
    return {"status": "started", "message": "Cifrado de logs iniciado en segundo plano"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
